{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45bbd454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ====== label space（沿用你之前那套 14 类；后续你想换可以改）======\n",
    "ID2LABEL_14 = [\n",
    "    \"Title\", \"Author\", \"Mail\", \"Affiliation\", \"Section\",\n",
    "    \"First-Line\", \"Para-Line\", \"Equation\", \"Table\", \"Figure\",\n",
    "    \"Caption\", \"Page-Footer\", \"Page-Header\", \"Footnote\"\n",
    "]\n",
    "LABEL2ID_14 = {k:i for i,k in enumerate(ID2LABEL_14)}\n",
    "\n",
    "REL2ID = {\"connect\": 0, \"contain\": 1, \"equality\": 2, \"meta\": 3}\n",
    "ID2REL = {v:k for k,v in REL2ID.items()}\n",
    "\n",
    "\n",
    "def map_hrd_class_to_14(c: str) -> int:\n",
    "    c = (c or \"\").lower().strip()\n",
    "    if c == \"title\":\n",
    "        return LABEL2ID_14[\"Title\"]\n",
    "    if c == \"author\":\n",
    "        return LABEL2ID_14[\"Author\"]\n",
    "    if c in (\"affili\", \"affiliation\"):\n",
    "        return LABEL2ID_14[\"Affiliation\"]\n",
    "    if c in (\"header\",):\n",
    "        return LABEL2ID_14[\"Page-Header\"]\n",
    "    if c in (\"footer\",):\n",
    "        return LABEL2ID_14[\"Page-Footer\"]\n",
    "    if c in (\"fnote\",):\n",
    "        return LABEL2ID_14[\"Footnote\"]\n",
    "    if c.startswith(\"sec\"):\n",
    "        return LABEL2ID_14[\"Section\"]\n",
    "    if c in (\"fstline\",):\n",
    "        return LABEL2ID_14[\"First-Line\"]\n",
    "    if c in (\"para\", \"opara\"):\n",
    "        return LABEL2ID_14[\"Para-Line\"]\n",
    "    # 兜底\n",
    "    return LABEL2ID_14[\"Para-Line\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_image_path(image_root, doc_id, page_id):\n",
    "    \"\"\"\n",
    "    Try multiple naming conventions:\n",
    "    1) HRDH-style: <image_root>/<doc_id>/<page_id>.png\n",
    "    2) HRDH-style with other ext: <image_root>/<doc_id>/<page_id>.(jpg/jpeg/png)\n",
    "    3) HRDS-style: <image_root>/<doc_id>/<doc_id>_<page_id>.(jpg/jpeg/png)\n",
    "    4) HRDS-style flat (just in image_root): <image_root>/<doc_id>_<page_id>.(jpg/jpeg/png)\n",
    "\n",
    "    Returns existing path; otherwise raises FileNotFoundError with helpful info.\n",
    "    \"\"\"\n",
    "    exts = (\"png\", \"jpg\", \"jpeg\", \"webp\")\n",
    "\n",
    "    # --- A) in subfolder <doc_id>/ ---\n",
    "    doc_dir = os.path.join(image_root, doc_id)\n",
    "\n",
    "    # 1) <page_id>.<ext>\n",
    "    for ext in exts:\n",
    "        p = os.path.join(doc_dir, f\"{page_id}.{ext}\")\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "\n",
    "    # 2) <doc_id>_<page_id>.<ext>\n",
    "    for ext in exts:\n",
    "        p = os.path.join(doc_dir, f\"{doc_id}_{page_id}.{ext}\")\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "\n",
    "    # --- B) flat in image_root ---\n",
    "    for ext in exts:\n",
    "        p = os.path.join(image_root, f\"{doc_id}_{page_id}.{ext}\")\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "\n",
    "    # --- C) glob fallback (covers weird ext/case) ---\n",
    "    # in doc folder\n",
    "    pats = [\n",
    "        os.path.join(doc_dir, f\"{page_id}.*\"),\n",
    "        os.path.join(doc_dir, f\"{doc_id}_{page_id}.*\"),\n",
    "        os.path.join(image_root, f\"{doc_id}_{page_id}.*\"),\n",
    "    ]\n",
    "    for pat in pats:\n",
    "        hits = glob.glob(pat)\n",
    "        if hits:\n",
    "            # pick the first deterministically (sorted)\n",
    "            hits = sorted(hits)\n",
    "            return hits[0]\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing page image for doc_id={doc_id}, page_id={page_id}. \"\n",
    "        f\"Tried under: {doc_dir} and {image_root}. \"\n",
    "        f\"Example expected: {os.path.join(doc_dir, str(page_id)+'.png')} or {os.path.join(doc_dir, f'{doc_id}_{page_id}.jpg')}\"\n",
    "    )\n",
    "\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "\n",
    "def topo_sort_with_reading_priority(items):\n",
    "    \"\"\"\n",
    "    items: HRDH json list，每个元素至少包含 box/page/parent_id\n",
    "    返回：满足 parent 一定在 child 前的顺序（同时尽量贴近阅读序）\n",
    "    \"\"\"\n",
    "    n = len(items)\n",
    "\n",
    "    # reading priority rank\n",
    "    ranks = []\n",
    "    for i, it in enumerate(items):\n",
    "        x0, y0, x1, y1 = it[\"box\"]\n",
    "        ranks.append((int(it[\"page\"]), float(y0), float(x0), i))\n",
    "\n",
    "    # parent -> child graph\n",
    "    indeg = [0] * n\n",
    "    g = defaultdict(list)\n",
    "    for child in range(n):\n",
    "        p = int(items[child].get(\"parent_id\", -1))\n",
    "        if p < 0:\n",
    "            continue\n",
    "        if 0 <= p < n:\n",
    "            g[p].append(child)\n",
    "            indeg[child] += 1\n",
    "\n",
    "    # Kahn with heap prioritized by reading rank\n",
    "    heap = []\n",
    "    for i in range(n):\n",
    "        if indeg[i] == 0:\n",
    "            heapq.heappush(heap, (ranks[i], i))\n",
    "\n",
    "    order = []\n",
    "    while heap:\n",
    "        _, u = heapq.heappop(heap)\n",
    "        order.append(u)\n",
    "        for v in g[u]:\n",
    "            indeg[v] -= 1\n",
    "            if indeg[v] == 0:\n",
    "                heapq.heappush(heap, (ranks[v], v))\n",
    "\n",
    "    # fallback if cycles/noise exist\n",
    "    if len(order) < n:\n",
    "        remaining = [i for i in range(n) if i not in set(order)]\n",
    "        remaining.sort(key=lambda i: ranks[i])\n",
    "        order.extend(remaining)\n",
    "\n",
    "    return order\n",
    "\n",
    "\n",
    "def load_hrdh_json(json_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    读取单个 HRDH json，返回 doc dict（units + labels + parent + relation），并重映射 parent 到新排序 index。\n",
    "    当前排序：按 (page, y0, x0) 近似阅读顺序（足够用于训练闭环；后续可替换为双栏阅读序）。\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        items = json.load(f)\n",
    "\n",
    "    def sort_key(idx_item):\n",
    "        idx, it = idx_item\n",
    "        x0, y0, x1, y1 = it[\"box\"]\n",
    "        return (int(it[\"page\"]), int(y0), int(x0), idx)\n",
    "\n",
    "    order_old = topo_sort_with_reading_priority(items)\n",
    "    indexed_sorted = [(i, items[i]) for i in order_old]\n",
    "\n",
    "\n",
    "    old2new = {old_i: new_i for new_i, (old_i, _) in enumerate(indexed_sorted)}\n",
    "\n",
    "    units = []\n",
    "    y_parent, y_rel, y_cls, is_meta = [], [], [], []\n",
    "\n",
    "    for new_i, (old_i, it) in enumerate(indexed_sorted):\n",
    "        text = (it.get(\"text\") or \"\").strip()\n",
    "        x0, y0, x1, y1 = it[\"box\"]\n",
    "        page_id = int(it[\"page\"])\n",
    "        cls_raw = it.get(\"class\", \"para\")\n",
    "        rel_raw = it.get(\"relation\", \"connect\")\n",
    "        meta_flag = bool(it.get(\"is_meta\", False))\n",
    "        parent_old = int(it.get(\"parent_id\", -1))\n",
    "        parent_new = -1 if parent_old == -1 else old2new.get(parent_old, -1)\n",
    "\n",
    "        units.append({\n",
    "            \"text\": text,\n",
    "            \"bbox\": (float(x0), float(y0), float(x1), float(y1)),  # pixel bbox\n",
    "            \"page_id\": page_id,\n",
    "            \"order_id\": new_i,\n",
    "            \"class_raw\": cls_raw,\n",
    "        })\n",
    "        y_parent.append(parent_new)\n",
    "        y_rel.append(REL2ID.get(rel_raw, 0))\n",
    "        y_cls.append(map_hrd_class_to_14(cls_raw))\n",
    "        is_meta.append(meta_flag)\n",
    "\n",
    "    doc_id = os.path.basename(json_path).replace(\".json\", \"\")\n",
    "    return {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"json_path\": json_path,\n",
    "        \"units\": units,\n",
    "        \"y_parent\": y_parent,\n",
    "        \"y_rel\": y_rel,\n",
    "        \"y_cls\": y_cls,\n",
    "        \"is_meta\": is_meta,\n",
    "    }\n",
    "\n",
    "\n",
    "class HRDHDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, split: str = \"train\", max_len: int = 512):\n",
    "        \"\"\"\n",
    "        root_dir: .../HRDH\n",
    "        split: train 或 test\n",
    "        max_len: 截断长度（论文常用 512）\n",
    "        \"\"\"\n",
    "        assert split in (\"train\", \"test\")\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.json_dir = os.path.join(root_dir, split)\n",
    "        self.image_root = os.path.join(root_dir, \"images\")  \n",
    "\n",
    "        self.json_paths = sorted(glob.glob(os.path.join(self.json_dir, \"*.json\")))\n",
    "        if not self.json_paths:\n",
    "            raise FileNotFoundError(f\"No json files found in: {self.json_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.json_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        json_path = self.json_paths[idx]\n",
    "        doc = load_hrdh_json(json_path)\n",
    "\n",
    "        # 截断（保持 parent 合法）\n",
    "        if len(doc[\"units\"]) > self.max_len:\n",
    "            keep = self.max_len\n",
    "            doc[\"units\"] = doc[\"units\"][:keep]\n",
    "            doc[\"y_cls\"] = doc[\"y_cls\"][:keep]\n",
    "            doc[\"y_rel\"] = doc[\"y_rel\"][:keep]\n",
    "            doc[\"is_meta\"] = doc[\"is_meta\"][:keep]\n",
    "            # parent: 超出范围的 parent 置为 0；指向被截断的也置 0（或 -1）\n",
    "            y_parent = []\n",
    "            for i, p in enumerate(doc[\"y_parent\"][:keep]):\n",
    "                if p == -1:\n",
    "                    y_parent.append(-1)\n",
    "                elif 0 <= p < keep:\n",
    "                    y_parent.append(p)\n",
    "                else:\n",
    "                    y_parent.append(-1)\n",
    "            doc[\"y_parent\"] = y_parent\n",
    "\n",
    "        # 为每个 unit 找到对应页图路径（按 page_id）\n",
    "        # 注意：同一页只加载一次图片，训练时可以再做缓存/预处理\n",
    "        page_ids = sorted(set(u[\"page_id\"] for u in doc[\"units\"]))\n",
    "        page_images = {}\n",
    "        for pid in page_ids:\n",
    "            page_images[pid] = get_image_path(self.image_root, doc[\"doc_id\"], pid)\n",
    "        doc[\"page_images\"] = page_images\n",
    "        for i, p in enumerate(doc[\"y_parent\"]):\n",
    "            if p >= 0:\n",
    "                assert p < i, f\"parent not causal: i={i}, p={p}\"\n",
    "            if \"is_meta\" in doc:\n",
    "                for i, flag in enumerate(doc[\"is_meta\"]):\n",
    "                    if flag:\n",
    "                        doc[\"y_parent\"][i] = -1  # ROOT\n",
    "        \n",
    "        return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c91fa554",
   "metadata": {},
   "outputs": [],
   "source": [
    "REL3 = {\"connect\": 0, \"contain\": 1, \"equality\": 2}  # 论文三类\n",
    "REL3_INV = {v:k for k,v in REL3.items()}\n",
    "\n",
    "def filter_meta_and_remap(units_raw):\n",
    "    \"\"\"\n",
    "    units_raw: list of dict from json, each has:\n",
    "      text, box, class, page, is_meta, parent_id, relation\n",
    "    return:\n",
    "      units_kept: list of dict with keys {text, box, page_id, cls_name, parent, rel}\n",
    "      old2new: dict old_index -> new_index  (only for kept)\n",
    "    \"\"\"\n",
    "    keep_idx = [i for i,u in enumerate(units_raw) if not u.get(\"is_meta\", False)]\n",
    "    old2new = {old:new for new,old in enumerate(keep_idx)}\n",
    "\n",
    "    units_kept = []\n",
    "    for old_i in keep_idx:\n",
    "        u = units_raw[old_i]\n",
    "        p_old = int(u.get(\"parent_id\", -1))\n",
    "        # 如果 parent 被过滤掉，或本来就是 -1，则设为 ROOT(-1)\n",
    "        p_new = old2new.get(p_old, -1) if p_old >= 0 else -1\n",
    "\n",
    "        rel = u.get(\"relation\", None)\n",
    "        # 过滤后不应该再出现 meta；如果仍然出现，直接跳过该样本或置默认\n",
    "        if rel == \"meta\":\n",
    "            # 这里选择：直接将该单元丢弃（更干净）\n",
    "            continue\n",
    "\n",
    "        if rel not in REL3:\n",
    "            raise ValueError(f\"Unknown relation: {rel}\")\n",
    "\n",
    "        units_kept.append({\n",
    "            \"text\": u.get(\"text\",\"\"),\n",
    "            \"box\": u.get(\"box\"),\n",
    "            \"page_id\": int(u.get(\"page\", 0)),\n",
    "            \"cls_name\": u.get(\"class\"),\n",
    "            \"parent\": p_new,\n",
    "            \"rel\": REL3[rel],\n",
    "        })\n",
    "\n",
    "    return units_kept, old2new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea27de2",
   "metadata": {},
   "source": [
    "## 1. 环境与设备检查\n",
    "\n",
    "- 检查 PyTorch / CUDA 可用性\n",
    "- 设置 device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d2adcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1\n",
      "cuda available: True\n",
      "cuda device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "cuda capability: (8, 9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"cuda capability:\", torch.cuda.get_device_capability(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060880a",
   "metadata": {},
   "source": [
    "## 2. 数据集路径与快速 sanity-check\n",
    "\n",
    "- 指定 HRDH_ROOT\n",
    "- 初始化数据集与样本查看\n",
    "\n",
    "说明：原 Notebook 中此部分出现了两段几乎等价的 quick-check（为保持对齐，这里全部保留）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4a5516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id: 1401.6399\n",
      "num_units: 512\n",
      "page_images sample: [(0, 'C:\\\\Users\\\\tomra\\\\Desktop\\\\PAPER\\\\final OBJ\\\\HRDH\\\\images\\\\1401.6399\\\\0.png'), (1, 'C:\\\\Users\\\\tomra\\\\Desktop\\\\PAPER\\\\final OBJ\\\\HRDH\\\\images\\\\1401.6399\\\\1.png'), (2, 'C:\\\\Users\\\\tomra\\\\Desktop\\\\PAPER\\\\final OBJ\\\\HRDH\\\\images\\\\1401.6399\\\\2.png')]\n"
     ]
    }
   ],
   "source": [
    "HRDH_ROOT = r\"C:\\Users\\tomra\\Desktop\\PAPER\\final OBJ\\HRDH\"\n",
    "ds = HRDHDataset(HRDH_ROOT, split=\"train\", max_len=512)\n",
    "\n",
    "doc = ds[0]\n",
    "print(\"doc_id:\", doc[\"doc_id\"])\n",
    "print(\"num_units:\", len(doc[\"units\"]))\n",
    "print(\"page_images sample:\", list(doc[\"page_images\"].items())[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "089da0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id: 1401.6399\n",
      "num_units: 512\n",
      "pages: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "missing_images: []\n",
      "bad_parent_count: 0 example: []\n",
      "0 p 0 Title par -1 | SIMD Compression and the Intersection\n",
      "1 p 0 Title par -1 | of Sorted Integers\n",
      "2 p 0 Author par -1 | D. Lemire1 *, L. Boytsov2, N. Kurz3\n",
      "3 p 0 Affiliation par -1 | 1LICEF Research Center, TELUQ, Montreal, QC, Canada\n",
      "4 p 0 Affiliation par -1 | 2 Carnegie Mellon University, Pittsburgh, PA USA\n",
      "5 p 0 Affiliation par -1 | 3 Verse Communications, Orinda, CA USA\n",
      "6 p 0 First-Line par -1 | KEY WORDS: performance; measurement; index compression; vector processing\n",
      "7 p 0 Section par -1 | 1. INTRODUCTION\n",
      "8 p 0 First-Line par 7 | An inverted index maps terms to lists of document identifiers. A column index in\n",
      "9 p 0 Para-Line par 8 | might, similarly, map attribute values to row identifiers. Storing all these lis\n"
     ]
    }
   ],
   "source": [
    "HRDH_ROOT = r\"C:\\Users\\tomra\\Desktop\\PAPER\\final OBJ\\HRDH\"\n",
    "\n",
    "ds = HRDHDataset(HRDH_ROOT, split=\"train\", max_len=512)\n",
    "sample = ds[0]\n",
    "\n",
    "print(\"doc_id:\", sample[\"doc_id\"])\n",
    "print(\"num_units:\", len(sample[\"units\"]))\n",
    "print(\"pages:\", sorted(sample[\"page_images\"].keys()))\n",
    "print(\"missing_images:\", [p for p,v in sample[\"page_images\"].items() if v is None][:10])\n",
    "\n",
    "# parent 合法性检查\n",
    "L = len(sample[\"units\"])\n",
    "bad = [(i,p) for i,p in enumerate(sample[\"y_parent\"]) if not (p==-1 or 0<=p<L)]\n",
    "print(\"bad_parent_count:\", len(bad), \"example:\", bad[:5])\n",
    "\n",
    "# 简单预览\n",
    "for i in range(min(10, L)):\n",
    "    u = sample[\"units\"][i]\n",
    "    print(i, \"p\", u[\"page_id\"], ID2LABEL_14[sample[\"y_cls\"][i]], \"par\", sample[\"y_parent\"][i], \"|\", u[\"text\"][:80])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca6bce",
   "metadata": {},
   "source": [
    "## 3. 先验统计：从数据集中估计 $M_{cp}$（Class Prior / Conditional Prior）\n",
    "\n",
    "说明：原 Notebook 内存在两个版本的 `compute_M_cp_from_dataset`（一个较简版、一个带 `defaultdict`）。为保证“一个逻辑都不许少”，两段均保留，按原顺序排列。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e8729e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_cp shape: (15, 14) colsum (should be 1): [1. 1. 1. 1. 1.]\n",
      "saved: M_cp_hrdh.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_M_cp_from_dataset(dataset: HRDHDataset, num_classes: int, pseudo_count: float = 5.0):\n",
    "    \"\"\"\n",
    "    返回 M_cp: shape (num_classes+1, num_classes)\n",
    "    行：parent_class + ROOT\n",
    "    列：child_class\n",
    "    \"\"\"\n",
    "    ROOT = num_classes  # extra row index\n",
    "    counts = np.zeros((num_classes + 1, num_classes), dtype=np.float64)\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        doc = dataset[i]\n",
    "        y_cls = doc[\"y_cls\"]\n",
    "        y_parent = doc[\"y_parent\"]\n",
    "        L = len(y_cls)\n",
    "\n",
    "        for child in range(L):\n",
    "            c = y_cls[child]\n",
    "            p = y_parent[child]\n",
    "            if p == -1:\n",
    "                pc = ROOT\n",
    "            else:\n",
    "                pc = y_cls[p]\n",
    "            counts[pc, c] += 1.0\n",
    "\n",
    "    # additive smoothing per column\n",
    "    counts += pseudo_count\n",
    "    # normalize columns -> probability\n",
    "    col_sum = counts.sum(axis=0, keepdims=True)\n",
    "    M_cp = counts / np.clip(col_sum, 1e-12, None)\n",
    "    return M_cp\n",
    "\n",
    "train_ds = HRDHDataset(HRDH_ROOT, split=\"train\", max_len=512)\n",
    "M_cp = compute_M_cp_from_dataset(train_ds, num_classes=len(ID2LABEL_14), pseudo_count=5.0)\n",
    "print(\"M_cp shape:\", M_cp.shape, \"colsum (should be 1):\", M_cp.sum(axis=0)[:5])\n",
    "\n",
    "np.save(\"M_cp_hrdh.npy\", M_cp)\n",
    "print(\"saved: M_cp_hrdh.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae03824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_cp shape: (15, 14)\n",
      "col sums (first 8): [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Top 25 (parent, child) by COUNT (after smoothing included):\n",
      "00  Para-Line    -> Para-Line     count=288776.0\n",
      "01  First-Line   -> Para-Line     count=57570.0\n",
      "02  First-Line   -> First-Line    count=54224.0\n",
      "03  ROOT         -> Para-Line     count=14250.0\n",
      "04  Section      -> First-Line    count=10697.0\n",
      "05  Section      -> Section       count=10588.0\n",
      "06  ROOT         -> Page-Header   count=6025.0\n",
      "07  ROOT         -> Affiliation   count=3185.0\n",
      "08  ROOT         -> Footnote      count=2470.0\n",
      "09  ROOT         -> Title         count=1762.0\n",
      "10  ROOT         -> Author        count=1641.0\n",
      "11  ROOT         -> Section       count=1005.0\n",
      "12  ROOT         -> First-Line    count=566.0\n",
      "13  Section      -> Para-Line     count=455.0\n",
      "14  Title        -> Title         count=5.0\n",
      "15  Title        -> Author        count=5.0\n",
      "16  Title        -> Mail          count=5.0\n",
      "17  Title        -> Affiliation   count=5.0\n",
      "18  Title        -> Section       count=5.0\n",
      "19  Title        -> First-Line    count=5.0\n",
      "20  Title        -> Para-Line     count=5.0\n",
      "21  Title        -> Equation      count=5.0\n",
      "22  Title        -> Table         count=5.0\n",
      "23  Title        -> Figure        count=5.0\n",
      "24  Title        -> Caption       count=5.0\n",
      "\n",
      "[Child] 0 Title\n",
      "  parent=ROOT          P=0.9618\n",
      "  parent=Title         P=0.0027\n",
      "  parent=Mail          P=0.0027\n",
      "  parent=Author        P=0.0027\n",
      "  parent=Section       P=0.0027\n",
      "  parent=First-Line    P=0.0027\n",
      "\n",
      "[Child] 1 Author\n",
      "  parent=ROOT          P=0.9591\n",
      "  parent=Title         P=0.0029\n",
      "  parent=Mail          P=0.0029\n",
      "  parent=Author        P=0.0029\n",
      "  parent=Section       P=0.0029\n",
      "  parent=First-Line    P=0.0029\n",
      "\n",
      "[Child] 2 Mail\n",
      "  parent=Title         P=0.0667\n",
      "  parent=Author        P=0.0667\n",
      "  parent=Mail          P=0.0667\n",
      "  parent=Affiliation   P=0.0667\n",
      "  parent=Section       P=0.0667\n",
      "  parent=First-Line    P=0.0667\n",
      "\n",
      "[Child] 3 Affiliation\n",
      "  parent=ROOT          P=0.9785\n",
      "  parent=Title         P=0.0015\n",
      "  parent=Mail          P=0.0015\n",
      "  parent=Author        P=0.0015\n",
      "  parent=Section       P=0.0015\n",
      "  parent=First-Line    P=0.0015\n",
      "\n",
      "[Child] 4 Section\n",
      "  parent=Section       P=0.9082\n",
      "  parent=ROOT          P=0.0862\n",
      "  parent=Mail          P=0.0004\n",
      "  parent=Title         P=0.0004\n",
      "  parent=Affiliation   P=0.0004\n",
      "  parent=First-Line    P=0.0004\n",
      "\n",
      "[Child] 5 First-Line\n",
      "  parent=First-Line    P=0.8273\n",
      "  parent=Section       P=0.1632\n",
      "  parent=ROOT          P=0.0086\n",
      "  parent=Title         P=0.0001\n",
      "  parent=Affiliation   P=0.0001\n",
      "  parent=Mail          P=0.0001\n",
      "\n",
      "[Child] 6 Para-Line\n",
      "  parent=Para-Line     P=0.7997\n",
      "  parent=First-Line    P=0.1594\n",
      "  parent=ROOT          P=0.0395\n",
      "  parent=Section       P=0.0013\n",
      "  parent=Affiliation   P=0.0000\n",
      "  parent=Mail          P=0.0000\n",
      "\n",
      "[Child] 7 Equation\n",
      "  parent=Title         P=0.0667\n",
      "  parent=Author        P=0.0667\n",
      "  parent=Mail          P=0.0667\n",
      "  parent=Affiliation   P=0.0667\n",
      "  parent=Section       P=0.0667\n",
      "  parent=First-Line    P=0.0667\n",
      "\n",
      "[Child] 8 Table\n",
      "  parent=Title         P=0.0667\n",
      "  parent=Author        P=0.0667\n",
      "  parent=Mail          P=0.0667\n",
      "  parent=Affiliation   P=0.0667\n",
      "  parent=Section       P=0.0667\n",
      "  parent=First-Line    P=0.0667\n",
      "\n",
      "[Child] 9 Figure\n",
      "  parent=Title         P=0.0667\n",
      "  parent=Author        P=0.0667\n",
      "  parent=Mail          P=0.0667\n",
      "  parent=Affiliation   P=0.0667\n",
      "  parent=Section       P=0.0667\n",
      "  parent=First-Line    P=0.0667\n",
      "\n",
      "[Child] 10 Caption\n",
      "  parent=Title         P=0.0667\n",
      "  parent=Author        P=0.0667\n",
      "  parent=Mail          P=0.0667\n",
      "  parent=Affiliation   P=0.0667\n",
      "  parent=Section       P=0.0667\n",
      "  parent=First-Line    P=0.0667\n",
      "\n",
      "[Child] 11 Page-Footer\n",
      "  parent=Title         P=0.0667\n",
      "  parent=Author        P=0.0667\n",
      "  parent=Mail          P=0.0667\n",
      "  parent=Affiliation   P=0.0667\n",
      "  parent=Section       P=0.0667\n",
      "  parent=First-Line    P=0.0667\n",
      "\n",
      "[Child] 12 Page-Header\n",
      "  parent=ROOT          P=0.9885\n",
      "  parent=Title         P=0.0008\n",
      "  parent=Mail          P=0.0008\n",
      "  parent=Author        P=0.0008\n",
      "  parent=Section       P=0.0008\n",
      "  parent=First-Line    P=0.0008\n",
      "\n",
      "[Child] 13 Footnote\n",
      "  parent=ROOT          P=0.9724\n",
      "  parent=Title         P=0.0020\n",
      "  parent=Mail          P=0.0020\n",
      "  parent=Author        P=0.0020\n",
      "  parent=Section       P=0.0020\n",
      "  parent=First-Line    P=0.0020\n",
      "\n",
      "Saved: M_cp_hrdh.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_M_cp_from_dataset(dataset, num_classes: int, pseudo_count: float = 5.0):\n",
    "    \"\"\"\n",
    "    M_cp shape: (num_classes+1, num_classes)\n",
    "      rows: parent_class + ROOT(row = num_classes)\n",
    "      cols: child_class\n",
    "    Column-normalized: sum over rows for each col = 1\n",
    "    \"\"\"\n",
    "    ROOT = num_classes\n",
    "    counts = np.zeros((num_classes + 1, num_classes), dtype=np.float64)\n",
    "\n",
    "    for di in range(len(dataset)):\n",
    "        doc = dataset[di]\n",
    "        y_cls = doc[\"y_cls\"]\n",
    "        y_parent = doc[\"y_parent\"]\n",
    "        L = len(y_cls)\n",
    "\n",
    "        for child in range(L):\n",
    "            c = int(y_cls[child])\n",
    "            p = int(y_parent[child])\n",
    "            if p == -1:\n",
    "                pc = ROOT\n",
    "            else:\n",
    "                pc = int(y_cls[p])\n",
    "            counts[pc, c] += 1.0\n",
    "\n",
    "    # Additive smoothing\n",
    "    counts += float(pseudo_count)\n",
    "\n",
    "    # Column normalize\n",
    "    col_sum = counts.sum(axis=0, keepdims=True)\n",
    "    M_cp = counts / np.clip(col_sum, 1e-12, None)\n",
    "    return M_cp, counts\n",
    "\n",
    "def top_parent_for_each_child(M_cp: np.ndarray, id2label: list, topk: int = 6):\n",
    "    \"\"\"\n",
    "    For each child class, list top-k parent classes (including ROOT).\n",
    "    \"\"\"\n",
    "    num_classes = len(id2label)\n",
    "    ROOT = num_classes\n",
    "\n",
    "    rows, cols = M_cp.shape\n",
    "    assert rows == num_classes + 1 and cols == num_classes\n",
    "\n",
    "    for child in range(num_classes):\n",
    "        probs = M_cp[:, child]\n",
    "        idxs = np.argsort(-probs)[:topk]\n",
    "        child_name = id2label[child]\n",
    "        print(f\"\\n[Child] {child} {child_name}\")\n",
    "        for r in idxs:\n",
    "            parent_name = \"ROOT\" if r == ROOT else id2label[r]\n",
    "            print(f\"  parent={parent_name:<12s}  P={probs[r]:.4f}\")\n",
    "\n",
    "def top_edges_global(counts: np.ndarray, id2label: list, topn: int = 30):\n",
    "    \"\"\"\n",
    "    Print global most frequent (parent, child) pairs from raw counts (before normalization).\n",
    "    \"\"\"\n",
    "    num_classes = len(id2label)\n",
    "    ROOT = num_classes\n",
    "\n",
    "    flat = []\n",
    "    for pc in range(num_classes + 1):\n",
    "        for cc in range(num_classes):\n",
    "            flat.append((counts[pc, cc], pc, cc))\n",
    "    flat.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    print(f\"\\nTop {topn} (parent, child) by COUNT (after smoothing included):\")\n",
    "    for k in range(topn):\n",
    "        cnt, pc, cc = flat[k]\n",
    "        parent_name = \"ROOT\" if pc == ROOT else id2label[pc]\n",
    "        child_name = id2label[cc]\n",
    "        print(f\"{k:02d}  {parent_name:<12s} -> {child_name:<12s}  count={cnt:.1f}\")\n",
    "\n",
    "# ==== run ====\n",
    "HRDH_ROOT = r\"C:\\Users\\tomra\\Desktop\\PAPER\\final OBJ\\HRDH\"\n",
    "train_ds = HRDHDataset(HRDH_ROOT, split=\"train\", max_len=512)\n",
    "\n",
    "M_cp, counts = compute_M_cp_from_dataset(train_ds, num_classes=len(ID2LABEL_14), pseudo_count=5.0)\n",
    "\n",
    "print(\"M_cp shape:\", M_cp.shape)\n",
    "print(\"col sums (first 8):\", np.round(M_cp.sum(axis=0)[:8], 6))\n",
    "\n",
    "# sanity: should be ~1.0\n",
    "assert np.allclose(M_cp.sum(axis=0), 1.0, atol=1e-6), \"Column sums not 1.0\"\n",
    "\n",
    "top_edges_global(counts, ID2LABEL_14, topn=25)\n",
    "top_parent_for_each_child(M_cp, ID2LABEL_14, topk=6)\n",
    "\n",
    "np.save(\"M_cp_hrdh.npy\", M_cp)\n",
    "print(\"\\nSaved: M_cp_hrdh.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ce62e",
   "metadata": {},
   "source": [
    "## 4. 训练复现性与全局配置\n",
    "\n",
    "- 随机种子\n",
    "- CFG 超参集中管理\n",
    "- bbox 归一化等通用函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1558130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c858ee09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edc7f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CFG:\n",
    "    # data\n",
    "    max_len: int = 512\n",
    "    batch_size: int = 1          # 强烈建议 doc-level batch=1（结构任务依赖整篇文档序列）\n",
    "    num_workers: int = 2\n",
    "\n",
    "    # model dims\n",
    "    d_model: int = 256\n",
    "    nhead: int = 8\n",
    "    num_layers: int = 4\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # embedding vocab sizes\n",
    "    max_1d_pos: int = 512\n",
    "    max_pages: int = 32          # 超过则截断或 clamp\n",
    "    layout_bins: int = 1001      # bbox 归一化到 [0,1000]\n",
    "    \n",
    "    # visual\n",
    "    vis_crop_size: int = 224\n",
    "    vis_out_dim: int = 256       # 视觉向量投影到 d_model\n",
    "    \n",
    "    # losses\n",
    "    alpha_parent: float = 1.0\n",
    "    alpha_rel: float = 1.0\n",
    "    focal_gamma: float = 2.0\n",
    "    focal_alpha: float = 0.25\n",
    "\n",
    "    # optim\n",
    "    lr: float = 2e-4\n",
    "    weight_decay: float = 1e-2\n",
    "    epochs: int = 10\n",
    "    \n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39e2c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_box_xyxy(box, page_w, page_h, bins=1000):\n",
    "    \"\"\"\n",
    "    输入：像素坐标 [x0,y0,x1,y1]\n",
    "    输出：整数归一化到 [0,bins]\n",
    "    \"\"\"\n",
    "    x0, y0, x1, y1 = box\n",
    "    x0 = int(np.clip(round(x0 / max(page_w, 1) * bins), 0, bins))\n",
    "    x1 = int(np.clip(round(x1 / max(page_w, 1) * bins), 0, bins))\n",
    "    y0 = int(np.clip(round(y0 / max(page_h, 1) * bins), 0, bins))\n",
    "    y1 = int(np.clip(round(y1 / max(page_h, 1) * bins), 0, bins))\n",
    "    return [x0, y0, x1, y1]\n",
    "\n",
    "\n",
    "def collate_doc(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    assert len(batch) == 1, \"当前实现是 doc-level batch=1\"\n",
    "    return batch[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b68879",
   "metadata": {},
   "source": [
    "## 5. 特征编码模块\n",
    "\n",
    "- 文本：SBERT/Transformer 句向量 + 投影\n",
    "- 布局：位置/页面 embedding\n",
    "- 视觉：图像 crop embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59ce0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBERTTextEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    用 sentence-transformers 得到每个 unit 的句向量，然后投影到 d_model。\n",
    "    - 支持简单的 dict 缓存：key=(doc_id, unit_idx)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        self._sbert = None\n",
    "        self.proj = nn.Linear(384, d_model)  # all-MiniLM-L6-v2 输出 384\n",
    "        self.cache: Dict[Tuple[str, int], torch.Tensor] = {}\n",
    "\n",
    "    def _lazy_load(self):\n",
    "        if self._sbert is None:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self._sbert = SentenceTransformer(self.model_name)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_texts(self, texts: List[str]) -> torch.Tensor:\n",
    "        self._lazy_load()\n",
    "        emb = self._sbert.encode(texts, convert_to_tensor=True, show_progress_bar=False)  # (L,384)\n",
    "        return emb\n",
    "\n",
    "    def forward(self, doc_id: str, texts: List[str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        返回 (L, d_model)，保证在 self.proj.weight.device 上\n",
    "        cache 存 CPU，取用时搬到目标 device\n",
    "        \"\"\"\n",
    "        device = self.proj.weight.device\n",
    "        L = len(texts)\n",
    "        out = [None] * L\n",
    "        missing_idx, missing_text = [], []\n",
    "\n",
    "        # 1) cache hit：取 CPU -> to(device)\n",
    "        for i, t in enumerate(texts):\n",
    "            key = (doc_id, i)\n",
    "            if key in self.cache:\n",
    "                out[i] = self.cache[key].to(device)\n",
    "            else:\n",
    "                missing_idx.append(i)\n",
    "                missing_text.append(t)\n",
    "\n",
    "        # 2) cache miss：SBERT encode -> detach/clone -> proj -> 存 CPU\n",
    "        if len(missing_idx) > 0:\n",
    "            emb_384 = self.encode_texts(missing_text)          # 可能是 inference tensor\n",
    "            emb_384 = emb_384.detach().clone().to(device)      # 关键：变普通 tensor + 上 GPU\n",
    "            emb_d = self.proj(emb_384)                         # (M,d_model) on GPU\n",
    "\n",
    "            for k, i in enumerate(missing_idx):\n",
    "                key = (doc_id, i)\n",
    "                self.cache[key] = emb_d[k].detach().cpu()      # cache 存 CPU，省显存\n",
    "                out[i] = emb_d[k]                              # 当前 batch 直接用 GPU tensor\n",
    "\n",
    "        return torch.stack(out, dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5a85339",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayoutPosPageEmbedder(nn.Module):\n",
    "    def __init__(self, d_model: int, layout_bins: int = 1001, max_pos: int = 512, max_pages: int = 32):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layout_bins = layout_bins\n",
    "\n",
    "        # LayoutLMv2 风格：x0,x1,w 与 y0,y1,h，各自 embedding 再 concat -> proj\n",
    "        self.emb_x0 = nn.Embedding(layout_bins, d_model//4)\n",
    "        self.emb_x1 = nn.Embedding(layout_bins, d_model//4)\n",
    "        self.emb_w  = nn.Embedding(layout_bins, d_model//4)\n",
    "        self.emb_y0 = nn.Embedding(layout_bins, d_model//4)\n",
    "        self.emb_y1 = nn.Embedding(layout_bins, d_model//4)\n",
    "        self.emb_h  = nn.Embedding(layout_bins, d_model//4)\n",
    "\n",
    "        self.proj_layout = nn.Linear((d_model//4)*6, d_model)\n",
    "\n",
    "        self.emb_pos = nn.Embedding(max_pos, d_model)\n",
    "        self.emb_page = nn.Embedding(max_pages, d_model)\n",
    "\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, units: List[Dict[str, Any]], page_images: Dict[int, str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        return: (L, d_model)\n",
    "        \"\"\"\n",
    "        L = len(units)\n",
    "        # 逐页拿宽高\n",
    "        page_wh = {}\n",
    "        for pid, pth in page_images.items():\n",
    "            img = Image.open(pth)\n",
    "            page_wh[int(pid)] = (img.width, img.height)\n",
    "\n",
    "        layout_vecs = []\n",
    "        pos_ids = []\n",
    "        page_ids = []\n",
    "\n",
    "        for i,u in enumerate(units):\n",
    "            pid = int(u[\"page_id\"])\n",
    "            w, h = page_wh[pid]\n",
    "            b = u.get(\"box\", None)\n",
    "            if b is None:\n",
    "                b = u.get(\"bbox\", None)\n",
    "            if b is None:\n",
    "                raise KeyError(\"Unit missing both 'box' and 'bbox'\")\n",
    "            x0, y0, x1, y1 = b\n",
    "\n",
    "            nb = normalize_box_xyxy([x0,y0,x1,y1], w, h, bins=self.layout_bins-1)\n",
    "            nx0, ny0, nx1, ny1 = nb\n",
    "            nw = int(np.clip(nx1 - nx0, 0, self.layout_bins-1))\n",
    "            nh = int(np.clip(ny1 - ny0, 0, self.layout_bins-1))\n",
    "\n",
    "            layout_vecs.append([nx0, nx1, nw, ny0, ny1, nh])\n",
    "            pos_ids.append(i)\n",
    "            page_ids.append(min(pid, self.emb_page.num_embeddings-1))\n",
    "\n",
    "        layout = torch.tensor(layout_vecs, dtype=torch.long, device=self.emb_pos.weight.device)  # (L,6)\n",
    "        pos = torch.tensor(pos_ids, dtype=torch.long, device=self.emb_pos.weight.device)        # (L,)\n",
    "        pages = torch.tensor(page_ids, dtype=torch.long, device=self.emb_pos.weight.device)     # (L,)\n",
    "\n",
    "        x0 = self.emb_x0(layout[:,0])\n",
    "        x1 = self.emb_x1(layout[:,1])\n",
    "        ww = self.emb_w(layout[:,2])\n",
    "        y0 = self.emb_y0(layout[:,3])\n",
    "        y1 = self.emb_y1(layout[:,4])\n",
    "        hh = self.emb_h(layout[:,5])\n",
    "        layout_cat = torch.cat([x0,x1,ww,y0,y1,hh], dim=-1)\n",
    "        layout_emb = self.proj_layout(layout_cat)\n",
    "\n",
    "        pos_emb = self.emb_pos(pos)\n",
    "        page_emb = self.emb_page(pages)\n",
    "\n",
    "        out = self.ln(layout_emb + pos_emb + page_emb)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3e51dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import roi_align\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "\n",
    "class VisualFPNRoIEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper-style visual embedding:\n",
    "    page image -> ResNet50+FPN -> RoIAlign by bbox -> pooled -> Linear -> (L, d_model)\n",
    "\n",
    "    Interface kept identical to your existing VisualCropEmbedder:\n",
    "      forward(units, page_images) -> Tensor[L, d_model]\n",
    "    where:\n",
    "      - units: list of dict, each must have:\n",
    "          u[\"page_id\"] : int\n",
    "          u[\"box\"]     : [x0, y0, x1, y1] in *pixel coords of the original page image*\n",
    "      - page_images: dict or list-like indexed by page_id -> image path\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        roi_out_size: int = 7,     # RoIAlign output spatial size (7x7 typical)\n",
    "        roi_sampling_ratio: int = 2,\n",
    "        fpn_level: str = \"0\",      # torchvision FPN returns keys like \"0\",\"1\",\"2\",\"3\",\"pool\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.roi_out_size = roi_out_size\n",
    "        self.roi_sampling_ratio = roi_sampling_ratio\n",
    "        self.fpn_level = fpn_level\n",
    "\n",
    "        # ResNet50 + FPN backbone (detection-style)\n",
    "        # returns dict of feature maps: {\"0\":P2,\"1\":P3,\"2\":P4,\"3\":P5,\"pool\":P6}\n",
    "        self.backbone = resnet_fpn_backbone(\n",
    "            backbone_name=\"resnet50\",\n",
    "            weights=torchvision.models.ResNet50_Weights.DEFAULT,\n",
    "            trainable_layers=3,   # 可按需调；最小化侵入先这么设\n",
    "        )\n",
    "\n",
    "        # For resnet_fpn_backbone, each pyramid level channel is 256\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.proj = nn.Linear(256, d_model)\n",
    "\n",
    "        # ImageNet normalization\n",
    "        w = torchvision.models.ResNet50_Weights.DEFAULT\n",
    "        mean, std = w.transforms().mean, w.transforms().std\n",
    "        self.tf = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "\n",
    "    def _load_page(self, page_path: str):\n",
    "        img = Image.open(page_path).convert(\"RGB\")\n",
    "        return img\n",
    "\n",
    "    def _to_tensor(self, pil_img: Image.Image, device: torch.device):\n",
    "        x = self.tf(pil_img).unsqueeze(0).to(device)  # (1,3,H,W)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _boxes_to_roi_format(boxes_xyxy, batch_idx: int = 0, device=None):\n",
    "        \"\"\"\n",
    "        roi_align expects boxes as Tensor[K,5] = (batch_idx, x1, y1, x2, y2)\n",
    "        in input image pixel coordinates when spatial_scale is set properly.\n",
    "        \"\"\"\n",
    "        b = torch.tensor(boxes_xyxy, dtype=torch.float32, device=device)\n",
    "        if b.numel() == 0:\n",
    "            return b.new_zeros((0, 5))\n",
    "        idx = torch.full((b.shape[0], 1), float(batch_idx), device=device)\n",
    "        return torch.cat([idx, b], dim=1)\n",
    "\n",
    "    def forward(self, units, page_images):\n",
    "        device = self.proj.weight.device\n",
    "\n",
    "        # group unit indices by page_id (so each page runs backbone once)\n",
    "        page_to_indices = {}\n",
    "        for i, u in enumerate(units):\n",
    "            pid = int(u[\"page_id\"])\n",
    "            page_to_indices.setdefault(pid, []).append(i)\n",
    "\n",
    "        out = torch.zeros((len(units), self.d_model), device=device)\n",
    "\n",
    "        for pid, idxs in page_to_indices.items():\n",
    "            page_path = page_images[pid]\n",
    "            pil_img = self._load_page(page_path)\n",
    "            W, H = pil_img.size\n",
    "\n",
    "            x = self._to_tensor(pil_img, device=device)  # (1,3,H,W)\n",
    "\n",
    "            feats = self.backbone(x)  # dict of feature maps\n",
    "            if self.fpn_level not in feats:\n",
    "                # fallback: use the highest-resolution level if key missing\n",
    "                # typical keys: \"0\",\"1\",\"2\",\"3\",\"pool\"\n",
    "                key = sorted([k for k in feats.keys() if k != \"pool\"])[0]\n",
    "            else:\n",
    "                key = self.fpn_level\n",
    "\n",
    "            fmap = feats[key]  # (1,256,hf,wf)\n",
    "            hf, wf = fmap.shape[-2], fmap.shape[-1]\n",
    "\n",
    "            # spatial_scale maps original image coords -> feature map coords\n",
    "            # roi_align uses a single scalar; assume isotropic scaling (works because fmap came from that image)\n",
    "            spatial_scale = wf / float(W)\n",
    "\n",
    "            boxes_xyxy = [units[i][\"box\"] for i in idxs]  # list of [x0,y0,x1,y1] in image pixels\n",
    "            rois = self._boxes_to_roi_format(boxes_xyxy, batch_idx=0, device=device)  # (K,5)\n",
    "\n",
    "            roi_feat = roi_align(\n",
    "                input=fmap,\n",
    "                boxes=rois,\n",
    "                output_size=(self.roi_out_size, self.roi_out_size),\n",
    "                spatial_scale=spatial_scale,\n",
    "                sampling_ratio=self.roi_sampling_ratio,\n",
    "                aligned=True,\n",
    "            )  # (K,256,roi,roi)\n",
    "\n",
    "            roi_feat = self.pool(roi_feat).flatten(1)  # (K,256)\n",
    "            emb = self.proj(roi_feat)                  # (K,d_model)\n",
    "\n",
    "            out[idxs] = emb\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def81414",
   "metadata": {},
   "source": [
    "## 6. DSPS 模型主体\n",
    "\n",
    "- 多模态编码\n",
    "- 单元分类（node class）\n",
    "- 关系分类（edge/rel）\n",
    "- 论文中的 DSPS 思路在此实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2af112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSPSModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        num_rel: int,\n",
    "        M_cp: np.ndarray,\n",
    "        cfg: CFG,\n",
    "        use_text: bool = False,\n",
    "        use_visual: bool = True,\n",
    "        use_softmask: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_softmask = use_softmask\n",
    "        self.num_classes = num_classes\n",
    "        self.num_rel = num_rel\n",
    "        self.cfg = cfg\n",
    "        self.use_text = use_text\n",
    "        self.use_visual = use_visual\n",
    "\n",
    "        # embeddings\n",
    "        self.layout_pos_page = LayoutPosPageEmbedder(\n",
    "            d_model=cfg.d_model,\n",
    "            layout_bins=cfg.layout_bins,\n",
    "            max_pos=cfg.max_1d_pos,\n",
    "            max_pages=cfg.max_pages,\n",
    "        )\n",
    "\n",
    "        if use_text:\n",
    "            self.text_emb = SBERTTextEmbedder(d_model=cfg.d_model)\n",
    "        else:\n",
    "            self.text_emb = None\n",
    "\n",
    "        if use_visual:\n",
    "            self.vis_emb = VisualFPNRoIEmbedder(d_model=cfg.d_model)\n",
    "        else:\n",
    "            self.vis_emb = None\n",
    "\n",
    "        self.fuse_ln = nn.LayerNorm(cfg.d_model)\n",
    "\n",
    "        # encoder (bidirectional)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=cfg.d_model,\n",
    "            nhead=cfg.nhead,\n",
    "            dim_feedforward=cfg.d_model * 4,\n",
    "            dropout=cfg.dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=cfg.num_layers)\n",
    "\n",
    "        # subtask1: class\n",
    "        self.cls_head = nn.Linear(cfg.d_model, num_classes)\n",
    "\n",
    "        # decoder: structure-aware GRU\n",
    "        self.gru = nn.GRU(input_size=cfg.d_model, hidden_size=cfg.d_model, batch_first=True)\n",
    "\n",
    "        # attention projections for parent finding\n",
    "        self.Wq = nn.Linear(cfg.d_model, cfg.d_model, bias=False)\n",
    "        self.Wk = nn.Linear(cfg.d_model, cfg.d_model, bias=False)\n",
    "\n",
    "        # relation head (concat)\n",
    "        self.rel_head = nn.Sequential(\n",
    "            nn.Linear(cfg.d_model * 2, cfg.d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "            nn.Linear(cfg.d_model, num_rel),\n",
    "        )\n",
    "\n",
    "        # store M_cp (torch)\n",
    "        # shape: (num_classes+1, num_classes)   rows=parent_class + ROOT(row=num_classes), cols=child_class\n",
    "        M = torch.tensor(M_cp, dtype=torch.float32)\n",
    "        self.register_buffer(\"M_cp\", M)\n",
    "\n",
    "    def forward(self, doc: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        doc keys from your dataset:\n",
    "          doc_id, units(list), y_cls(list), y_parent(list), y_rel(list), page_images(dict)\n",
    "        returns logits:\n",
    "          cls_logits: (L,C)\n",
    "          par_logits: list of length L where par_logits[i] is (i,) logits for j in [0..i-1] + ROOT at index 0?\n",
    "          rel_logits: (L, R) using GT parent during training convenience (可在 loss 里选)\n",
    "        \"\"\"\n",
    "        units = doc[\"units\"]\n",
    "        L = len(units)\n",
    "        doc_id = doc[\"doc_id\"]\n",
    "        page_images = doc[\"page_images\"]\n",
    "\n",
    "        # ---- embeddings sum: text + visual + layout/pos/page ----\n",
    "        x = self.layout_pos_page(units, page_images)  # (L,d)\n",
    "\n",
    "        if self.use_text:\n",
    "            texts = [u.get(\"text\",\"\") for u in units]\n",
    "            x = x + self.text_emb(doc_id, texts)\n",
    "\n",
    "        if self.use_visual:\n",
    "            x = x + self.vis_emb(units, page_images)\n",
    "\n",
    "        x = self.fuse_ln(x)            # (L,d)\n",
    "        x = x.unsqueeze(0)             # (1,L,d) for transformer batch_first\n",
    "\n",
    "        # ---- encoder ----\n",
    "        x_star = self.encoder(x)       # (1,L,d)\n",
    "        x_star = x_star.squeeze(0)     # (L,d)\n",
    "\n",
    "        # ---- class logits ----\n",
    "        cls_logits = self.cls_head(x_star)  # (L,C)\n",
    "        cls_prob = F.softmax(cls_logits, dim=-1)  # (L,C)\n",
    "\n",
    "        # ---- ROOT representation ----\n",
    "        root = x_star.mean(dim=0, keepdim=True)  # (1,d)\n",
    "\n",
    "        # ---- GRU decoder (causal) ----\n",
    "        # 论文写用 x*_{i-1} 驱动，这里用全序列输入 + 自己取 h_i（等价实现）\n",
    "        h_seq, _ = self.gru(x_star.unsqueeze(0))  # (1,L,d)\n",
    "        h_seq = h_seq.squeeze(0)                  # (L,d)\n",
    "\n",
    "        # ---- parent logits with soft-mask ----\n",
    "        # 我们把 candidate set 定义为 [ROOT] + [0..i-1]\n",
    "        # 输出 par_logits[i] 形状 (i+1,) 其中 index0=ROOT, index k>0 对应 parent=j=k-1\n",
    "        par_logits = []\n",
    "        eps = 1e-8\n",
    "\n",
    "        # 预先准备 ROOT 的 class distribution：用 uniform 或者用 mean prob；论文是扩展 P_cls(0) 为 (C+1)，ROOT=1\n",
    "        # 我这里做：P_cls_root_over_parentclass = one-hot at ROOT row\n",
    "        # 计算 P_dom 时使用 rows=parentclass+ROOT\n",
    "        # 具体：P̃_cls(j) = [P_cls(j), 0] for real nodes；ROOT 用 [0..0,1]\n",
    "        root_one = torch.zeros((1, self.num_classes + 1), device=x_star.device)\n",
    "        root_one[0, self.num_classes] = 1.0\n",
    "\n",
    "        # child prob 扩成 (C) 即原本；parent prob 扩成 (C+1)\n",
    "        # 对每个 i:\n",
    "        for i in range(L):\n",
    "            q = self.Wq(h_seq[i:i+1])                # (1,d)\n",
    "            # keys = [ROOT] + past h\n",
    "            k_root = self.Wk(root)                   # (1,d)\n",
    "            if i == 0:\n",
    "                keys = k_root                        # (1,d)\n",
    "            else:\n",
    "                k_past = self.Wk(h_seq[:i])          # (i,d)\n",
    "                keys = torch.cat([k_root, k_past], dim=0)  # (i+1,d)\n",
    "\n",
    "            # dot-product scores\n",
    "            score = (q @ keys.t()).squeeze(0)        # (i+1,)\n",
    "\n",
    "            # ----- soft-mask prior: P_dom(i, j) -----\n",
    "            # child distribution: (C)\n",
    "            p_child = cls_prob[i:i+1]                # (1,C)\n",
    "            # parent distributions:\n",
    "            if i == 0:\n",
    "                p_parent_ext = root_one              # (1,C+1)\n",
    "            else:\n",
    "                p_parent = cls_prob[:i]              # (i,C)\n",
    "                zeros = torch.zeros((i,1), device=x_star.device)\n",
    "                p_parent_ext = torch.cat([p_parent, zeros], dim=1)  # (i,C+1)\n",
    "                p_parent_ext = torch.cat([root_one, p_parent_ext], dim=0)  # (i+1,C+1)\n",
    "\n",
    "            # P_dom = p_parent_ext @ M_cp @ p_child^T\n",
    "            # M_cp: (C+1,C)\n",
    "            prior = (p_parent_ext @ self.M_cp @ p_child.t()).squeeze(-1)  # (i+1,)\n",
    "\n",
    "            if self.use_softmask:\n",
    "                score = score + torch.log(prior + eps)\n",
    "\n",
    "            par_logits.append(score)\n",
    "\n",
    "        # ---- relation logits (用 GT parent 保持训练稳定；推理时再用预测 parent) ----\n",
    "        # 这里先输出一个 (L,R) 的 logits，其中第 i 个是与 GT parent 的关系\n",
    "        # 对于 parent=-1 的（ROOT），relation 按你的数据里常见是 \"contain\"/\"meta\"，这里仍然算一个 rel loss（你也可 mask 掉）\n",
    "        y_parent = doc.get(\"y_parent\", [-1]*L)\n",
    "        rel_logits = []\n",
    "        for i in range(L):\n",
    "            p = y_parent[i]\n",
    "            if p is None or p < 0:\n",
    "                # ROOT\n",
    "                parent_vec = root.squeeze(0)\n",
    "            else:\n",
    "                parent_vec = h_seq[p]\n",
    "            feat = torch.cat([h_seq[i], parent_vec], dim=-1)\n",
    "            rel_logits.append(self.rel_head(feat))\n",
    "        rel_logits = torch.stack(rel_logits, dim=0)  # (L,R)\n",
    "\n",
    "        return {\n",
    "            \"cls_logits\": cls_logits,\n",
    "            \"par_logits\": par_logits,  # list of tensors\n",
    "            \"rel_logits\": rel_logits,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9745d4e",
   "metadata": {},
   "source": [
    "## 7. 损失函数与训练目标\n",
    "\n",
    "- FocalLoss\n",
    "- compute_losses：对分类/关系等分项计算并聚合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eca9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=0.25, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        logits: (N,C)\n",
    "        targets: (N,)\n",
    "        \"\"\"\n",
    "        ce = F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-ce)\n",
    "        focal = self.alpha * (1 - pt) ** self.gamma * ce\n",
    "        if self.reduction == \"mean\":\n",
    "            return focal.mean()\n",
    "        if self.reduction == \"sum\":\n",
    "            return focal.sum()\n",
    "        return focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "48f004bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses(\n",
    "    out: Dict[str, Any],\n",
    "    doc: Dict[str, Any],\n",
    "    num_classes: int,\n",
    "    num_rel: int,\n",
    "    cfg: CFG,\n",
    "    focal_cls: nn.Module,\n",
    "    focal_rel: nn.Module,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "\n",
    "    # ---- targets ----\n",
    "    y_cls = torch.tensor(doc[\"y_cls\"], dtype=torch.long, device=device)\n",
    "    y_parent = doc[\"y_parent\"]\n",
    "    y_rel = torch.tensor(doc[\"y_rel\"], dtype=torch.long, device=device)\n",
    "\n",
    "    # ---- logits ----\n",
    "    cls_logits = out[\"cls_logits\"].to(device)   # (L,C)\n",
    "    rel_logits = out[\"rel_logits\"].to(device)   # (L,R)\n",
    "    par_logits_list = out[\"par_logits\"]         # list length L\n",
    "\n",
    "    L = len(par_logits_list)\n",
    "\n",
    "    # ========== sanity checks ==========\n",
    "    # cls range\n",
    "    cls_min = int(y_cls.min().item())\n",
    "    cls_max = int(y_cls.max().item())\n",
    "    if cls_min < 0 or cls_max >= num_classes:\n",
    "        raise ValueError(f\"y_cls out of range: min={cls_min}, max={cls_max}, num_classes={num_classes}\")\n",
    "\n",
    "    # rel range\n",
    "    rel_min = int(y_rel.min().item())\n",
    "    rel_max = int(y_rel.max().item())\n",
    "    if rel_min < 0 or rel_max >= num_rel:\n",
    "        raise ValueError(f\"y_rel out of range: min={rel_min}, max={rel_max}, num_rel={num_rel}\")\n",
    "\n",
    "    # parent range per position\n",
    "    for i in range(L):\n",
    "        p = y_parent[i]\n",
    "        if p is None:\n",
    "            continue\n",
    "        if p >= i:  # 注意：parent 必须 < i（只能指向过去）\n",
    "            raise ValueError(f\"y_parent invalid at i={i}: parent={p} but must be < {i}\")\n",
    "        if p < -1:\n",
    "            raise ValueError(f\"y_parent invalid at i={i}: parent={p} (should be -1 or >=0)\")\n",
    "\n",
    "    # ========== losses ==========\n",
    "    # meta mask：True 表示参与结构监督的单元（非 meta）\n",
    "    is_meta = torch.tensor(doc.get(\"is_meta\", [False]*L), dtype=torch.bool, device=device)\n",
    "    struct_mask = ~is_meta  # (L,)\n",
    "\n",
    "    # Subtask1: class（是否 mask meta 看你后续要不要对齐论文；先不 mask，保证能跑通）\n",
    "    loss_cls = focal_cls(cls_logits, y_cls)\n",
    "\n",
    "    # Subtask2: parent（只对非 meta 计算）\n",
    "    loss_par = 0.0\n",
    "    denom_par = 0\n",
    "    for i, logits_i in enumerate(par_logits_list):\n",
    "        if not bool(struct_mask[i].item()):\n",
    "            continue\n",
    "\n",
    "        p = y_parent[i]\n",
    "        tgt_i = 0 if (p is None or p < 0) else (p + 1)\n",
    "\n",
    "        if tgt_i < 0 or tgt_i >= logits_i.numel():\n",
    "            raise ValueError(\n",
    "                f\"parent target out of range at i={i}: tgt={tgt_i}, \"\n",
    "                f\"logits_len={logits_i.numel()}, raw_parent={p}\"\n",
    "            )\n",
    "\n",
    "        tgt = torch.tensor([tgt_i], dtype=torch.long, device=device)\n",
    "        # logits_i 可能是 shape=(K,) 或 (1,K)。统一压成 (1,K)\n",
    "        logits_ce = logits_i.to(device).reshape(1, -1)\n",
    "        loss_par = loss_par + F.cross_entropy(logits_ce, tgt)\n",
    "\n",
    "        denom_par += 1\n",
    "\n",
    "    if denom_par == 0:\n",
    "        loss_par = torch.zeros((), device=device)\n",
    "    else:\n",
    "        loss_par = loss_par / denom_par\n",
    "\n",
    "    # Subtask3: relation（只对非 meta 计算）\n",
    "    if struct_mask.any():\n",
    "        loss_rel = focal_rel(rel_logits[struct_mask], y_rel[struct_mask])\n",
    "    else:\n",
    "        loss_rel = torch.zeros((), device=device)\n",
    "\n",
    "    loss = loss_cls + cfg.alpha_parent * loss_par + cfg.alpha_rel * loss_rel\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"loss_cls\": loss_cls.detach(),\n",
    "        \"loss_par\": loss_par.detach(),\n",
    "        \"loss_rel\": loss_rel.detach(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49b541",
   "metadata": {},
   "source": [
    "## 8. 训练循环\n",
    "\n",
    "- train_one_epoch：单 epoch 训练\n",
    "- DataLoader 构建\n",
    "- 优化器与训练入口\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "360be9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, cfg, focal_cls, focal_rel):\n",
    "    model.train()\n",
    "    logs = {\"loss\": [], \"loss_cls\": [], \"loss_par\": [], \"loss_rel\": []}\n",
    "\n",
    "    start_time = time.time()\n",
    "    last_print = start_time\n",
    "\n",
    "    print(f\"\\n[Train] start epoch, num_docs = {len(loader)}\")\n",
    "\n",
    "    for step, doc in enumerate(loader):\n",
    "        t0 = time.time()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # ===== forward =====\n",
    "        out = model(doc)\n",
    "\n",
    "        # ===== loss =====\n",
    "        losses = compute_losses(out, doc, model.num_classes, model.num_rel,\n",
    "                                cfg, focal_cls, focal_rel)\n",
    "\n",
    "        # ===== backward =====\n",
    "        losses[\"loss\"].backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # ===== log =====\n",
    "        for k in logs:\n",
    "            logs[k].append(float(losses[k].cpu()))\n",
    "\n",
    "        # ===== progress print =====\n",
    "        if step == 0 or (step + 1) % 5 == 0:\n",
    "            now = time.time()\n",
    "            step_time = now - t0\n",
    "            elapsed = now - start_time\n",
    "\n",
    "            print(\n",
    "                f\"  step {step+1:4d}/{len(loader)} | \"\n",
    "                f\"step_time={step_time:5.2f}s | \"\n",
    "                f\"loss={logs['loss'][-1]:.4f} \"\n",
    "                f\"(cls={logs['loss_cls'][-1]:.3f}, \"\n",
    "                f\"par={logs['loss_par'][-1]:.3f}, \"\n",
    "                f\"rel={logs['loss_rel'][-1]:.3f}) | \"\n",
    "                f\"elapsed={elapsed/60:.1f}min\"\n",
    "            )\n",
    "\n",
    "        # ===== quick sanity check (只在最前面几步) =====\n",
    "        if step < 2:\n",
    "            L = len(doc[\"units\"])\n",
    "            print(f\"    sanity: seq_len={L}, \"\n",
    "                  f\"avg_parent_candidates={sum(len(x) for x in out['par_logits'])/L:.1f}\")\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"[Train] epoch done in {epoch_time/60:.2f} min\")\n",
    "\n",
    "    return {k: sum(v)/max(len(v),1) for k,v in logs.items()}\n",
    "\n",
    "def eval_one_epoch(model, loader, cfg, focal_cls, focal_rel):\n",
    "    model.eval()\n",
    "    logs = {\"loss\": [], \"loss_cls\": [], \"loss_par\": [], \"loss_rel\": []}\n",
    "\n",
    "    for doc in loader:\n",
    "        out = model(doc)\n",
    "        losses = compute_losses(out, doc, model.num_classes, model.num_rel, cfg, focal_cls, focal_rel)\n",
    "        for k in logs:\n",
    "            logs[k].append(float(losses[k].cpu()))\n",
    "    return {k: sum(v)/max(len(v),1) for k,v in logs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acbe49ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "# ===== dataset =====\n",
    "train_ds = HRDHDataset(HRDH_ROOT, split=\"train\", max_len=cfg.max_len)\n",
    "test_ds  = HRDHDataset(HRDH_ROOT, split=\"test\",  max_len=cfg.max_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_doc,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_doc,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "\n",
    "# ===== M_cp =====\n",
    "M_cp = np.load(\"M_cp_hrdh.npy\")  # shape (C+1,C)\n",
    "\n",
    "# ===== model =====\n",
    "num_classes = len(ID2LABEL_14)\n",
    "num_rel = len(REL2ID)  # 你的 REL2ID 包含 meta；如果你想只做三类，把 meta 从标签里移除并重映射\n",
    "\n",
    "model = DSPSModel(\n",
    "    num_classes=num_classes,\n",
    "    num_rel=num_rel,\n",
    "    M_cp=M_cp,\n",
    "    cfg=cfg,\n",
    "    use_text=False,      # 如果本地没 sentence-transformers 可先设 False\n",
    "    use_visual=False,    # 如果显存吃紧可先 False\n",
    ").to(device)\n",
    "\n",
    "# ===== optim =====\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "focal_cls = FocalLoss(gamma=cfg.focal_gamma, alpha=cfg.focal_alpha).to(device)\n",
    "focal_rel = FocalLoss(gamma=cfg.focal_gamma, alpha=cfg.focal_alpha).to(device)\n",
    "\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b04793f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train] start epoch, num_docs = 1000\n",
      "  step    1/1000 | step_time= 1.14s | loss=6.2914 (cls=0.612, par=5.467, rel=0.212) | elapsed=0.0min\n",
      "    sanity: seq_len=512, avg_parent_candidates=256.5\n",
      "    sanity: seq_len=512, avg_parent_candidates=256.5\n",
      "  step    5/1000 | step_time= 0.81s | loss=3.9795 (cls=0.089, par=3.761, rel=0.129) | elapsed=0.1min\n",
      "  step   10/1000 | step_time= 0.79s | loss=4.2965 (cls=0.143, par=3.999, rel=0.155) | elapsed=0.1min\n",
      "  step   15/1000 | step_time= 0.78s | loss=3.5786 (cls=0.100, par=3.391, rel=0.088) | elapsed=0.2min\n",
      "  step   20/1000 | step_time= 0.80s | loss=3.4521 (cls=0.159, par=3.202, rel=0.092) | elapsed=0.2min\n",
      "  step   25/1000 | step_time= 0.81s | loss=3.0337 (cls=0.165, par=2.778, rel=0.091) | elapsed=0.3min\n",
      "  step   30/1000 | step_time= 0.57s | loss=3.0748 (cls=0.153, par=2.833, rel=0.089) | elapsed=0.4min\n",
      "  step   35/1000 | step_time= 0.31s | loss=2.5898 (cls=0.197, par=2.304, rel=0.088) | elapsed=0.4min\n",
      "  step   40/1000 | step_time= 0.80s | loss=2.5969 (cls=0.098, par=2.423, rel=0.075) | elapsed=0.5min\n",
      "  step   45/1000 | step_time= 0.64s | loss=2.9486 (cls=0.057, par=2.839, rel=0.052) | elapsed=0.5min\n",
      "  step   50/1000 | step_time= 0.78s | loss=2.1547 (cls=0.155, par=1.942, rel=0.057) | elapsed=0.6min\n",
      "  step   55/1000 | step_time= 0.49s | loss=2.0650 (cls=0.118, par=1.868, rel=0.079) | elapsed=0.7min\n",
      "  step   60/1000 | step_time= 0.83s | loss=2.4882 (cls=0.071, par=2.360, rel=0.058) | elapsed=0.7min\n",
      "  step   65/1000 | step_time= 0.38s | loss=2.1284 (cls=0.096, par=1.953, rel=0.079) | elapsed=0.8min\n",
      "  step   70/1000 | step_time= 0.67s | loss=1.9306 (cls=0.077, par=1.801, rel=0.053) | elapsed=0.8min\n",
      "  step   75/1000 | step_time= 0.71s | loss=1.9651 (cls=0.102, par=1.782, rel=0.081) | elapsed=0.9min\n",
      "  step   80/1000 | step_time= 0.84s | loss=2.2528 (cls=0.074, par=2.119, rel=0.060) | elapsed=1.0min\n",
      "  step   85/1000 | step_time= 0.81s | loss=2.3189 (cls=0.086, par=2.171, rel=0.062) | elapsed=1.0min\n",
      "  step   90/1000 | step_time= 0.78s | loss=1.9246 (cls=0.137, par=1.720, rel=0.067) | elapsed=1.1min\n",
      "  step   95/1000 | step_time= 0.77s | loss=2.1966 (cls=0.100, par=2.010, rel=0.086) | elapsed=1.2min\n",
      "  step  100/1000 | step_time= 0.89s | loss=2.0239 (cls=0.080, par=1.875, rel=0.069) | elapsed=1.2min\n",
      "  step  105/1000 | step_time= 0.93s | loss=1.9514 (cls=0.159, par=1.738, rel=0.055) | elapsed=1.3min\n",
      "  step  110/1000 | step_time= 0.87s | loss=1.8375 (cls=0.088, par=1.682, rel=0.067) | elapsed=1.4min\n",
      "  step  115/1000 | step_time= 0.85s | loss=2.1334 (cls=0.103, par=1.943, rel=0.088) | elapsed=1.4min\n",
      "  step  120/1000 | step_time= 0.84s | loss=2.0837 (cls=0.091, par=1.916, rel=0.076) | elapsed=1.5min\n",
      "  step  125/1000 | step_time= 0.87s | loss=2.3201 (cls=0.070, par=2.205, rel=0.045) | elapsed=1.6min\n",
      "  step  130/1000 | step_time= 0.66s | loss=1.9048 (cls=0.138, par=1.702, rel=0.065) | elapsed=1.6min\n",
      "  step  135/1000 | step_time= 0.88s | loss=2.0040 (cls=0.051, par=1.907, rel=0.045) | elapsed=1.7min\n",
      "  step  140/1000 | step_time= 0.86s | loss=2.1346 (cls=0.123, par=1.951, rel=0.061) | elapsed=1.8min\n",
      "  step  145/1000 | step_time= 0.84s | loss=1.7192 (cls=0.139, par=1.516, rel=0.064) | elapsed=1.8min\n",
      "  step  150/1000 | step_time= 0.80s | loss=1.6001 (cls=0.072, par=1.461, rel=0.067) | elapsed=1.9min\n",
      "  step  155/1000 | step_time= 0.86s | loss=2.1343 (cls=0.066, par=2.024, rel=0.044) | elapsed=2.0min\n",
      "  step  160/1000 | step_time= 0.83s | loss=1.7505 (cls=0.065, par=1.627, rel=0.058) | elapsed=2.0min\n",
      "  step  165/1000 | step_time= 0.71s | loss=1.5335 (cls=0.073, par=1.401, rel=0.060) | elapsed=2.1min\n",
      "  step  170/1000 | step_time= 0.70s | loss=2.1477 (cls=0.075, par=2.007, rel=0.065) | elapsed=2.2min\n",
      "  step  175/1000 | step_time= 0.84s | loss=2.0111 (cls=0.062, par=1.895, rel=0.054) | elapsed=2.2min\n",
      "  step  180/1000 | step_time= 0.66s | loss=1.8252 (cls=0.058, par=1.720, rel=0.048) | elapsed=2.3min\n",
      "  step  185/1000 | step_time= 0.85s | loss=1.7888 (cls=0.045, par=1.710, rel=0.033) | elapsed=2.4min\n",
      "  step  190/1000 | step_time= 0.87s | loss=2.1246 (cls=0.092, par=1.968, rel=0.065) | elapsed=2.4min\n",
      "  step  195/1000 | step_time= 0.55s | loss=1.6018 (cls=0.080, par=1.464, rel=0.058) | elapsed=2.5min\n",
      "  step  200/1000 | step_time= 0.64s | loss=1.5763 (cls=0.115, par=1.391, rel=0.070) | elapsed=2.6min\n",
      "  step  205/1000 | step_time= 0.85s | loss=2.0627 (cls=0.077, par=1.937, rel=0.049) | elapsed=2.6min\n",
      "  step  210/1000 | step_time= 0.85s | loss=1.6188 (cls=0.089, par=1.475, rel=0.056) | elapsed=2.7min\n",
      "  step  215/1000 | step_time= 0.86s | loss=1.6907 (cls=0.052, par=1.597, rel=0.042) | elapsed=2.7min\n",
      "  step  220/1000 | step_time= 0.69s | loss=2.0423 (cls=0.076, par=1.903, rel=0.063) | elapsed=2.8min\n",
      "  step  225/1000 | step_time= 0.79s | loss=1.5626 (cls=0.067, par=1.435, rel=0.060) | elapsed=2.9min\n",
      "  step  230/1000 | step_time= 0.84s | loss=1.5611 (cls=0.111, par=1.405, rel=0.045) | elapsed=2.9min\n",
      "  step  235/1000 | step_time= 0.86s | loss=1.6891 (cls=0.071, par=1.582, rel=0.036) | elapsed=3.0min\n",
      "  step  240/1000 | step_time= 0.54s | loss=1.5489 (cls=0.097, par=1.383, rel=0.069) | elapsed=3.1min\n",
      "  step  245/1000 | step_time= 0.43s | loss=2.0070 (cls=0.149, par=1.788, rel=0.070) | elapsed=3.1min\n",
      "  step  250/1000 | step_time= 0.55s | loss=1.8136 (cls=0.099, par=1.661, rel=0.054) | elapsed=3.2min\n",
      "  step  255/1000 | step_time= 0.87s | loss=1.6107 (cls=0.051, par=1.522, rel=0.038) | elapsed=3.3min\n",
      "  step  260/1000 | step_time= 0.84s | loss=1.7604 (cls=0.066, par=1.641, rel=0.054) | elapsed=3.3min\n",
      "  step  265/1000 | step_time= 0.65s | loss=1.5965 (cls=0.100, par=1.436, rel=0.061) | elapsed=3.4min\n",
      "  step  270/1000 | step_time= 0.82s | loss=1.7030 (cls=0.062, par=1.588, rel=0.053) | elapsed=3.4min\n",
      "  step  275/1000 | step_time= 0.77s | loss=1.7220 (cls=0.073, par=1.602, rel=0.047) | elapsed=3.5min\n",
      "  step  280/1000 | step_time= 0.90s | loss=1.5743 (cls=0.082, par=1.424, rel=0.068) | elapsed=3.6min\n",
      "  step  285/1000 | step_time= 0.99s | loss=1.4434 (cls=0.044, par=1.363, rel=0.037) | elapsed=3.7min\n",
      "  step  290/1000 | step_time= 0.98s | loss=1.7849 (cls=0.067, par=1.676, rel=0.042) | elapsed=3.7min\n",
      "  step  295/1000 | step_time= 0.87s | loss=1.6802 (cls=0.131, par=1.464, rel=0.085) | elapsed=3.8min\n",
      "  step  300/1000 | step_time= 1.04s | loss=1.6559 (cls=0.057, par=1.552, rel=0.047) | elapsed=3.9min\n",
      "  step  305/1000 | step_time= 0.98s | loss=1.4878 (cls=0.070, par=1.376, rel=0.042) | elapsed=3.9min\n",
      "  step  310/1000 | step_time= 0.99s | loss=1.7062 (cls=0.051, par=1.616, rel=0.040) | elapsed=4.0min\n",
      "  step  315/1000 | step_time= 0.94s | loss=1.3444 (cls=0.081, par=1.213, rel=0.050) | elapsed=4.1min\n",
      "  step  320/1000 | step_time= 0.78s | loss=1.7147 (cls=0.106, par=1.532, rel=0.076) | elapsed=4.1min\n",
      "  step  325/1000 | step_time= 0.97s | loss=1.5904 (cls=0.087, par=1.434, rel=0.070) | elapsed=4.2min\n",
      "  step  330/1000 | step_time= 0.96s | loss=1.9034 (cls=0.060, par=1.795, rel=0.048) | elapsed=4.3min\n",
      "  step  335/1000 | step_time= 1.01s | loss=1.4625 (cls=0.073, par=1.324, rel=0.066) | elapsed=4.4min\n",
      "  step  340/1000 | step_time= 0.97s | loss=1.6230 (cls=0.054, par=1.529, rel=0.040) | elapsed=4.5min\n",
      "  step  345/1000 | step_time= 1.01s | loss=1.7346 (cls=0.051, par=1.638, rel=0.045) | elapsed=4.5min\n",
      "  step  350/1000 | step_time= 0.95s | loss=1.6292 (cls=0.084, par=1.493, rel=0.052) | elapsed=4.6min\n",
      "  step  355/1000 | step_time= 1.02s | loss=1.4847 (cls=0.082, par=1.346, rel=0.056) | elapsed=4.7min\n",
      "  step  360/1000 | step_time= 1.00s | loss=1.5471 (cls=0.059, par=1.449, rel=0.039) | elapsed=4.8min\n",
      "  step  365/1000 | step_time= 0.99s | loss=1.1185 (cls=0.041, par=1.035, rel=0.043) | elapsed=4.8min\n",
      "  step  370/1000 | step_time= 0.58s | loss=1.1337 (cls=0.055, par=1.028, rel=0.050) | elapsed=4.9min\n",
      "  step  375/1000 | step_time= 0.99s | loss=1.3266 (cls=0.043, par=1.242, rel=0.041) | elapsed=5.0min\n",
      "  step  380/1000 | step_time= 0.67s | loss=1.7036 (cls=0.107, par=1.533, rel=0.063) | elapsed=5.1min\n",
      "  step  385/1000 | step_time= 0.32s | loss=0.8737 (cls=0.061, par=0.764, rel=0.049) | elapsed=5.1min\n",
      "  step  390/1000 | step_time= 0.89s | loss=1.5206 (cls=0.098, par=1.357, rel=0.066) | elapsed=5.2min\n",
      "  step  395/1000 | step_time= 0.94s | loss=1.4961 (cls=0.113, par=1.327, rel=0.056) | elapsed=5.3min\n",
      "  step  400/1000 | step_time= 0.76s | loss=1.5356 (cls=0.098, par=1.385, rel=0.052) | elapsed=5.3min\n",
      "  step  405/1000 | step_time= 0.43s | loss=1.2698 (cls=0.083, par=1.119, rel=0.068) | elapsed=5.4min\n",
      "  step  410/1000 | step_time= 0.74s | loss=1.4836 (cls=0.095, par=1.328, rel=0.061) | elapsed=5.5min\n",
      "  step  415/1000 | step_time= 0.85s | loss=1.4458 (cls=0.061, par=1.350, rel=0.035) | elapsed=5.5min\n",
      "  step  420/1000 | step_time= 0.91s | loss=1.1333 (cls=0.048, par=1.052, rel=0.034) | elapsed=5.6min\n",
      "  step  425/1000 | step_time= 1.01s | loss=1.5243 (cls=0.130, par=1.340, rel=0.055) | elapsed=5.7min\n",
      "  step  430/1000 | step_time= 0.99s | loss=1.3395 (cls=0.045, par=1.258, rel=0.037) | elapsed=5.7min\n",
      "  step  435/1000 | step_time= 0.78s | loss=1.5172 (cls=0.086, par=1.363, rel=0.068) | elapsed=5.8min\n",
      "  step  440/1000 | step_time= 1.14s | loss=1.3893 (cls=0.115, par=1.221, rel=0.053) | elapsed=5.9min\n",
      "  step  445/1000 | step_time= 0.80s | loss=1.3015 (cls=0.071, par=1.163, rel=0.068) | elapsed=5.9min\n",
      "  step  450/1000 | step_time= 0.96s | loss=1.3630 (cls=0.096, par=1.214, rel=0.053) | elapsed=6.0min\n",
      "  step  455/1000 | step_time= 0.88s | loss=0.9789 (cls=0.043, par=0.906, rel=0.030) | elapsed=6.1min\n",
      "  step  460/1000 | step_time= 0.57s | loss=1.4520 (cls=0.090, par=1.285, rel=0.077) | elapsed=6.2min\n",
      "  step  465/1000 | step_time= 0.69s | loss=1.1442 (cls=0.049, par=1.058, rel=0.038) | elapsed=6.2min\n",
      "  step  470/1000 | step_time= 0.59s | loss=1.1083 (cls=0.065, par=0.995, rel=0.048) | elapsed=6.3min\n",
      "  step  475/1000 | step_time= 0.98s | loss=1.5125 (cls=0.063, par=1.396, rel=0.053) | elapsed=6.4min\n",
      "  step  480/1000 | step_time= 0.64s | loss=1.2183 (cls=0.065, par=1.106, rel=0.048) | elapsed=6.5min\n",
      "  step  485/1000 | step_time= 0.99s | loss=1.4042 (cls=0.068, par=1.284, rel=0.052) | elapsed=6.5min\n",
      "  step  490/1000 | step_time= 0.98s | loss=1.4906 (cls=0.105, par=1.340, rel=0.046) | elapsed=6.6min\n",
      "  step  495/1000 | step_time= 0.93s | loss=1.0479 (cls=0.067, par=0.950, rel=0.030) | elapsed=6.7min\n",
      "  step  500/1000 | step_time= 0.61s | loss=2.0885 (cls=0.088, par=1.950, rel=0.050) | elapsed=7.0min\n",
      "  step  505/1000 | step_time= 1.00s | loss=1.2165 (cls=0.041, par=1.139, rel=0.037) | elapsed=7.1min\n",
      "  step  510/1000 | step_time= 0.96s | loss=1.1989 (cls=0.054, par=1.097, rel=0.048) | elapsed=7.1min\n",
      "  step  515/1000 | step_time= 0.71s | loss=1.4098 (cls=0.077, par=1.272, rel=0.061) | elapsed=7.2min\n",
      "  step  520/1000 | step_time= 0.62s | loss=1.3410 (cls=0.080, par=1.200, rel=0.062) | elapsed=7.3min\n",
      "  step  525/1000 | step_time= 0.41s | loss=1.4165 (cls=0.080, par=1.265, rel=0.071) | elapsed=7.3min\n",
      "  step  530/1000 | step_time= 0.91s | loss=1.0445 (cls=0.036, par=0.981, rel=0.028) | elapsed=7.4min\n",
      "  step  535/1000 | step_time= 0.86s | loss=1.4276 (cls=0.057, par=1.333, rel=0.037) | elapsed=7.5min\n",
      "  step  540/1000 | step_time= 0.92s | loss=1.3242 (cls=0.117, par=1.155, rel=0.053) | elapsed=7.5min\n",
      "  step  545/1000 | step_time= 2.92s | loss=1.0795 (cls=0.049, par=0.992, rel=0.039) | elapsed=7.7min\n",
      "  step  550/1000 | step_time= 0.88s | loss=1.5611 (cls=0.052, par=1.468, rel=0.041) | elapsed=7.8min\n",
      "  step  555/1000 | step_time= 0.96s | loss=0.9706 (cls=0.040, par=0.905, rel=0.026) | elapsed=7.9min\n",
      "  step  560/1000 | step_time= 0.97s | loss=1.1627 (cls=0.044, par=1.086, rel=0.033) | elapsed=7.9min\n",
      "  step  565/1000 | step_time= 0.83s | loss=1.4006 (cls=0.060, par=1.307, rel=0.033) | elapsed=8.0min\n",
      "  step  570/1000 | step_time= 0.90s | loss=1.3992 (cls=0.081, par=1.268, rel=0.050) | elapsed=8.1min\n",
      "  step  575/1000 | step_time= 0.98s | loss=1.1576 (cls=0.056, par=1.064, rel=0.037) | elapsed=8.1min\n",
      "  step  580/1000 | step_time= 0.74s | loss=1.3579 (cls=0.100, par=1.203, rel=0.055) | elapsed=8.2min\n",
      "  step  585/1000 | step_time= 0.41s | loss=1.2741 (cls=0.109, par=1.098, rel=0.068) | elapsed=8.3min\n",
      "  step  590/1000 | step_time= 0.46s | loss=1.5243 (cls=0.083, par=1.355, rel=0.087) | elapsed=8.4min\n",
      "  step  595/1000 | step_time= 3.06s | loss=1.0031 (cls=0.048, par=0.921, rel=0.034) | elapsed=8.5min\n",
      "  step  600/1000 | step_time= 2.72s | loss=1.3439 (cls=0.047, par=1.259, rel=0.038) | elapsed=8.7min\n",
      "  step  605/1000 | step_time= 1.74s | loss=1.0705 (cls=0.050, par=0.982, rel=0.039) | elapsed=8.9min\n",
      "  step  610/1000 | step_time= 0.93s | loss=1.1997 (cls=0.064, par=1.101, rel=0.035) | elapsed=9.0min\n",
      "  step  615/1000 | step_time= 3.73s | loss=1.7772 (cls=0.065, par=1.647, rel=0.065) | elapsed=9.1min\n",
      "  step  620/1000 | step_time=10.93s | loss=1.4630 (cls=0.104, par=1.299, rel=0.060) | elapsed=9.5min\n",
      "  step  625/1000 | step_time= 0.94s | loss=1.0420 (cls=0.035, par=0.983, rel=0.025) | elapsed=9.7min\n",
      "  step  630/1000 | step_time= 0.62s | loss=1.0725 (cls=0.091, par=0.941, rel=0.040) | elapsed=9.8min\n",
      "  step  635/1000 | step_time= 0.84s | loss=1.2902 (cls=0.096, par=1.135, rel=0.059) | elapsed=9.8min\n",
      "  step  640/1000 | step_time= 0.80s | loss=1.6066 (cls=0.426, par=1.157, rel=0.024) | elapsed=9.9min\n",
      "  step  645/1000 | step_time= 0.83s | loss=0.7016 (cls=0.035, par=0.645, rel=0.022) | elapsed=10.0min\n",
      "  step  650/1000 | step_time= 0.60s | loss=1.0854 (cls=0.077, par=0.965, rel=0.044) | elapsed=10.0min\n",
      "  step  655/1000 | step_time= 0.51s | loss=1.3442 (cls=0.089, par=1.188, rel=0.067) | elapsed=10.1min\n",
      "  step  660/1000 | step_time= 0.58s | loss=1.0153 (cls=0.064, par=0.905, rel=0.046) | elapsed=10.2min\n",
      "  step  665/1000 | step_time= 0.45s | loss=1.1275 (cls=0.070, par=1.006, rel=0.052) | elapsed=10.2min\n",
      "  step  670/1000 | step_time= 0.86s | loss=1.0296 (cls=0.054, par=0.945, rel=0.031) | elapsed=10.3min\n",
      "  step  675/1000 | step_time= 0.92s | loss=1.4235 (cls=0.085, par=1.280, rel=0.059) | elapsed=10.3min\n",
      "  step  680/1000 | step_time= 0.63s | loss=1.3731 (cls=0.102, par=1.223, rel=0.049) | elapsed=10.4min\n",
      "  step  685/1000 | step_time= 0.86s | loss=1.3510 (cls=0.086, par=1.217, rel=0.048) | elapsed=10.5min\n",
      "  step  690/1000 | step_time= 0.97s | loss=1.3179 (cls=0.069, par=1.189, rel=0.060) | elapsed=10.6min\n",
      "  step  695/1000 | step_time= 0.93s | loss=1.0053 (cls=0.041, par=0.941, rel=0.024) | elapsed=10.6min\n",
      "  step  700/1000 | step_time= 0.53s | loss=1.2870 (cls=0.068, par=1.156, rel=0.063) | elapsed=10.7min\n",
      "  step  705/1000 | step_time= 0.85s | loss=1.2317 (cls=0.069, par=1.107, rel=0.056) | elapsed=10.8min\n",
      "  step  710/1000 | step_time= 0.85s | loss=1.0554 (cls=0.044, par=0.984, rel=0.028) | elapsed=10.9min\n",
      "  step  715/1000 | step_time= 0.85s | loss=1.2680 (cls=0.109, par=1.115, rel=0.044) | elapsed=10.9min\n",
      "  step  720/1000 | step_time= 0.85s | loss=0.9725 (cls=0.051, par=0.896, rel=0.025) | elapsed=11.0min\n",
      "  step  725/1000 | step_time= 0.85s | loss=1.3205 (cls=0.061, par=1.207, rel=0.053) | elapsed=11.1min\n",
      "  step  730/1000 | step_time= 0.92s | loss=1.1261 (cls=0.059, par=1.040, rel=0.028) | elapsed=11.1min\n",
      "  step  735/1000 | step_time= 0.87s | loss=0.7934 (cls=0.039, par=0.738, rel=0.017) | elapsed=11.2min\n",
      "  step  740/1000 | step_time= 0.57s | loss=1.4136 (cls=0.087, par=1.262, rel=0.064) | elapsed=11.3min\n",
      "  step  745/1000 | step_time= 0.88s | loss=1.0764 (cls=0.047, par=0.995, rel=0.035) | elapsed=11.3min\n",
      "  step  750/1000 | step_time= 0.87s | loss=1.0925 (cls=0.050, par=1.007, rel=0.036) | elapsed=11.4min\n",
      "  step  755/1000 | step_time= 0.37s | loss=1.8274 (cls=0.116, par=1.594, rel=0.118) | elapsed=11.4min\n",
      "  step  760/1000 | step_time= 0.74s | loss=0.9636 (cls=0.059, par=0.863, rel=0.041) | elapsed=11.5min\n",
      "  step  765/1000 | step_time= 0.71s | loss=1.3979 (cls=0.092, par=1.241, rel=0.064) | elapsed=11.6min\n",
      "  step  770/1000 | step_time= 0.86s | loss=1.1361 (cls=0.065, par=1.035, rel=0.036) | elapsed=11.6min\n",
      "  step  775/1000 | step_time= 0.85s | loss=1.0027 (cls=0.064, par=0.905, rel=0.034) | elapsed=11.7min\n",
      "  step  780/1000 | step_time= 0.87s | loss=1.4124 (cls=0.076, par=1.265, rel=0.072) | elapsed=11.8min\n",
      "  step  785/1000 | step_time= 0.87s | loss=1.2707 (cls=0.100, par=1.131, rel=0.040) | elapsed=11.8min\n",
      "  step  790/1000 | step_time= 0.86s | loss=0.7934 (cls=0.052, par=0.713, rel=0.028) | elapsed=11.9min\n",
      "  step  795/1000 | step_time= 0.44s | loss=1.1624 (cls=0.074, par=1.032, rel=0.056) | elapsed=11.9min\n",
      "  step  800/1000 | step_time= 0.86s | loss=1.6042 (cls=0.076, par=1.476, rel=0.052) | elapsed=12.0min\n",
      "  step  805/1000 | step_time= 0.86s | loss=1.1250 (cls=0.055, par=1.027, rel=0.043) | elapsed=12.1min\n",
      "  step  810/1000 | step_time= 0.86s | loss=1.1488 (cls=0.065, par=1.034, rel=0.050) | elapsed=12.2min\n",
      "  step  815/1000 | step_time= 0.88s | loss=1.2651 (cls=0.077, par=1.135, rel=0.052) | elapsed=12.2min\n",
      "  step  820/1000 | step_time= 0.85s | loss=0.9963 (cls=0.050, par=0.920, rel=0.027) | elapsed=12.3min\n",
      "  step  825/1000 | step_time= 0.86s | loss=1.0188 (cls=0.050, par=0.939, rel=0.029) | elapsed=12.4min\n",
      "  step  830/1000 | step_time= 0.87s | loss=0.8906 (cls=0.044, par=0.821, rel=0.025) | elapsed=12.4min\n",
      "  step  835/1000 | step_time= 0.94s | loss=1.4586 (cls=0.105, par=1.286, rel=0.068) | elapsed=12.5min\n",
      "  step  840/1000 | step_time= 0.86s | loss=0.9064 (cls=0.048, par=0.832, rel=0.027) | elapsed=12.6min\n",
      "  step  845/1000 | step_time= 0.87s | loss=1.0839 (cls=0.092, par=0.956, rel=0.035) | elapsed=12.6min\n",
      "  step  850/1000 | step_time= 0.86s | loss=0.9223 (cls=0.037, par=0.865, rel=0.021) | elapsed=12.7min\n",
      "  step  855/1000 | step_time= 0.96s | loss=1.0239 (cls=0.083, par=0.900, rel=0.041) | elapsed=12.8min\n",
      "  step  860/1000 | step_time= 0.86s | loss=1.5729 (cls=0.046, par=1.494, rel=0.033) | elapsed=12.8min\n",
      "  step  865/1000 | step_time= 0.66s | loss=1.2599 (cls=0.091, par=1.125, rel=0.044) | elapsed=12.9min\n",
      "  step  870/1000 | step_time= 0.86s | loss=0.9607 (cls=0.072, par=0.855, rel=0.034) | elapsed=13.0min\n",
      "  step  875/1000 | step_time= 0.54s | loss=1.2088 (cls=0.083, par=1.088, rel=0.038) | elapsed=13.0min\n",
      "  step  880/1000 | step_time= 0.86s | loss=0.7546 (cls=0.067, par=0.666, rel=0.023) | elapsed=13.1min\n",
      "  step  885/1000 | step_time= 0.87s | loss=0.8624 (cls=0.027, par=0.812, rel=0.024) | elapsed=13.1min\n",
      "  step  890/1000 | step_time= 0.87s | loss=0.8275 (cls=0.036, par=0.769, rel=0.022) | elapsed=13.2min\n",
      "  step  895/1000 | step_time= 0.88s | loss=1.0728 (cls=0.034, par=1.016, rel=0.023) | elapsed=13.3min\n",
      "  step  900/1000 | step_time= 0.86s | loss=1.0941 (cls=0.062, par=0.998, rel=0.034) | elapsed=13.4min\n",
      "  step  905/1000 | step_time= 0.89s | loss=1.0164 (cls=0.054, par=0.931, rel=0.032) | elapsed=13.4min\n",
      "  step  910/1000 | step_time= 0.87s | loss=1.3195 (cls=0.069, par=1.188, rel=0.063) | elapsed=13.5min\n",
      "  step  915/1000 | step_time= 0.86s | loss=1.5310 (cls=0.072, par=1.392, rel=0.067) | elapsed=13.5min\n",
      "  step  920/1000 | step_time= 0.87s | loss=0.8579 (cls=0.030, par=0.811, rel=0.017) | elapsed=13.6min\n",
      "  step  925/1000 | step_time= 0.87s | loss=1.0848 (cls=0.064, par=0.999, rel=0.022) | elapsed=13.7min\n",
      "  step  930/1000 | step_time= 0.41s | loss=1.3010 (cls=0.088, par=1.165, rel=0.048) | elapsed=13.7min\n",
      "  step  935/1000 | step_time= 0.87s | loss=0.7941 (cls=0.032, par=0.749, rel=0.013) | elapsed=13.8min\n",
      "  step  940/1000 | step_time= 0.82s | loss=1.6226 (cls=0.075, par=1.491, rel=0.057) | elapsed=13.9min\n",
      "  step  945/1000 | step_time= 0.86s | loss=0.9084 (cls=0.042, par=0.844, rel=0.022) | elapsed=13.9min\n",
      "  step  950/1000 | step_time= 0.91s | loss=1.3355 (cls=0.072, par=1.222, rel=0.042) | elapsed=14.0min\n",
      "  step  955/1000 | step_time= 0.68s | loss=1.0347 (cls=0.077, par=0.928, rel=0.030) | elapsed=14.1min\n",
      "  step  960/1000 | step_time= 0.88s | loss=1.2284 (cls=0.063, par=1.131, rel=0.034) | elapsed=14.2min\n",
      "  step  965/1000 | step_time= 0.87s | loss=0.7405 (cls=0.034, par=0.692, rel=0.014) | elapsed=14.2min\n",
      "  step  970/1000 | step_time= 0.85s | loss=1.2693 (cls=0.083, par=1.137, rel=0.049) | elapsed=14.3min\n",
      "  step  975/1000 | step_time= 0.72s | loss=0.9972 (cls=0.062, par=0.905, rel=0.031) | elapsed=14.3min\n",
      "  step  980/1000 | step_time= 0.94s | loss=0.8504 (cls=0.048, par=0.762, rel=0.040) | elapsed=14.4min\n",
      "  step  985/1000 | step_time= 0.59s | loss=0.7882 (cls=0.040, par=0.725, rel=0.023) | elapsed=14.5min\n",
      "  step  990/1000 | step_time= 0.87s | loss=0.9064 (cls=0.053, par=0.824, rel=0.029) | elapsed=14.5min\n",
      "  step  995/1000 | step_time= 0.85s | loss=1.1634 (cls=0.048, par=1.085, rel=0.030) | elapsed=14.6min\n",
      "  step 1000/1000 | step_time= 0.89s | loss=1.2147 (cls=0.059, par=1.120, rel=0.036) | elapsed=14.7min\n",
      "[Train] epoch done in 14.66 min\n",
      "1 {'loss': 1.5309644168019294, 'loss_cls': 0.08171500859037041, 'loss_par': 1.3971258195638656, 'loss_rel': 0.0521235885983333}\n",
      "\n",
      "[Train] start epoch, num_docs = 1000\n",
      "  step    1/1000 | step_time= 0.86s | loss=0.7160 (cls=0.058, par=0.631, rel=0.027) | elapsed=0.0min\n",
      "    sanity: seq_len=512, avg_parent_candidates=256.5\n",
      "    sanity: seq_len=512, avg_parent_candidates=256.5\n",
      "  step    5/1000 | step_time= 0.86s | loss=0.8140 (cls=0.043, par=0.746, rel=0.025) | elapsed=0.1min\n",
      "  step   10/1000 | step_time= 0.88s | loss=0.8766 (cls=0.048, par=0.800, rel=0.028) | elapsed=0.1min\n",
      "  step   15/1000 | step_time= 0.66s | loss=1.0349 (cls=0.090, par=0.905, rel=0.040) | elapsed=0.2min\n",
      "  step   20/1000 | step_time= 0.78s | loss=0.9568 (cls=0.071, par=0.859, rel=0.027) | elapsed=0.3min\n",
      "  step   25/1000 | step_time= 0.85s | loss=1.0936 (cls=0.072, par=0.987, rel=0.035) | elapsed=0.3min\n",
      "  step   30/1000 | step_time= 0.22s | loss=0.9216 (cls=0.104, par=0.765, rel=0.052) | elapsed=0.4min\n",
      "  step   35/1000 | step_time= 0.86s | loss=1.3414 (cls=0.080, par=1.218, rel=0.043) | elapsed=0.5min\n",
      "  step   40/1000 | step_time= 0.87s | loss=0.8681 (cls=0.041, par=0.805, rel=0.022) | elapsed=0.5min\n",
      "  step   45/1000 | step_time= 0.71s | loss=0.9841 (cls=0.081, par=0.861, rel=0.043) | elapsed=0.6min\n",
      "  step   50/1000 | step_time= 0.67s | loss=0.7282 (cls=0.048, par=0.655, rel=0.025) | elapsed=0.7min\n",
      "  step   55/1000 | step_time= 0.88s | loss=0.8416 (cls=0.049, par=0.762, rel=0.031) | elapsed=0.7min\n",
      "  step   60/1000 | step_time= 0.87s | loss=0.8339 (cls=0.049, par=0.762, rel=0.023) | elapsed=0.8min\n",
      "  step   65/1000 | step_time= 0.85s | loss=0.8715 (cls=0.051, par=0.797, rel=0.023) | elapsed=0.9min\n",
      "  step   70/1000 | step_time= 0.93s | loss=1.1365 (cls=0.063, par=1.041, rel=0.032) | elapsed=0.9min\n",
      "  step   75/1000 | step_time= 0.70s | loss=0.8298 (cls=0.059, par=0.745, rel=0.026) | elapsed=1.0min\n",
      "  step   80/1000 | step_time= 0.86s | loss=0.6094 (cls=0.023, par=0.575, rel=0.011) | elapsed=1.1min\n",
      "  step   85/1000 | step_time= 0.86s | loss=0.9432 (cls=0.049, par=0.862, rel=0.032) | elapsed=1.1min\n",
      "  step   90/1000 | step_time= 0.55s | loss=1.0555 (cls=0.101, par=0.912, rel=0.042) | elapsed=1.2min\n",
      "  step   95/1000 | step_time= 0.87s | loss=1.1066 (cls=0.071, par=1.003, rel=0.032) | elapsed=1.2min\n",
      "  step  100/1000 | step_time= 0.97s | loss=0.8666 (cls=0.048, par=0.796, rel=0.022) | elapsed=1.3min\n",
      "  step  105/1000 | step_time= 0.85s | loss=0.8320 (cls=0.062, par=0.747, rel=0.023) | elapsed=1.4min\n",
      "  step  110/1000 | step_time= 0.87s | loss=1.3483 (cls=0.104, par=1.199, rel=0.045) | elapsed=1.5min\n",
      "  step  115/1000 | step_time= 0.85s | loss=0.9627 (cls=0.051, par=0.883, rel=0.029) | elapsed=1.5min\n",
      "  step  120/1000 | step_time= 0.62s | loss=1.1281 (cls=0.074, par=1.015, rel=0.039) | elapsed=1.6min\n",
      "  step  125/1000 | step_time= 0.90s | loss=0.7402 (cls=0.034, par=0.686, rel=0.020) | elapsed=1.6min\n",
      "  step  130/1000 | step_time= 0.55s | loss=1.6308 (cls=0.073, par=1.522, rel=0.036) | elapsed=1.7min\n",
      "  step  135/1000 | step_time= 0.70s | loss=0.7954 (cls=0.069, par=0.693, rel=0.033) | elapsed=1.8min\n",
      "  step  140/1000 | step_time= 0.87s | loss=1.6221 (cls=0.031, par=1.576, rel=0.015) | elapsed=1.8min\n",
      "  step  145/1000 | step_time= 0.85s | loss=0.9039 (cls=0.082, par=0.787, rel=0.035) | elapsed=1.9min\n",
      "  step  150/1000 | step_time= 0.29s | loss=0.8373 (cls=0.076, par=0.709, rel=0.052) | elapsed=1.9min\n",
      "  step  155/1000 | step_time= 0.86s | loss=0.7774 (cls=0.041, par=0.720, rel=0.017) | elapsed=2.0min\n",
      "  step  160/1000 | step_time= 0.58s | loss=0.9767 (cls=0.055, par=0.889, rel=0.033) | elapsed=2.1min\n",
      "  step  165/1000 | step_time= 0.87s | loss=0.6212 (cls=0.033, par=0.576, rel=0.013) | elapsed=2.2min\n",
      "  step  170/1000 | step_time= 0.85s | loss=0.5812 (cls=0.067, par=0.494, rel=0.020) | elapsed=2.2min\n",
      "  step  175/1000 | step_time= 0.91s | loss=0.7336 (cls=0.048, par=0.670, rel=0.016) | elapsed=2.3min\n",
      "  step  180/1000 | step_time= 0.85s | loss=0.9633 (cls=0.056, par=0.873, rel=0.034) | elapsed=2.4min\n",
      "  step  185/1000 | step_time= 0.82s | loss=0.9985 (cls=0.074, par=0.892, rel=0.032) | elapsed=2.4min\n",
      "  step  190/1000 | step_time= 0.33s | loss=1.1698 (cls=0.142, par=0.932, rel=0.096) | elapsed=2.5min\n",
      "  step  195/1000 | step_time= 0.89s | loss=1.2160 (cls=0.074, par=1.093, rel=0.049) | elapsed=2.6min\n",
      "  step  200/1000 | step_time= 0.88s | loss=1.1529 (cls=0.092, par=1.017, rel=0.045) | elapsed=2.6min\n",
      "  step  205/1000 | step_time= 0.78s | loss=1.1311 (cls=0.083, par=0.976, rel=0.072) | elapsed=2.7min\n",
      "  step  210/1000 | step_time= 0.97s | loss=0.8011 (cls=0.038, par=0.742, rel=0.021) | elapsed=2.8min\n",
      "  step  215/1000 | step_time= 0.64s | loss=0.7335 (cls=0.060, par=0.655, rel=0.019) | elapsed=2.8min\n",
      "  step  220/1000 | step_time= 0.91s | loss=0.8741 (cls=0.038, par=0.811, rel=0.025) | elapsed=2.9min\n",
      "  step  225/1000 | step_time= 0.81s | loss=1.0631 (cls=0.044, par=0.991, rel=0.029) | elapsed=3.0min\n",
      "  step  230/1000 | step_time= 0.98s | loss=1.0327 (cls=0.044, par=0.963, rel=0.026) | elapsed=3.1min\n",
      "  step  235/1000 | step_time= 0.90s | loss=1.1381 (cls=0.044, par=1.060, rel=0.035) | elapsed=3.1min\n",
      "  step  240/1000 | step_time= 0.88s | loss=0.6497 (cls=0.032, par=0.604, rel=0.014) | elapsed=3.2min\n",
      "  step  245/1000 | step_time= 0.55s | loss=1.0292 (cls=0.057, par=0.942, rel=0.030) | elapsed=3.3min\n",
      "  step  250/1000 | step_time= 0.87s | loss=0.9390 (cls=0.068, par=0.831, rel=0.041) | elapsed=3.3min\n",
      "  step  255/1000 | step_time= 0.89s | loss=0.7861 (cls=0.058, par=0.700, rel=0.029) | elapsed=3.4min\n",
      "  step  260/1000 | step_time= 0.90s | loss=1.0850 (cls=0.062, par=0.989, rel=0.033) | elapsed=3.5min\n",
      "  step  265/1000 | step_time= 0.52s | loss=1.1582 (cls=0.075, par=1.040, rel=0.044) | elapsed=3.5min\n",
      "  step  270/1000 | step_time= 0.59s | loss=1.1486 (cls=0.092, par=1.009, rel=0.048) | elapsed=3.6min\n",
      "  step  275/1000 | step_time= 0.35s | loss=0.7164 (cls=0.076, par=0.611, rel=0.030) | elapsed=3.6min\n",
      "  step  280/1000 | step_time= 0.88s | loss=1.1376 (cls=0.074, par=1.029, rel=0.034) | elapsed=3.7min\n",
      "  step  285/1000 | step_time= 0.72s | loss=1.2598 (cls=0.071, par=1.138, rel=0.051) | elapsed=3.8min\n",
      "  step  290/1000 | step_time= 0.94s | loss=1.2035 (cls=0.040, par=1.134, rel=0.029) | elapsed=3.9min\n",
      "  step  295/1000 | step_time= 0.64s | loss=0.5527 (cls=0.034, par=0.503, rel=0.016) | elapsed=3.9min\n",
      "  step  300/1000 | step_time= 0.66s | loss=0.6486 (cls=0.042, par=0.588, rel=0.019) | elapsed=4.0min\n",
      "  step  305/1000 | step_time= 0.71s | loss=0.9296 (cls=0.065, par=0.832, rel=0.033) | elapsed=4.1min\n",
      "  step  310/1000 | step_time= 0.86s | loss=0.5196 (cls=0.057, par=0.449, rel=0.014) | elapsed=4.1min\n",
      "  step  315/1000 | step_time= 0.88s | loss=0.8450 (cls=0.051, par=0.766, rel=0.027) | elapsed=4.2min\n",
      "  step  320/1000 | step_time= 0.63s | loss=1.0162 (cls=0.076, par=0.916, rel=0.024) | elapsed=4.2min\n",
      "  step  325/1000 | step_time= 0.23s | loss=1.3857 (cls=0.187, par=1.160, rel=0.038) | elapsed=4.3min\n",
      "  step  330/1000 | step_time= 0.87s | loss=1.1458 (cls=0.098, par=1.001, rel=0.046) | elapsed=4.4min\n",
      "  step  335/1000 | step_time= 0.89s | loss=1.2535 (cls=0.077, par=1.118, rel=0.059) | elapsed=4.4min\n",
      "  step  340/1000 | step_time= 0.91s | loss=0.9075 (cls=0.057, par=0.822, rel=0.029) | elapsed=4.5min\n",
      "  step  345/1000 | step_time= 0.79s | loss=0.5768 (cls=0.044, par=0.512, rel=0.021) | elapsed=4.6min\n",
      "  step  350/1000 | step_time= 0.65s | loss=0.7975 (cls=0.081, par=0.691, rel=0.025) | elapsed=4.7min\n",
      "  step  355/1000 | step_time= 0.72s | loss=0.9777 (cls=0.049, par=0.900, rel=0.029) | elapsed=4.7min\n",
      "  step  360/1000 | step_time= 0.46s | loss=1.3073 (cls=0.157, par=1.111, rel=0.039) | elapsed=4.8min\n",
      "  step  365/1000 | step_time= 0.90s | loss=1.0213 (cls=0.038, par=0.964, rel=0.020) | elapsed=4.9min\n",
      "  step  370/1000 | step_time= 0.74s | loss=1.1941 (cls=0.106, par=1.050, rel=0.039) | elapsed=4.9min\n",
      "  step  375/1000 | step_time= 0.86s | loss=1.3241 (cls=0.104, par=1.175, rel=0.045) | elapsed=5.0min\n",
      "  step  380/1000 | step_time= 0.89s | loss=0.8591 (cls=0.035, par=0.797, rel=0.028) | elapsed=5.1min\n",
      "  step  385/1000 | step_time= 0.98s | loss=0.5608 (cls=0.026, par=0.524, rel=0.012) | elapsed=5.1min\n",
      "  step  390/1000 | step_time= 0.42s | loss=1.3374 (cls=0.071, par=1.215, rel=0.052) | elapsed=5.2min\n",
      "  step  395/1000 | step_time= 0.82s | loss=1.3864 (cls=0.065, par=1.265, rel=0.056) | elapsed=5.2min\n",
      "  step  400/1000 | step_time= 0.82s | loss=1.0769 (cls=0.095, par=0.938, rel=0.044) | elapsed=5.3min\n",
      "  step  405/1000 | step_time= 0.87s | loss=1.0588 (cls=0.076, par=0.947, rel=0.036) | elapsed=5.4min\n",
      "  step  410/1000 | step_time= 0.28s | loss=0.6847 (cls=0.092, par=0.569, rel=0.024) | elapsed=5.4min\n",
      "  step  415/1000 | step_time= 0.87s | loss=1.0909 (cls=0.072, par=0.983, rel=0.036) | elapsed=5.5min\n",
      "  step  420/1000 | step_time= 0.59s | loss=0.6894 (cls=0.075, par=0.594, rel=0.020) | elapsed=5.6min\n",
      "  step  425/1000 | step_time= 0.84s | loss=1.1623 (cls=0.088, par=1.034, rel=0.040) | elapsed=5.6min\n",
      "  step  430/1000 | step_time= 0.88s | loss=0.6177 (cls=0.033, par=0.570, rel=0.014) | elapsed=5.7min\n",
      "  step  435/1000 | step_time= 0.89s | loss=0.8715 (cls=0.055, par=0.798, rel=0.018) | elapsed=5.7min\n",
      "  step  440/1000 | step_time= 0.92s | loss=0.6076 (cls=0.025, par=0.571, rel=0.012) | elapsed=5.8min\n",
      "  step  445/1000 | step_time= 0.65s | loss=0.7086 (cls=0.056, par=0.632, rel=0.021) | elapsed=5.9min\n",
      "  step  450/1000 | step_time= 0.80s | loss=0.8707 (cls=0.066, par=0.784, rel=0.021) | elapsed=5.9min\n",
      "  step  455/1000 | step_time= 0.87s | loss=1.0427 (cls=0.063, par=0.936, rel=0.044) | elapsed=6.0min\n",
      "  step  460/1000 | step_time= 0.45s | loss=0.8721 (cls=0.084, par=0.754, rel=0.035) | elapsed=6.1min\n",
      "  step  465/1000 | step_time= 0.87s | loss=0.9114 (cls=0.058, par=0.818, rel=0.036) | elapsed=6.1min\n",
      "  step  470/1000 | step_time= 0.63s | loss=0.7034 (cls=0.049, par=0.625, rel=0.029) | elapsed=6.2min\n",
      "  step  475/1000 | step_time= 0.89s | loss=0.9663 (cls=0.064, par=0.873, rel=0.029) | elapsed=6.3min\n",
      "  step  480/1000 | step_time= 0.83s | loss=0.6351 (cls=0.044, par=0.575, rel=0.016) | elapsed=6.3min\n",
      "  step  485/1000 | step_time= 0.62s | loss=1.0807 (cls=0.080, par=0.961, rel=0.039) | elapsed=6.4min\n",
      "  step  490/1000 | step_time= 0.66s | loss=0.5764 (cls=0.088, par=0.477, rel=0.012) | elapsed=6.5min\n",
      "  step  495/1000 | step_time= 0.87s | loss=0.6135 (cls=0.056, par=0.538, rel=0.020) | elapsed=6.5min\n",
      "  step  500/1000 | step_time= 0.86s | loss=0.9232 (cls=0.045, par=0.850, rel=0.028) | elapsed=6.6min\n",
      "  step  505/1000 | step_time= 0.92s | loss=0.9006 (cls=0.101, par=0.772, rel=0.028) | elapsed=6.6min\n",
      "  step  510/1000 | step_time= 0.86s | loss=0.5567 (cls=0.021, par=0.526, rel=0.009) | elapsed=6.7min\n",
      "  step  515/1000 | step_time= 0.62s | loss=0.8626 (cls=0.049, par=0.776, rel=0.037) | elapsed=6.8min\n",
      "  step  520/1000 | step_time= 0.70s | loss=0.4495 (cls=0.043, par=0.393, rel=0.014) | elapsed=6.8min\n",
      "  step  525/1000 | step_time= 0.85s | loss=1.4037 (cls=0.097, par=1.252, rel=0.055) | elapsed=6.9min\n",
      "  step  530/1000 | step_time= 0.38s | loss=0.7496 (cls=0.064, par=0.658, rel=0.027) | elapsed=7.0min\n",
      "  step  535/1000 | step_time= 0.69s | loss=0.7338 (cls=0.049, par=0.660, rel=0.024) | elapsed=7.0min\n",
      "  step  540/1000 | step_time= 0.86s | loss=0.4311 (cls=0.021, par=0.402, rel=0.008) | elapsed=7.1min\n",
      "  step  545/1000 | step_time= 0.86s | loss=0.5619 (cls=0.026, par=0.523, rel=0.013) | elapsed=7.2min\n",
      "  step  550/1000 | step_time= 0.78s | loss=0.4303 (cls=0.059, par=0.362, rel=0.009) | elapsed=7.2min\n",
      "  step  555/1000 | step_time= 0.59s | loss=0.5327 (cls=0.056, par=0.455, rel=0.021) | elapsed=7.3min\n",
      "  step  560/1000 | step_time= 0.86s | loss=0.3609 (cls=0.021, par=0.332, rel=0.008) | elapsed=7.4min\n",
      "  step  565/1000 | step_time= 0.84s | loss=0.8681 (cls=0.078, par=0.755, rel=0.035) | elapsed=7.4min\n",
      "  step  570/1000 | step_time= 0.85s | loss=1.0077 (cls=0.083, par=0.893, rel=0.031) | elapsed=7.5min\n",
      "  step  575/1000 | step_time= 0.76s | loss=0.5586 (cls=0.058, par=0.486, rel=0.015) | elapsed=7.6min\n",
      "  step  580/1000 | step_time= 0.84s | loss=0.7155 (cls=0.059, par=0.638, rel=0.018) | elapsed=7.6min\n",
      "  step  585/1000 | step_time= 0.85s | loss=1.4251 (cls=0.077, par=1.289, rel=0.059) | elapsed=7.7min\n",
      "  step  590/1000 | step_time= 0.87s | loss=0.7034 (cls=0.051, par=0.633, rel=0.019) | elapsed=7.7min\n",
      "  step  595/1000 | step_time= 0.70s | loss=1.1544 (cls=0.071, par=1.043, rel=0.040) | elapsed=7.8min\n",
      "  step  600/1000 | step_time= 0.57s | loss=0.7862 (cls=0.054, par=0.707, rel=0.026) | elapsed=7.9min\n",
      "  step  605/1000 | step_time= 0.95s | loss=0.6342 (cls=0.027, par=0.594, rel=0.013) | elapsed=8.0min\n",
      "  step  610/1000 | step_time= 0.85s | loss=0.6843 (cls=0.047, par=0.623, rel=0.014) | elapsed=8.0min\n",
      "  step  615/1000 | step_time= 0.89s | loss=0.6755 (cls=0.047, par=0.599, rel=0.030) | elapsed=8.1min\n",
      "  step  620/1000 | step_time= 0.52s | loss=0.9445 (cls=0.082, par=0.831, rel=0.031) | elapsed=8.2min\n",
      "  step  625/1000 | step_time= 0.70s | loss=0.7613 (cls=0.053, par=0.685, rel=0.024) | elapsed=8.2min\n",
      "  step  630/1000 | step_time= 0.90s | loss=0.7138 (cls=0.040, par=0.657, rel=0.017) | elapsed=8.3min\n",
      "  step  635/1000 | step_time= 0.73s | loss=0.5662 (cls=0.049, par=0.493, rel=0.024) | elapsed=8.4min\n",
      "  step  640/1000 | step_time= 0.88s | loss=0.7975 (cls=0.058, par=0.716, rel=0.023) | elapsed=8.4min\n",
      "  step  645/1000 | step_time= 0.91s | loss=0.6190 (cls=0.032, par=0.572, rel=0.015) | elapsed=8.5min\n",
      "  step  650/1000 | step_time= 0.77s | loss=0.9170 (cls=0.073, par=0.816, rel=0.028) | elapsed=8.6min\n",
      "  step  655/1000 | step_time= 0.90s | loss=0.4900 (cls=0.027, par=0.445, rel=0.017) | elapsed=8.7min\n",
      "  step  660/1000 | step_time= 0.64s | loss=1.0209 (cls=0.068, par=0.908, rel=0.045) | elapsed=8.7min\n",
      "  step  665/1000 | step_time= 0.89s | loss=1.0041 (cls=0.072, par=0.902, rel=0.030) | elapsed=8.8min\n",
      "  step  670/1000 | step_time= 0.89s | loss=0.4589 (cls=0.055, par=0.394, rel=0.011) | elapsed=8.9min\n",
      "  step  675/1000 | step_time= 0.89s | loss=0.5294 (cls=0.045, par=0.474, rel=0.011) | elapsed=8.9min\n",
      "  step  680/1000 | step_time= 0.90s | loss=0.5678 (cls=0.025, par=0.530, rel=0.014) | elapsed=9.0min\n",
      "  step  685/1000 | step_time= 0.42s | loss=1.2846 (cls=0.125, par=1.119, rel=0.040) | elapsed=9.1min\n",
      "  step  690/1000 | step_time= 0.57s | loss=0.5384 (cls=0.043, par=0.477, rel=0.018) | elapsed=9.1min\n",
      "  step  695/1000 | step_time= 0.93s | loss=0.5428 (cls=0.035, par=0.495, rel=0.014) | elapsed=9.2min\n",
      "  step  700/1000 | step_time= 0.90s | loss=0.6996 (cls=0.043, par=0.640, rel=0.016) | elapsed=9.3min\n",
      "  step  705/1000 | step_time= 0.91s | loss=0.5742 (cls=0.031, par=0.531, rel=0.012) | elapsed=9.3min\n",
      "  step  710/1000 | step_time= 0.74s | loss=0.6710 (cls=0.046, par=0.598, rel=0.027) | elapsed=9.4min\n",
      "  step  715/1000 | step_time= 0.90s | loss=0.8798 (cls=0.050, par=0.813, rel=0.017) | elapsed=9.5min\n",
      "  step  720/1000 | step_time= 0.62s | loss=0.8521 (cls=0.070, par=0.738, rel=0.045) | elapsed=9.6min\n",
      "  step  725/1000 | step_time= 0.91s | loss=0.9270 (cls=0.056, par=0.850, rel=0.021) | elapsed=9.6min\n",
      "  step  730/1000 | step_time= 0.91s | loss=0.7246 (cls=0.045, par=0.657, rel=0.023) | elapsed=9.7min\n",
      "  step  735/1000 | step_time= 0.91s | loss=0.6269 (cls=0.082, par=0.522, rel=0.023) | elapsed=9.8min\n",
      "  step  740/1000 | step_time= 0.83s | loss=0.9246 (cls=0.056, par=0.839, rel=0.029) | elapsed=9.8min\n",
      "  step  745/1000 | step_time= 0.88s | loss=0.8224 (cls=0.077, par=0.723, rel=0.022) | elapsed=9.9min\n",
      "  step  750/1000 | step_time= 0.76s | loss=1.2733 (cls=0.064, par=1.162, rel=0.048) | elapsed=10.0min\n",
      "  step  755/1000 | step_time= 0.91s | loss=0.5875 (cls=0.045, par=0.528, rel=0.015) | elapsed=10.0min\n",
      "  step  760/1000 | step_time= 0.88s | loss=0.9088 (cls=0.079, par=0.806, rel=0.024) | elapsed=10.1min\n",
      "  step  765/1000 | step_time= 0.86s | loss=0.6727 (cls=0.039, par=0.610, rel=0.024) | elapsed=10.1min\n",
      "  step  770/1000 | step_time= 0.86s | loss=1.0284 (cls=0.061, par=0.922, rel=0.046) | elapsed=10.2min\n",
      "  step  775/1000 | step_time= 0.79s | loss=0.8870 (cls=0.036, par=0.829, rel=0.022) | elapsed=10.3min\n",
      "  step  780/1000 | step_time= 0.89s | loss=0.9409 (cls=0.082, par=0.825, rel=0.034) | elapsed=10.3min\n",
      "  step  865/1000 | step_time= 0.60s | loss=0.7230 (cls=0.058, par=0.640, rel=0.025) | elapsed=11.5min\n",
      "  step  870/1000 | step_time= 0.84s | loss=0.4290 (cls=0.058, par=0.361, rel=0.010) | elapsed=11.6min\n",
      "  step  875/1000 | step_time= 0.93s | loss=0.7288 (cls=0.058, par=0.645, rel=0.026) | elapsed=11.7min\n",
      "  step  880/1000 | step_time= 0.99s | loss=1.3069 (cls=0.098, par=1.157, rel=0.052) | elapsed=11.7min\n",
      "  step  885/1000 | step_time= 0.89s | loss=1.0291 (cls=0.060, par=0.927, rel=0.041) | elapsed=11.8min\n",
      "  step  890/1000 | step_time= 0.89s | loss=0.7383 (cls=0.053, par=0.666, rel=0.019) | elapsed=11.9min\n",
      "  step  895/1000 | step_time= 0.85s | loss=1.2912 (cls=0.086, par=1.161, rel=0.045) | elapsed=11.9min\n",
      "  step  900/1000 | step_time= 0.77s | loss=1.1792 (cls=0.102, par=1.044, rel=0.033) | elapsed=12.0min\n",
      "  step  905/1000 | step_time= 0.90s | loss=0.9710 (cls=0.067, par=0.865, rel=0.039) | elapsed=12.1min\n",
      "  step  910/1000 | step_time= 0.85s | loss=0.7694 (cls=0.061, par=0.692, rel=0.017) | elapsed=12.2min\n",
      "  step  915/1000 | step_time= 0.91s | loss=0.5825 (cls=0.027, par=0.549, rel=0.007) | elapsed=12.2min\n",
      "  step  920/1000 | step_time= 0.66s | loss=1.1278 (cls=0.075, par=1.013, rel=0.040) | elapsed=12.3min\n",
      "  step  925/1000 | step_time= 0.89s | loss=0.4205 (cls=0.018, par=0.396, rel=0.006) | elapsed=12.3min\n",
      "  step  930/1000 | step_time= 0.91s | loss=0.4973 (cls=0.017, par=0.474, rel=0.007) | elapsed=12.4min\n",
      "  step  935/1000 | step_time= 0.90s | loss=1.0053 (cls=0.097, par=0.873, rel=0.036) | elapsed=12.5min\n",
      "  step  940/1000 | step_time= 0.89s | loss=0.9254 (cls=0.056, par=0.839, rel=0.031) | elapsed=12.6min\n",
      "  step  945/1000 | step_time= 0.91s | loss=0.6937 (cls=0.062, par=0.611, rel=0.020) | elapsed=12.6min\n",
      "  step  950/1000 | step_time= 0.76s | loss=1.2322 (cls=0.061, par=1.130, rel=0.041) | elapsed=12.7min\n",
      "  step  955/1000 | step_time= 0.89s | loss=0.8496 (cls=0.052, par=0.773, rel=0.024) | elapsed=12.8min\n",
      "  step  960/1000 | step_time= 0.90s | loss=1.0395 (cls=0.076, par=0.932, rel=0.031) | elapsed=12.8min\n",
      "  step  965/1000 | step_time= 0.97s | loss=0.9411 (cls=0.079, par=0.836, rel=0.026) | elapsed=12.9min\n",
      "  step  970/1000 | step_time= 0.89s | loss=1.1179 (cls=0.068, par=1.006, rel=0.045) | elapsed=13.0min\n",
      "  step  975/1000 | step_time= 0.67s | loss=0.5007 (cls=0.054, par=0.429, rel=0.018) | elapsed=13.0min\n",
      "  step  980/1000 | step_time= 0.86s | loss=1.4364 (cls=0.133, par=1.236, rel=0.067) | elapsed=13.1min\n",
      "  step  985/1000 | step_time= 0.75s | loss=1.1628 (cls=0.099, par=1.012, rel=0.052) | elapsed=13.1min\n",
      "  step  990/1000 | step_time= 0.73s | loss=1.1088 (cls=0.046, par=1.034, rel=0.030) | elapsed=13.2min\n",
      "  step  995/1000 | step_time= 0.62s | loss=0.8814 (cls=0.061, par=0.791, rel=0.029) | elapsed=13.3min\n",
      "  step 1000/1000 | step_time= 0.69s | loss=0.5319 (cls=0.029, par=0.486, rel=0.017) | elapsed=13.3min\n",
      "[Train] epoch done in 13.34 min\n",
      "2 {'loss': 0.894334643393755, 'loss_cls': 0.06128766025230289, 'loss_par': 0.8037800373733044, 'loss_rel': 0.029266946077812463}\n"
     ]
    }
   ],
   "source": [
    "cfg.epochs = 2\n",
    "cfg.num_workers = 0\n",
    "\n",
    "model = DSPSModel(\n",
    "    num_classes=num_classes,\n",
    "    num_rel=num_rel,\n",
    "    M_cp=M_cp,\n",
    "    cfg=cfg,\n",
    "    use_text=False,\n",
    "    use_visual=False,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-2)\n",
    "focal_cls = FocalLoss(gamma=2.0, alpha=0.25).to(device)\n",
    "focal_rel = FocalLoss(gamma=2.0, alpha=0.25).to(device)\n",
    "\n",
    "for ep in range(1, cfg.epochs+1):\n",
    "    tr = train_one_epoch(model, train_loader, optimizer, cfg, focal_cls, focal_rel)\n",
    "    print(ep, tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4e38104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train] start epoch, num_docs = 1000\n",
      "  step    1/1000 | step_time= 0.57s | loss=0.5724 (cls=0.053, par=0.498, rel=0.021) | elapsed=0.0min\n",
      "    sanity: seq_len=333, avg_parent_candidates=167.0\n",
      "    sanity: seq_len=327, avg_parent_candidates=164.0\n",
      "  step    5/1000 | step_time= 0.90s | loss=0.4039 (cls=0.018, par=0.381, rel=0.005) | elapsed=0.1min\n",
      "  step   10/1000 | step_time= 0.88s | loss=0.7159 (cls=0.045, par=0.656, rel=0.014) | elapsed=0.1min\n",
      "  step   15/1000 | step_time= 0.88s | loss=0.8368 (cls=0.060, par=0.749, rel=0.028) | elapsed=0.2min\n",
      "  step   20/1000 | step_time= 0.91s | loss=0.8008 (cls=0.055, par=0.721, rel=0.025) | elapsed=0.3min\n",
      "  step   25/1000 | step_time= 0.90s | loss=0.8840 (cls=0.087, par=0.775, rel=0.022) | elapsed=0.3min\n",
      "  step   30/1000 | step_time= 0.89s | loss=0.6159 (cls=0.044, par=0.555, rel=0.018) | elapsed=0.4min\n",
      "  step   35/1000 | step_time= 0.89s | loss=0.3452 (cls=0.035, par=0.301, rel=0.008) | elapsed=0.5min\n",
      "  step   40/1000 | step_time= 0.96s | loss=0.8597 (cls=0.082, par=0.748, rel=0.029) | elapsed=0.5min\n",
      "  step   45/1000 | step_time= 0.58s | loss=0.6392 (cls=0.049, par=0.572, rel=0.018) | elapsed=0.6min\n",
      "  step   50/1000 | step_time= 0.72s | loss=0.6246 (cls=0.048, par=0.549, rel=0.027) | elapsed=0.7min\n",
      "  step   55/1000 | step_time= 0.78s | loss=0.9199 (cls=0.076, par=0.771, rel=0.073) | elapsed=0.7min\n",
      "  step   60/1000 | step_time= 0.87s | loss=0.6348 (cls=0.055, par=0.552, rel=0.028) | elapsed=0.8min\n",
      "  step   65/1000 | step_time= 0.83s | loss=1.0485 (cls=0.107, par=0.905, rel=0.037) | elapsed=0.9min\n",
      "  step   70/1000 | step_time= 0.62s | loss=0.9134 (cls=0.051, par=0.824, rel=0.038) | elapsed=0.9min\n",
      "  step   75/1000 | step_time= 0.85s | loss=0.6379 (cls=0.043, par=0.571, rel=0.023) | elapsed=1.0min\n",
      "  step   80/1000 | step_time= 0.89s | loss=0.4358 (cls=0.055, par=0.373, rel=0.008) | elapsed=1.0min\n",
      "  step   85/1000 | step_time= 0.75s | loss=0.4583 (cls=0.033, par=0.410, rel=0.016) | elapsed=1.1min\n",
      "  step   90/1000 | step_time= 0.87s | loss=0.5050 (cls=0.035, par=0.453, rel=0.017) | elapsed=1.2min\n",
      "  step   95/1000 | step_time= 0.87s | loss=0.7017 (cls=0.039, par=0.645, rel=0.017) | elapsed=1.3min\n",
      "  step  100/1000 | step_time= 0.77s | loss=0.6169 (cls=0.038, par=0.561, rel=0.018) | elapsed=1.3min\n",
      "  step  105/1000 | step_time= 0.87s | loss=0.5296 (cls=0.027, par=0.493, rel=0.009) | elapsed=1.4min\n",
      "  step  110/1000 | step_time= 0.78s | loss=0.9493 (cls=0.079, par=0.836, rel=0.034) | elapsed=1.4min\n",
      "  step  115/1000 | step_time= 0.87s | loss=0.9777 (cls=0.044, par=0.906, rel=0.028) | elapsed=1.5min\n",
      "  step  120/1000 | step_time= 0.85s | loss=1.0347 (cls=0.061, par=0.945, rel=0.028) | elapsed=1.6min\n",
      "  step  125/1000 | step_time= 0.96s | loss=0.8333 (cls=0.067, par=0.740, rel=0.026) | elapsed=1.7min\n",
      "  step  130/1000 | step_time= 0.85s | loss=0.8379 (cls=0.034, par=0.785, rel=0.019) | elapsed=1.7min\n",
      "  step  135/1000 | step_time= 0.59s | loss=0.9521 (cls=0.071, par=0.840, rel=0.041) | elapsed=1.8min\n",
      "  step  140/1000 | step_time= 0.86s | loss=0.9182 (cls=0.048, par=0.847, rel=0.023) | elapsed=1.8min\n",
      "  step  145/1000 | step_time= 0.87s | loss=0.4899 (cls=0.018, par=0.459, rel=0.012) | elapsed=1.9min\n",
      "  step  150/1000 | step_time= 0.71s | loss=0.8469 (cls=0.044, par=0.784, rel=0.018) | elapsed=2.0min\n",
      "  step  155/1000 | step_time= 0.58s | loss=0.7802 (cls=0.068, par=0.687, rel=0.025) | elapsed=2.0min\n",
      "  step  160/1000 | step_time= 0.22s | loss=0.3898 (cls=0.050, par=0.324, rel=0.016) | elapsed=2.1min\n",
      "  step  165/1000 | step_time= 0.60s | loss=1.0114 (cls=0.082, par=0.890, rel=0.040) | elapsed=2.2min\n",
      "  step  170/1000 | step_time= 0.68s | loss=0.7174 (cls=0.083, par=0.617, rel=0.018) | elapsed=2.2min\n",
      "  step  175/1000 | step_time= 0.53s | loss=0.4799 (cls=0.045, par=0.413, rel=0.021) | elapsed=2.3min\n",
      "  step  180/1000 | step_time= 0.86s | loss=1.0789 (cls=0.058, par=0.988, rel=0.033) | elapsed=2.4min\n",
      "  step  185/1000 | step_time= 0.98s | loss=0.6547 (cls=0.026, par=0.611, rel=0.018) | elapsed=2.4min\n",
      "  step  190/1000 | step_time= 0.86s | loss=0.9298 (cls=0.072, par=0.829, rel=0.029) | elapsed=2.5min\n",
      "  step  195/1000 | step_time= 0.59s | loss=0.8398 (cls=0.054, par=0.757, rel=0.029) | elapsed=2.6min\n",
      "  step  200/1000 | step_time= 0.93s | loss=0.5171 (cls=0.063, par=0.434, rel=0.021) | elapsed=2.6min\n",
      "  step  205/1000 | step_time= 0.89s | loss=0.9415 (cls=0.079, par=0.833, rel=0.030) | elapsed=2.7min\n",
      "  step  210/1000 | step_time= 0.70s | loss=0.5486 (cls=0.057, par=0.480, rel=0.012) | elapsed=2.8min\n",
      "  step  215/1000 | step_time= 0.90s | loss=1.1597 (cls=0.073, par=1.035, rel=0.052) | elapsed=2.8min\n",
      "  step  220/1000 | step_time= 0.89s | loss=0.4357 (cls=0.020, par=0.406, rel=0.010) | elapsed=2.9min\n",
      "  step  225/1000 | step_time= 0.89s | loss=0.8847 (cls=0.066, par=0.797, rel=0.022) | elapsed=3.0min\n",
      "  step  230/1000 | step_time= 0.89s | loss=0.4542 (cls=0.025, par=0.417, rel=0.011) | elapsed=3.1min\n",
      "  step  235/1000 | step_time= 0.89s | loss=0.4224 (cls=0.040, par=0.375, rel=0.008) | elapsed=3.1min\n",
      "  step  240/1000 | step_time= 0.78s | loss=0.7797 (cls=0.073, par=0.684, rel=0.022) | elapsed=3.2min\n",
      "  step  245/1000 | step_time= 0.91s | loss=0.5786 (cls=0.053, par=0.513, rel=0.013) | elapsed=3.3min\n",
      "  step  250/1000 | step_time= 0.59s | loss=0.4602 (cls=0.060, par=0.389, rel=0.011) | elapsed=3.3min\n",
      "  step  255/1000 | step_time= 0.77s | loss=0.6978 (cls=0.040, par=0.637, rel=0.020) | elapsed=3.4min\n",
      "  step  260/1000 | step_time= 0.66s | loss=0.7173 (cls=0.077, par=0.625, rel=0.016) | elapsed=3.5min\n",
      "  step  265/1000 | step_time= 0.85s | loss=0.5919 (cls=0.026, par=0.551, rel=0.015) | elapsed=3.5min\n",
      "  step  270/1000 | step_time= 0.85s | loss=0.6999 (cls=0.041, par=0.634, rel=0.025) | elapsed=3.6min\n",
      "  step  275/1000 | step_time= 0.89s | loss=0.4286 (cls=0.034, par=0.383, rel=0.012) | elapsed=3.7min\n",
      "  step  280/1000 | step_time= 0.86s | loss=0.5095 (cls=0.039, par=0.459, rel=0.012) | elapsed=3.7min\n",
      "  step  285/1000 | step_time= 0.82s | loss=0.6476 (cls=0.046, par=0.584, rel=0.018) | elapsed=3.8min\n",
      "  step  290/1000 | step_time= 0.85s | loss=0.8283 (cls=0.063, par=0.735, rel=0.030) | elapsed=3.9min\n",
      "  step  295/1000 | step_time= 0.90s | loss=0.3426 (cls=0.019, par=0.317, rel=0.006) | elapsed=3.9min\n",
      "  step  300/1000 | step_time= 0.91s | loss=0.6912 (cls=0.049, par=0.626, rel=0.017) | elapsed=4.0min\n",
      "  step  305/1000 | step_time= 0.86s | loss=0.9342 (cls=0.116, par=0.792, rel=0.026) | elapsed=4.1min\n",
      "  step  310/1000 | step_time= 0.91s | loss=0.6458 (cls=0.036, par=0.595, rel=0.015) | elapsed=4.2min\n",
      "  step  315/1000 | step_time= 0.91s | loss=0.4775 (cls=0.037, par=0.429, rel=0.012) | elapsed=4.2min\n",
      "  step  320/1000 | step_time= 0.94s | loss=0.5829 (cls=0.061, par=0.506, rel=0.016) | elapsed=4.3min\n",
      "  step  325/1000 | step_time= 0.87s | loss=0.7112 (cls=0.100, par=0.590, rel=0.021) | elapsed=4.4min\n",
      "  step  330/1000 | step_time= 0.30s | loss=1.1987 (cls=0.084, par=1.065, rel=0.049) | elapsed=4.4min\n",
      "  step  335/1000 | step_time= 0.91s | loss=0.6992 (cls=0.042, par=0.639, rel=0.018) | elapsed=4.5min\n",
      "  step  340/1000 | step_time= 0.38s | loss=0.4657 (cls=0.055, par=0.396, rel=0.014) | elapsed=4.5min\n",
      "  step  345/1000 | step_time= 0.65s | loss=1.1686 (cls=0.061, par=1.062, rel=0.045) | elapsed=4.6min\n",
      "  step  350/1000 | step_time= 0.89s | loss=1.0880 (cls=0.065, par=0.991, rel=0.032) | elapsed=4.7min\n",
      "  step  355/1000 | step_time= 0.90s | loss=0.4316 (cls=0.028, par=0.394, rel=0.009) | elapsed=4.7min\n",
      "  step  360/1000 | step_time= 0.90s | loss=0.6204 (cls=0.033, par=0.569, rel=0.018) | elapsed=4.8min\n",
      "  step  365/1000 | step_time= 0.89s | loss=0.4025 (cls=0.024, par=0.369, rel=0.009) | elapsed=4.9min\n",
      "  step  370/1000 | step_time= 0.89s | loss=0.6704 (cls=0.059, par=0.591, rel=0.020) | elapsed=4.9min\n",
      "  step  375/1000 | step_time= 0.85s | loss=0.9414 (cls=0.045, par=0.866, rel=0.031) | elapsed=5.0min\n",
      "  step  380/1000 | step_time= 0.82s | loss=1.2365 (cls=0.089, par=1.104, rel=0.044) | elapsed=5.1min\n",
      "  step  385/1000 | step_time= 0.88s | loss=0.4815 (cls=0.052, par=0.419, rel=0.011) | elapsed=5.2min\n",
      "  step  390/1000 | step_time= 0.88s | loss=0.5576 (cls=0.021, par=0.527, rel=0.010) | elapsed=5.2min\n",
      "  step  395/1000 | step_time= 0.57s | loss=0.8310 (cls=0.064, par=0.743, rel=0.024) | elapsed=5.3min\n",
      "  step  400/1000 | step_time= 0.88s | loss=0.4920 (cls=0.027, par=0.456, rel=0.008) | elapsed=5.4min\n",
      "  step  405/1000 | step_time= 0.89s | loss=0.3922 (cls=0.017, par=0.366, rel=0.009) | elapsed=5.4min\n",
      "  step  410/1000 | step_time= 0.89s | loss=0.3492 (cls=0.015, par=0.328, rel=0.007) | elapsed=5.5min\n",
      "  step  415/1000 | step_time= 0.90s | loss=0.5074 (cls=0.030, par=0.466, rel=0.011) | elapsed=5.6min\n",
      "  step  420/1000 | step_time= 0.60s | loss=1.1837 (cls=0.086, par=1.051, rel=0.047) | elapsed=5.6min\n",
      "  step  425/1000 | step_time= 0.88s | loss=1.3178 (cls=0.078, par=1.191, rel=0.049) | elapsed=5.7min\n",
      "  step  430/1000 | step_time= 0.86s | loss=0.9848 (cls=0.079, par=0.873, rel=0.033) | elapsed=5.8min\n",
      "  step  435/1000 | step_time= 0.54s | loss=0.7881 (cls=0.079, par=0.681, rel=0.028) | elapsed=5.8min\n",
      "  step  440/1000 | step_time= 0.86s | loss=0.6532 (cls=0.059, par=0.581, rel=0.013) | elapsed=5.9min\n",
      "  step  445/1000 | step_time= 0.78s | loss=1.2427 (cls=0.109, par=1.099, rel=0.035) | elapsed=6.0min\n",
      "  step  450/1000 | step_time= 0.99s | loss=0.5722 (cls=0.051, par=0.505, rel=0.017) | elapsed=6.0min\n",
      "  step  455/1000 | step_time= 0.90s | loss=0.5927 (cls=0.049, par=0.532, rel=0.012) | elapsed=6.1min\n",
      "  step  460/1000 | step_time= 0.63s | loss=0.3662 (cls=0.033, par=0.325, rel=0.009) | elapsed=6.2min\n",
      "  step  465/1000 | step_time= 0.87s | loss=0.9246 (cls=0.074, par=0.830, rel=0.021) | elapsed=6.2min\n",
      "  step  470/1000 | step_time= 0.32s | loss=0.9583 (cls=0.056, par=0.869, rel=0.034) | elapsed=6.3min\n",
      "  step  475/1000 | step_time= 0.88s | loss=0.4044 (cls=0.036, par=0.359, rel=0.009) | elapsed=6.4min\n",
      "  step  480/1000 | step_time= 0.88s | loss=0.5343 (cls=0.032, par=0.487, rel=0.016) | elapsed=6.4min\n",
      "  step  485/1000 | step_time= 0.88s | loss=0.3512 (cls=0.043, par=0.302, rel=0.007) | elapsed=6.5min\n",
      "  step  490/1000 | step_time= 0.45s | loss=0.5697 (cls=0.077, par=0.472, rel=0.020) | elapsed=6.6min\n",
      "  step  495/1000 | step_time= 0.90s | loss=0.3394 (cls=0.016, par=0.319, rel=0.005) | elapsed=6.6min\n",
      "  step  500/1000 | step_time= 0.87s | loss=0.6532 (cls=0.060, par=0.568, rel=0.025) | elapsed=6.7min\n",
      "  step  505/1000 | step_time= 0.87s | loss=0.7583 (cls=0.069, par=0.668, rel=0.022) | elapsed=6.8min\n",
      "  step  510/1000 | step_time= 0.87s | loss=0.6916 (cls=0.056, par=0.612, rel=0.023) | elapsed=6.8min\n",
      "  step  515/1000 | step_time= 0.25s | loss=0.7168 (cls=0.103, par=0.558, rel=0.056) | elapsed=6.9min\n",
      "  step  520/1000 | step_time= 0.39s | loss=1.3617 (cls=0.095, par=1.200, rel=0.068) | elapsed=7.0min\n",
      "  step  525/1000 | step_time= 0.90s | loss=0.5929 (cls=0.047, par=0.531, rel=0.015) | elapsed=7.0min\n",
      "  step  530/1000 | step_time= 0.89s | loss=0.4031 (cls=0.033, par=0.361, rel=0.009) | elapsed=7.1min\n",
      "  step  535/1000 | step_time= 0.66s | loss=0.3221 (cls=0.050, par=0.260, rel=0.012) | elapsed=7.2min\n",
      "  step  540/1000 | step_time= 0.84s | loss=0.6349 (cls=0.048, par=0.575, rel=0.012) | elapsed=7.2min\n",
      "  step  545/1000 | step_time= 0.86s | loss=0.5448 (cls=0.051, par=0.476, rel=0.018) | elapsed=7.3min\n",
      "  step  550/1000 | step_time= 0.85s | loss=0.8172 (cls=0.038, par=0.763, rel=0.015) | elapsed=7.4min\n",
      "  step  555/1000 | step_time= 0.86s | loss=0.4608 (cls=0.018, par=0.435, rel=0.008) | elapsed=7.4min\n",
      "  step  560/1000 | step_time= 0.85s | loss=0.7227 (cls=0.048, par=0.655, rel=0.020) | elapsed=7.5min\n",
      "  step  565/1000 | step_time= 0.72s | loss=0.9747 (cls=0.050, par=0.899, rel=0.027) | elapsed=7.6min\n",
      "  step  570/1000 | step_time= 0.83s | loss=0.5270 (cls=0.037, par=0.482, rel=0.007) | elapsed=7.6min\n",
      "  step  575/1000 | step_time= 0.86s | loss=0.4590 (cls=0.056, par=0.394, rel=0.009) | elapsed=7.7min\n",
      "  step  580/1000 | step_time= 0.90s | loss=0.4404 (cls=0.032, par=0.397, rel=0.012) | elapsed=7.7min\n",
      "  step  585/1000 | step_time= 0.87s | loss=0.7608 (cls=0.039, par=0.699, rel=0.023) | elapsed=7.8min\n",
      "  step  590/1000 | step_time= 0.66s | loss=0.5534 (cls=0.052, par=0.485, rel=0.016) | elapsed=7.9min\n",
      "  step  595/1000 | step_time= 0.86s | loss=1.3218 (cls=0.062, par=1.218, rel=0.043) | elapsed=7.9min\n",
      "  step  600/1000 | step_time= 0.91s | loss=0.7091 (cls=0.042, par=0.649, rel=0.018) | elapsed=8.0min\n",
      "  step  605/1000 | step_time= 0.91s | loss=0.4729 (cls=0.023, par=0.437, rel=0.012) | elapsed=8.1min\n",
      "  step  610/1000 | step_time= 0.63s | loss=0.8859 (cls=0.080, par=0.782, rel=0.023) | elapsed=8.1min\n",
      "  step  615/1000 | step_time= 0.88s | loss=0.5540 (cls=0.063, par=0.480, rel=0.011) | elapsed=8.2min\n",
      "  step  620/1000 | step_time= 0.97s | loss=0.7050 (cls=0.060, par=0.616, rel=0.029) | elapsed=8.3min\n",
      "  step  625/1000 | step_time= 0.89s | loss=0.4432 (cls=0.040, par=0.391, rel=0.012) | elapsed=8.3min\n",
      "  step  630/1000 | step_time= 0.56s | loss=0.3968 (cls=0.050, par=0.330, rel=0.016) | elapsed=8.4min\n",
      "  step  635/1000 | step_time= 0.88s | loss=0.7857 (cls=0.102, par=0.665, rel=0.019) | elapsed=8.5min\n",
      "  step  640/1000 | step_time= 0.67s | loss=0.3750 (cls=0.023, par=0.338, rel=0.013) | elapsed=8.5min\n",
      "  step  645/1000 | step_time= 0.87s | loss=0.3381 (cls=0.017, par=0.316, rel=0.005) | elapsed=8.6min\n",
      "  step  650/1000 | step_time= 0.86s | loss=0.6671 (cls=0.057, par=0.595, rel=0.016) | elapsed=8.7min\n",
      "  step  655/1000 | step_time= 0.87s | loss=1.3099 (cls=0.076, par=1.192, rel=0.041) | elapsed=8.7min\n",
      "  step  660/1000 | step_time= 0.90s | loss=0.3355 (cls=0.017, par=0.314, rel=0.005) | elapsed=8.8min\n",
      "  step  665/1000 | step_time= 0.99s | loss=0.5191 (cls=0.022, par=0.487, rel=0.009) | elapsed=8.9min\n",
      "  step  670/1000 | step_time= 0.91s | loss=0.4172 (cls=0.022, par=0.383, rel=0.012) | elapsed=8.9min\n",
      "  step  675/1000 | step_time= 0.25s | loss=1.3730 (cls=0.116, par=1.187, rel=0.069) | elapsed=9.0min\n",
      "  step  680/1000 | step_time= 0.89s | loss=0.3825 (cls=0.020, par=0.354, rel=0.009) | elapsed=9.1min\n",
      "  step  685/1000 | step_time= 0.89s | loss=0.9638 (cls=0.070, par=0.868, rel=0.026) | elapsed=9.1min\n",
      "  step  690/1000 | step_time= 0.52s | loss=0.2548 (cls=0.038, par=0.212, rel=0.005) | elapsed=9.2min\n",
      "  step  695/1000 | step_time= 0.89s | loss=1.0506 (cls=0.058, par=0.959, rel=0.034) | elapsed=9.3min\n",
      "  step  700/1000 | step_time= 0.89s | loss=0.6315 (cls=0.046, par=0.570, rel=0.016) | elapsed=9.3min\n",
      "  step  705/1000 | step_time= 0.65s | loss=0.7281 (cls=0.039, par=0.660, rel=0.029) | elapsed=9.4min\n",
      "  step  710/1000 | step_time= 0.89s | loss=0.4230 (cls=0.024, par=0.390, rel=0.010) | elapsed=9.5min\n",
      "  step  715/1000 | step_time= 0.89s | loss=0.9853 (cls=0.059, par=0.902, rel=0.025) | elapsed=9.5min\n",
      "  step  720/1000 | step_time= 0.69s | loss=0.8500 (cls=0.063, par=0.760, rel=0.027) | elapsed=9.6min\n",
      "  step  725/1000 | step_time= 0.91s | loss=0.3072 (cls=0.014, par=0.289, rel=0.004) | elapsed=9.7min\n",
      "  step  730/1000 | step_time= 0.87s | loss=0.7206 (cls=0.060, par=0.631, rel=0.029) | elapsed=9.7min\n",
      "  step  735/1000 | step_time= 0.90s | loss=0.4236 (cls=0.047, par=0.365, rel=0.012) | elapsed=9.8min\n",
      "  step  740/1000 | step_time= 0.35s | loss=0.6885 (cls=0.079, par=0.569, rel=0.041) | elapsed=9.9min\n",
      "  step  745/1000 | step_time= 0.88s | loss=0.6868 (cls=0.049, par=0.622, rel=0.016) | elapsed=9.9min\n",
      "  step  750/1000 | step_time= 0.55s | loss=0.3832 (cls=0.045, par=0.326, rel=0.012) | elapsed=10.0min\n",
      "  step  755/1000 | step_time= 0.90s | loss=0.6711 (cls=0.041, par=0.615, rel=0.015) | elapsed=10.1min\n",
      "  step  760/1000 | step_time= 0.89s | loss=0.5613 (cls=0.062, par=0.486, rel=0.013) | elapsed=10.1min\n",
      "  step  765/1000 | step_time= 0.89s | loss=0.4966 (cls=0.031, par=0.449, rel=0.016) | elapsed=10.2min\n",
      "  step  770/1000 | step_time= 0.62s | loss=0.7399 (cls=0.055, par=0.661, rel=0.024) | elapsed=10.3min\n",
      "  step  775/1000 | step_time= 0.54s | loss=0.4416 (cls=0.043, par=0.384, rel=0.014) | elapsed=10.3min\n",
      "  step  780/1000 | step_time= 0.43s | loss=0.4403 (cls=0.040, par=0.387, rel=0.014) | elapsed=10.4min\n",
      "  step  785/1000 | step_time= 0.83s | loss=0.4190 (cls=0.039, par=0.362, rel=0.018) | elapsed=10.5min\n",
      "  step  790/1000 | step_time= 0.72s | loss=0.4970 (cls=0.042, par=0.437, rel=0.018) | elapsed=10.5min\n",
      "  step  795/1000 | step_time= 0.90s | loss=0.5739 (cls=0.052, par=0.501, rel=0.021) | elapsed=10.6min\n",
      "  step  800/1000 | step_time= 0.91s | loss=0.6935 (cls=0.028, par=0.651, rel=0.015) | elapsed=10.7min\n",
      "  step  805/1000 | step_time= 0.44s | loss=0.6349 (cls=0.071, par=0.549, rel=0.015) | elapsed=10.7min\n",
      "  step  810/1000 | step_time= 0.92s | loss=0.8511 (cls=0.064, par=0.757, rel=0.030) | elapsed=10.8min\n",
      "  step  815/1000 | step_time= 0.89s | loss=0.5639 (cls=0.031, par=0.522, rel=0.011) | elapsed=10.9min\n",
      "  step  820/1000 | step_time= 0.33s | loss=0.6676 (cls=0.055, par=0.592, rel=0.020) | elapsed=10.9min\n",
      "  step  825/1000 | step_time= 1.00s | loss=0.5060 (cls=0.032, par=0.464, rel=0.010) | elapsed=11.0min\n",
      "  step  830/1000 | step_time= 0.88s | loss=0.3451 (cls=0.022, par=0.318, rel=0.006) | elapsed=11.1min\n",
      "  step  835/1000 | step_time= 0.30s | loss=1.0305 (cls=0.116, par=0.862, rel=0.053) | elapsed=11.1min\n",
      "  step  840/1000 | step_time= 0.87s | loss=0.9823 (cls=0.057, par=0.892, rel=0.033) | elapsed=11.2min\n",
      "  step  845/1000 | step_time= 0.71s | loss=1.1192 (cls=0.581, par=0.528, rel=0.010) | elapsed=11.3min\n",
      "  step  850/1000 | step_time= 0.81s | loss=0.7874 (cls=0.037, par=0.731, rel=0.020) | elapsed=11.3min\n",
      "  step  855/1000 | step_time= 0.89s | loss=0.4049 (cls=0.020, par=0.374, rel=0.011) | elapsed=11.4min\n",
      "  step  860/1000 | step_time= 0.77s | loss=0.6430 (cls=0.065, par=0.561, rel=0.018) | elapsed=11.4min\n",
      "  step  865/1000 | step_time= 0.85s | loss=0.7971 (cls=0.052, par=0.722, rel=0.023) | elapsed=11.5min\n",
      "  step  870/1000 | step_time= 0.71s | loss=0.7095 (cls=0.041, par=0.648, rel=0.021) | elapsed=11.6min\n",
      "  step  875/1000 | step_time= 0.87s | loss=0.7854 (cls=0.058, par=0.712, rel=0.015) | elapsed=11.6min\n",
      "  step  880/1000 | step_time= 0.44s | loss=1.1482 (cls=0.080, par=1.018, rel=0.050) | elapsed=11.7min\n",
      "  step  885/1000 | step_time= 0.52s | loss=0.5019 (cls=0.047, par=0.442, rel=0.012) | elapsed=11.8min\n",
      "  step  890/1000 | step_time= 0.91s | loss=0.8109 (cls=0.041, par=0.750, rel=0.020) | elapsed=11.8min\n",
      "  step  895/1000 | step_time= 0.87s | loss=0.5710 (cls=0.023, par=0.538, rel=0.010) | elapsed=11.9min\n",
      "  step  900/1000 | step_time= 0.86s | loss=0.9377 (cls=0.050, par=0.855, rel=0.032) | elapsed=11.9min\n",
      "  step  905/1000 | step_time= 0.85s | loss=0.6464 (cls=0.053, par=0.579, rel=0.014) | elapsed=12.0min\n",
      "  step  910/1000 | step_time= 0.57s | loss=0.7406 (cls=0.076, par=0.642, rel=0.022) | elapsed=12.1min\n",
      "  step  915/1000 | step_time= 0.89s | loss=0.4121 (cls=0.017, par=0.388, rel=0.007) | elapsed=12.1min\n",
      "  step  920/1000 | step_time= 0.89s | loss=0.3981 (cls=0.018, par=0.375, rel=0.005) | elapsed=12.2min\n",
      "  step  925/1000 | step_time= 0.62s | loss=0.3524 (cls=0.045, par=0.296, rel=0.012) | elapsed=12.3min\n",
      "  step  930/1000 | step_time= 0.86s | loss=0.6395 (cls=0.037, par=0.590, rel=0.012) | elapsed=12.4min\n",
      "  step  935/1000 | step_time= 0.45s | loss=0.5201 (cls=0.101, par=0.412, rel=0.007) | elapsed=12.4min\n",
      "  step  940/1000 | step_time= 0.67s | loss=0.6628 (cls=0.107, par=0.534, rel=0.022) | elapsed=12.5min\n",
      "  step  945/1000 | step_time= 0.89s | loss=0.5178 (cls=0.060, par=0.439, rel=0.020) | elapsed=12.5min\n",
      "  step  950/1000 | step_time= 0.83s | loss=0.4878 (cls=0.048, par=0.423, rel=0.017) | elapsed=12.6min\n",
      "  step  955/1000 | step_time= 0.91s | loss=0.4479 (cls=0.028, par=0.410, rel=0.010) | elapsed=12.7min\n",
      "  step  960/1000 | step_time= 0.89s | loss=0.7519 (cls=0.050, par=0.681, rel=0.021) | elapsed=12.8min\n",
      "  step  965/1000 | step_time= 0.89s | loss=0.6968 (cls=0.066, par=0.607, rel=0.023) | elapsed=12.8min\n",
      "  step  970/1000 | step_time= 0.89s | loss=0.6471 (cls=0.035, par=0.596, rel=0.017) | elapsed=12.9min\n",
      "  step  975/1000 | step_time= 0.91s | loss=0.5377 (cls=0.053, par=0.471, rel=0.014) | elapsed=13.0min\n",
      "  step  980/1000 | step_time= 0.85s | loss=0.4753 (cls=0.055, par=0.409, rel=0.011) | elapsed=13.0min\n",
      "  step  985/1000 | step_time= 0.91s | loss=0.3815 (cls=0.022, par=0.351, rel=0.009) | elapsed=13.1min\n",
      "  step  990/1000 | step_time= 0.88s | loss=0.7060 (cls=0.054, par=0.629, rel=0.023) | elapsed=13.2min\n",
      "  step  995/1000 | step_time= 0.95s | loss=0.4350 (cls=0.035, par=0.392, rel=0.009) | elapsed=13.3min\n",
      "  step 1000/1000 | step_time= 0.63s | loss=0.6285 (cls=0.054, par=0.556, rel=0.018) | elapsed=13.3min\n",
      "[Train] epoch done in 13.31 min\n",
      "[ep 1] train: {'loss': 0.685865966156125, 'loss_cls': 0.05325906952098012, 'loss_par': 0.6119506717920303, 'loss_rel': 0.020656225103652105}  | test: {'loss': 0.6581302085518836, 'loss_cls': 0.047387752590700986, 'loss_par': 0.5929456651508808, 'loss_rel': 0.017796793021261692}\n",
      "saved: dsps_hrdh_best.pt\n",
      "\n",
      "[Train] start epoch, num_docs = 1000\n",
      "  step    1/1000 | step_time= 0.59s | loss=0.5295 (cls=0.038, par=0.478, rel=0.013) | elapsed=0.0min\n",
      "    sanity: seq_len=347, avg_parent_candidates=174.0\n",
      "    sanity: seq_len=341, avg_parent_candidates=171.0\n",
      "  step    5/1000 | step_time= 0.86s | loss=0.8526 (cls=0.055, par=0.775, rel=0.023) | elapsed=0.1min\n",
      "  step   10/1000 | step_time= 0.85s | loss=0.6045 (cls=0.054, par=0.535, rel=0.015) | elapsed=0.1min\n",
      "  step   15/1000 | step_time= 0.88s | loss=0.7399 (cls=0.052, par=0.667, rel=0.021) | elapsed=0.2min\n",
      "  step   20/1000 | step_time= 0.81s | loss=0.3644 (cls=0.030, par=0.327, rel=0.008) | elapsed=0.3min\n",
      "  step   25/1000 | step_time= 0.72s | loss=0.3892 (cls=0.046, par=0.326, rel=0.017) | elapsed=0.3min\n",
      "  step   30/1000 | step_time= 0.86s | loss=0.6547 (cls=0.041, par=0.597, rel=0.016) | elapsed=0.4min\n",
      "  step   35/1000 | step_time= 0.87s | loss=0.7025 (cls=0.062, par=0.624, rel=0.017) | elapsed=0.5min\n",
      "  step   40/1000 | step_time= 0.60s | loss=0.4478 (cls=0.050, par=0.382, rel=0.016) | elapsed=0.5min\n",
      "  step   45/1000 | step_time= 0.53s | loss=1.3646 (cls=0.123, par=1.195, rel=0.046) | elapsed=0.6min\n",
      "  step   50/1000 | step_time= 0.49s | loss=0.5546 (cls=0.045, par=0.483, rel=0.027) | elapsed=0.6min\n",
      "  step   55/1000 | step_time= 0.85s | loss=0.7021 (cls=0.036, par=0.647, rel=0.019) | elapsed=0.7min\n",
      "  step   60/1000 | step_time= 0.67s | loss=0.5288 (cls=0.081, par=0.436, rel=0.012) | elapsed=0.8min\n",
      "  step   65/1000 | step_time= 0.87s | loss=0.8686 (cls=0.058, par=0.788, rel=0.022) | elapsed=0.8min\n",
      "  step   70/1000 | step_time= 0.57s | loss=0.4431 (cls=0.019, par=0.415, rel=0.009) | elapsed=0.9min\n",
      "  step   75/1000 | step_time= 0.84s | loss=0.6578 (cls=0.076, par=0.564, rel=0.017) | elapsed=1.0min\n",
      "  step   80/1000 | step_time= 0.94s | loss=0.6583 (cls=0.046, par=0.593, rel=0.019) | elapsed=1.0min\n",
      "  step   85/1000 | step_time= 0.85s | loss=0.9633 (cls=0.052, par=0.886, rel=0.025) | elapsed=1.1min\n",
      "  step   90/1000 | step_time= 0.86s | loss=0.2310 (cls=0.013, par=0.212, rel=0.006) | elapsed=1.2min\n",
      "  step   95/1000 | step_time= 0.85s | loss=0.3849 (cls=0.030, par=0.345, rel=0.010) | elapsed=1.2min\n",
      "  step  100/1000 | step_time= 0.85s | loss=0.3895 (cls=0.040, par=0.338, rel=0.012) | elapsed=1.3min\n",
      "  step  105/1000 | step_time= 0.85s | loss=0.4812 (cls=0.050, par=0.419, rel=0.012) | elapsed=1.4min\n",
      "  step  110/1000 | step_time= 0.87s | loss=0.4253 (cls=0.035, par=0.381, rel=0.009) | elapsed=1.5min\n",
      "  step  115/1000 | step_time= 0.84s | loss=0.6079 (cls=0.048, par=0.540, rel=0.020) | elapsed=1.5min\n",
      "  step  120/1000 | step_time= 0.64s | loss=0.3403 (cls=0.025, par=0.305, rel=0.011) | elapsed=1.6min\n",
      "  step  125/1000 | step_time= 0.62s | loss=0.8928 (cls=0.063, par=0.802, rel=0.027) | elapsed=1.6min\n",
      "  step  130/1000 | step_time= 0.87s | loss=0.5451 (cls=0.038, par=0.494, rel=0.013) | elapsed=1.7min\n",
      "  step  135/1000 | step_time= 0.46s | loss=0.2503 (cls=0.038, par=0.207, rel=0.005) | elapsed=1.8min\n",
      "  step  140/1000 | step_time= 0.50s | loss=0.7455 (cls=0.075, par=0.650, rel=0.021) | elapsed=1.8min\n",
      "  step  145/1000 | step_time= 0.45s | loss=0.9622 (cls=0.038, par=0.886, rel=0.038) | elapsed=1.9min\n",
      "  step  150/1000 | step_time= 0.71s | loss=0.5833 (cls=0.077, par=0.489, rel=0.017) | elapsed=1.9min\n",
      "  step  155/1000 | step_time= 0.83s | loss=0.4517 (cls=0.028, par=0.411, rel=0.013) | elapsed=2.0min\n",
      "  step  160/1000 | step_time= 0.86s | loss=0.7670 (cls=0.047, par=0.701, rel=0.019) | elapsed=2.1min\n",
      "  step  165/1000 | step_time= 0.22s | loss=0.4410 (cls=0.052, par=0.378, rel=0.010) | elapsed=2.1min\n",
      "  step  170/1000 | step_time= 0.85s | loss=0.4157 (cls=0.040, par=0.368, rel=0.008) | elapsed=2.2min\n",
      "  step  175/1000 | step_time= 0.85s | loss=0.4145 (cls=0.025, par=0.383, rel=0.006) | elapsed=2.3min\n",
      "  step  180/1000 | step_time= 0.85s | loss=0.2744 (cls=0.015, par=0.257, rel=0.003) | elapsed=2.3min\n",
      "  step  185/1000 | step_time= 0.91s | loss=0.5271 (cls=0.025, par=0.490, rel=0.012) | elapsed=2.4min\n",
      "  step  190/1000 | step_time= 0.58s | loss=0.2206 (cls=0.026, par=0.189, rel=0.006) | elapsed=2.5min\n",
      "  step  195/1000 | step_time= 0.86s | loss=0.6547 (cls=0.037, par=0.604, rel=0.013) | elapsed=2.5min\n",
      "  step  200/1000 | step_time= 0.28s | loss=0.3129 (cls=0.058, par=0.239, rel=0.016) | elapsed=2.6min\n",
      "  step  205/1000 | step_time= 0.38s | loss=0.3733 (cls=0.043, par=0.314, rel=0.016) | elapsed=2.7min\n",
      "  step  210/1000 | step_time= 0.40s | loss=0.5005 (cls=0.046, par=0.445, rel=0.010) | elapsed=2.7min\n",
      "  step  215/1000 | step_time= 0.84s | loss=0.7116 (cls=0.028, par=0.667, rel=0.017) | elapsed=2.8min\n",
      "  step  220/1000 | step_time= 0.82s | loss=0.4670 (cls=0.024, par=0.430, rel=0.013) | elapsed=2.8min\n",
      "  step  225/1000 | step_time= 1.08s | loss=0.3989 (cls=0.022, par=0.368, rel=0.009) | elapsed=2.9min\n",
      "  step  230/1000 | step_time= 0.90s | loss=0.6359 (cls=0.054, par=0.562, rel=0.020) | elapsed=3.0min\n",
      "  step  235/1000 | step_time= 0.86s | loss=0.5881 (cls=0.057, par=0.516, rel=0.015) | elapsed=3.1min\n",
      "  step  240/1000 | step_time= 0.90s | loss=0.5968 (cls=0.099, par=0.478, rel=0.019) | elapsed=3.2min\n",
      "  step  245/1000 | step_time= 0.84s | loss=0.7819 (cls=0.047, par=0.708, rel=0.027) | elapsed=3.2min\n",
      "  step  250/1000 | step_time= 0.91s | loss=0.7067 (cls=0.041, par=0.644, rel=0.022) | elapsed=3.3min\n",
      "  step  255/1000 | step_time= 0.53s | loss=0.6056 (cls=0.067, par=0.518, rel=0.020) | elapsed=3.4min\n",
      "  step  260/1000 | step_time= 0.84s | loss=0.4811 (cls=0.021, par=0.452, rel=0.008) | elapsed=3.4min\n",
      "  step  265/1000 | step_time= 0.70s | loss=0.6183 (cls=0.039, par=0.570, rel=0.010) | elapsed=3.5min\n",
      "  step  270/1000 | step_time= 0.67s | loss=0.5365 (cls=0.072, par=0.417, rel=0.048) | elapsed=3.5min\n",
      "  step  275/1000 | step_time= 0.94s | loss=0.4774 (cls=0.040, par=0.427, rel=0.010) | elapsed=3.6min\n",
      "  step  280/1000 | step_time= 0.31s | loss=1.1466 (cls=0.083, par=1.021, rel=0.042) | elapsed=3.7min\n",
      "  step  285/1000 | step_time= 0.85s | loss=0.2732 (cls=0.040, par=0.228, rel=0.005) | elapsed=3.7min\n",
      "  step  290/1000 | step_time= 0.36s | loss=0.6855 (cls=0.080, par=0.580, rel=0.026) | elapsed=3.8min\n",
      "  step  295/1000 | step_time= 0.86s | loss=0.3703 (cls=0.013, par=0.350, rel=0.007) | elapsed=3.9min\n",
      "  step  300/1000 | step_time= 0.82s | loss=0.7249 (cls=0.047, par=0.659, rel=0.018) | elapsed=3.9min\n",
      "  step  305/1000 | step_time= 0.85s | loss=0.6998 (cls=0.034, par=0.649, rel=0.016) | elapsed=4.0min\n",
      "  step  310/1000 | step_time= 0.93s | loss=0.6861 (cls=0.051, par=0.620, rel=0.015) | elapsed=4.0min\n",
      "  step  315/1000 | step_time= 0.83s | loss=0.9808 (cls=0.099, par=0.844, rel=0.037) | elapsed=4.1min\n",
      "  step  320/1000 | step_time= 0.49s | loss=1.0477 (cls=0.094, par=0.904, rel=0.050) | elapsed=4.2min\n",
      "  step  325/1000 | step_time= 0.61s | loss=0.6976 (cls=0.073, par=0.603, rel=0.021) | elapsed=4.2min\n",
      "  step  330/1000 | step_time= 0.87s | loss=0.6228 (cls=0.060, par=0.543, rel=0.021) | elapsed=4.3min\n",
      "  step  335/1000 | step_time= 0.87s | loss=0.7025 (cls=0.058, par=0.627, rel=0.017) | elapsed=4.3min\n",
      "  step  340/1000 | step_time= 0.79s | loss=0.3521 (cls=0.023, par=0.319, rel=0.010) | elapsed=4.4min\n",
      "  step  345/1000 | step_time= 0.88s | loss=0.2180 (cls=0.010, par=0.206, rel=0.002) | elapsed=4.5min\n",
      "  step  350/1000 | step_time= 0.81s | loss=0.2143 (cls=0.036, par=0.172, rel=0.006) | elapsed=4.5min\n",
      "  step  355/1000 | step_time= 0.83s | loss=0.8861 (cls=0.058, par=0.806, rel=0.022) | elapsed=4.6min\n",
      "  step  360/1000 | step_time= 0.81s | loss=0.3968 (cls=0.052, par=0.334, rel=0.011) | elapsed=4.7min\n",
      "  step  365/1000 | step_time= 0.86s | loss=1.0086 (cls=0.063, par=0.916, rel=0.030) | elapsed=4.7min\n",
      "  step  370/1000 | step_time= 0.82s | loss=0.3649 (cls=0.036, par=0.323, rel=0.007) | elapsed=4.8min\n",
      "  step  375/1000 | step_time= 0.83s | loss=0.3740 (cls=0.033, par=0.332, rel=0.009) | elapsed=4.9min\n",
      "  step  380/1000 | step_time= 0.81s | loss=0.5539 (cls=0.039, par=0.500, rel=0.015) | elapsed=4.9min\n",
      "  step  385/1000 | step_time= 0.81s | loss=0.3756 (cls=0.019, par=0.347, rel=0.010) | elapsed=5.0min\n",
      "  step  390/1000 | step_time= 0.97s | loss=0.5608 (cls=0.032, par=0.514, rel=0.015) | elapsed=5.0min\n",
      "  step  395/1000 | step_time= 0.76s | loss=0.3077 (cls=0.029, par=0.271, rel=0.008) | elapsed=5.1min\n",
      "  step  400/1000 | step_time= 0.82s | loss=0.6798 (cls=0.040, par=0.621, rel=0.019) | elapsed=5.2min\n",
      "  step  405/1000 | step_time= 0.77s | loss=0.4389 (cls=0.055, par=0.366, rel=0.018) | elapsed=5.2min\n",
      "  step  410/1000 | step_time= 0.65s | loss=0.6109 (cls=0.036, par=0.554, rel=0.022) | elapsed=5.3min\n",
      "  step  415/1000 | step_time= 0.52s | loss=0.3851 (cls=0.038, par=0.330, rel=0.017) | elapsed=5.3min\n",
      "  step  420/1000 | step_time= 0.47s | loss=0.1691 (cls=0.031, par=0.135, rel=0.003) | elapsed=5.4min\n",
      "  step  425/1000 | step_time= 0.83s | loss=0.4524 (cls=0.041, par=0.401, rel=0.011) | elapsed=5.5min\n",
      "  step  430/1000 | step_time= 0.81s | loss=0.3250 (cls=0.027, par=0.290, rel=0.008) | elapsed=5.5min\n",
      "  step  435/1000 | step_time= 0.81s | loss=0.5796 (cls=0.046, par=0.519, rel=0.014) | elapsed=5.6min\n",
      "  step  440/1000 | step_time= 0.94s | loss=0.3790 (cls=0.036, par=0.334, rel=0.009) | elapsed=5.7min\n",
      "  step  445/1000 | step_time= 0.86s | loss=1.1417 (cls=0.049, par=1.065, rel=0.028) | elapsed=5.7min\n",
      "  step  450/1000 | step_time= 0.80s | loss=0.4577 (cls=0.057, par=0.390, rel=0.010) | elapsed=5.8min\n",
      "  step  455/1000 | step_time= 0.49s | loss=0.4209 (cls=0.044, par=0.368, rel=0.009) | elapsed=5.8min\n",
      "  step  460/1000 | step_time= 0.81s | loss=0.3062 (cls=0.022, par=0.278, rel=0.006) | elapsed=5.9min\n",
      "  step  465/1000 | step_time= 0.76s | loss=0.4250 (cls=0.056, par=0.361, rel=0.008) | elapsed=5.9min\n",
      "  step  470/1000 | step_time= 0.81s | loss=0.4574 (cls=0.034, par=0.414, rel=0.010) | elapsed=6.0min\n",
      "  step  475/1000 | step_time= 0.80s | loss=0.5043 (cls=0.032, par=0.463, rel=0.010) | elapsed=6.1min\n",
      "  step  480/1000 | step_time= 0.83s | loss=0.4867 (cls=0.026, par=0.448, rel=0.012) | elapsed=6.1min\n",
      "  step  485/1000 | step_time= 0.84s | loss=0.1884 (cls=0.008, par=0.178, rel=0.003) | elapsed=6.2min\n",
      "  step  490/1000 | step_time= 0.87s | loss=0.5355 (cls=0.053, par=0.472, rel=0.010) | elapsed=6.2min\n",
      "  step  495/1000 | step_time= 0.77s | loss=0.2934 (cls=0.049, par=0.237, rel=0.007) | elapsed=6.3min\n",
      "  step  500/1000 | step_time= 0.87s | loss=0.3722 (cls=0.031, par=0.329, rel=0.012) | elapsed=6.4min\n",
      "  step  505/1000 | step_time= 0.86s | loss=0.5631 (cls=0.045, par=0.504, rel=0.014) | elapsed=6.4min\n",
      "  step  510/1000 | step_time= 0.85s | loss=0.4583 (cls=0.052, par=0.397, rel=0.010) | elapsed=6.5min\n",
      "  step  515/1000 | step_time= 0.91s | loss=0.8398 (cls=0.057, par=0.755, rel=0.028) | elapsed=6.6min\n",
      "  step  520/1000 | step_time= 0.86s | loss=0.5383 (cls=0.035, par=0.493, rel=0.011) | elapsed=6.6min\n",
      "  step  525/1000 | step_time= 0.47s | loss=0.3875 (cls=0.097, par=0.286, rel=0.005) | elapsed=6.7min\n",
      "  step  530/1000 | step_time= 0.85s | loss=0.6225 (cls=0.038, par=0.565, rel=0.019) | elapsed=6.8min\n",
      "  step  535/1000 | step_time= 0.87s | loss=0.5744 (cls=0.033, par=0.530, rel=0.012) | elapsed=6.8min\n",
      "  step  540/1000 | step_time= 0.86s | loss=0.7943 (cls=0.037, par=0.736, rel=0.021) | elapsed=6.9min\n",
      "  step  545/1000 | step_time= 0.65s | loss=0.2520 (cls=0.028, par=0.219, rel=0.005) | elapsed=7.0min\n",
      "  step  550/1000 | step_time= 0.86s | loss=0.4850 (cls=0.049, par=0.427, rel=0.008) | elapsed=7.0min\n",
      "  step  555/1000 | step_time= 0.84s | loss=0.7194 (cls=0.037, par=0.665, rel=0.017) | elapsed=7.1min\n",
      "  step  560/1000 | step_time= 0.85s | loss=0.3510 (cls=0.048, par=0.293, rel=0.010) | elapsed=7.2min\n",
      "  step  565/1000 | step_time= 0.83s | loss=1.1771 (cls=0.071, par=1.070, rel=0.036) | elapsed=7.2min\n",
      "  step  570/1000 | step_time= 0.82s | loss=0.4934 (cls=0.038, par=0.442, rel=0.014) | elapsed=7.3min\n",
      "  step  575/1000 | step_time= 0.83s | loss=0.3112 (cls=0.016, par=0.288, rel=0.007) | elapsed=7.4min\n",
      "  step  580/1000 | step_time= 0.82s | loss=0.4159 (cls=0.045, par=0.358, rel=0.012) | elapsed=7.4min\n",
      "  step  585/1000 | step_time= 0.79s | loss=1.0166 (cls=0.058, par=0.931, rel=0.027) | elapsed=7.5min\n",
      "  step  590/1000 | step_time= 0.55s | loss=0.6189 (cls=0.050, par=0.555, rel=0.014) | elapsed=7.6min\n",
      "  step  595/1000 | step_time= 0.80s | loss=0.2475 (cls=0.019, par=0.226, rel=0.003) | elapsed=7.6min\n",
      "  step  600/1000 | step_time= 0.77s | loss=0.7598 (cls=0.045, par=0.695, rel=0.020) | elapsed=7.7min\n",
      "  step  605/1000 | step_time= 0.25s | loss=0.3140 (cls=0.067, par=0.235, rel=0.011) | elapsed=7.7min\n",
      "  step  610/1000 | step_time= 0.79s | loss=0.6579 (cls=0.062, par=0.571, rel=0.025) | elapsed=7.8min\n",
      "  step  615/1000 | step_time= 0.79s | loss=0.9697 (cls=0.083, par=0.855, rel=0.031) | elapsed=7.8min\n",
      "  step  620/1000 | step_time= 0.52s | loss=0.6338 (cls=0.067, par=0.548, rel=0.019) | elapsed=7.9min\n",
      "  step  625/1000 | step_time= 0.86s | loss=0.6701 (cls=0.055, par=0.596, rel=0.019) | elapsed=8.0min\n",
      "  step  630/1000 | step_time= 0.78s | loss=0.9962 (cls=0.076, par=0.892, rel=0.028) | elapsed=8.0min\n",
      "  step  635/1000 | step_time= 0.81s | loss=0.7272 (cls=0.044, par=0.669, rel=0.015) | elapsed=8.1min\n",
      "  step  640/1000 | step_time= 0.61s | loss=0.9155 (cls=0.073, par=0.810, rel=0.032) | elapsed=8.2min\n",
      "  step  645/1000 | step_time= 0.75s | loss=0.6537 (cls=0.049, par=0.567, rel=0.037) | elapsed=8.2min\n",
      "  step  650/1000 | step_time= 0.78s | loss=0.4300 (cls=0.047, par=0.375, rel=0.008) | elapsed=8.3min\n",
      "  step  655/1000 | step_time= 0.79s | loss=0.6764 (cls=0.041, par=0.618, rel=0.018) | elapsed=8.3min\n",
      "  step  660/1000 | step_time= 0.61s | loss=0.4968 (cls=0.038, par=0.446, rel=0.012) | elapsed=8.4min\n",
      "  step  665/1000 | step_time= 0.58s | loss=1.0054 (cls=0.066, par=0.908, rel=0.031) | elapsed=8.4min\n",
      "  step  670/1000 | step_time= 0.31s | loss=0.3487 (cls=0.040, par=0.290, rel=0.019) | elapsed=8.5min\n",
      "  step  675/1000 | step_time= 0.59s | loss=0.5008 (cls=0.059, par=0.431, rel=0.010) | elapsed=8.6min\n",
      "  step  680/1000 | step_time= 0.78s | loss=0.4904 (cls=0.032, par=0.445, rel=0.014) | elapsed=8.6min\n",
      "  step  685/1000 | step_time= 0.64s | loss=0.4076 (cls=0.032, par=0.364, rel=0.012) | elapsed=8.7min\n",
      "  step  690/1000 | step_time= 0.70s | loss=0.4159 (cls=0.041, par=0.367, rel=0.008) | elapsed=8.7min\n",
      "  step  695/1000 | step_time= 0.76s | loss=0.6665 (cls=0.054, par=0.598, rel=0.014) | elapsed=8.8min\n",
      "  step  700/1000 | step_time= 0.78s | loss=0.3839 (cls=0.026, par=0.351, rel=0.007) | elapsed=8.9min\n",
      "  step  705/1000 | step_time= 0.79s | loss=0.7682 (cls=0.063, par=0.684, rel=0.020) | elapsed=8.9min\n",
      "  step  710/1000 | step_time= 0.28s | loss=0.7941 (cls=0.109, par=0.615, rel=0.070) | elapsed=9.0min\n",
      "  step  715/1000 | step_time= 0.78s | loss=0.7822 (cls=0.053, par=0.704, rel=0.026) | elapsed=9.0min\n",
      "  step  720/1000 | step_time= 0.80s | loss=0.4020 (cls=0.027, par=0.361, rel=0.014) | elapsed=9.1min\n",
      "  step  725/1000 | step_time= 0.60s | loss=1.0667 (cls=0.056, par=0.976, rel=0.035) | elapsed=9.2min\n",
      "  step  730/1000 | step_time= 0.83s | loss=0.5226 (cls=0.053, par=0.456, rel=0.013) | elapsed=9.2min\n",
      "  step  735/1000 | step_time= 0.83s | loss=0.3976 (cls=0.049, par=0.338, rel=0.011) | elapsed=9.3min\n",
      "  step  740/1000 | step_time= 0.84s | loss=0.2303 (cls=0.011, par=0.215, rel=0.003) | elapsed=9.4min\n",
      "  step  745/1000 | step_time= 0.27s | loss=0.1578 (cls=0.028, par=0.124, rel=0.006) | elapsed=9.4min\n",
      "  step  750/1000 | step_time= 0.81s | loss=0.5222 (cls=0.030, par=0.477, rel=0.015) | elapsed=9.5min\n",
      "  step  755/1000 | step_time= 0.83s | loss=0.4681 (cls=0.036, par=0.421, rel=0.011) | elapsed=9.5min\n",
      "  step  760/1000 | step_time= 0.83s | loss=0.3823 (cls=0.020, par=0.357, rel=0.005) | elapsed=9.6min\n",
      "  step  765/1000 | step_time= 0.62s | loss=0.4178 (cls=0.060, par=0.348, rel=0.010) | elapsed=9.7min\n",
      "  step  770/1000 | step_time= 0.83s | loss=0.5033 (cls=0.047, par=0.429, rel=0.027) | elapsed=9.7min\n",
      "  step  775/1000 | step_time= 0.86s | loss=0.5414 (cls=0.027, par=0.498, rel=0.016) | elapsed=9.8min\n",
      "  step  780/1000 | step_time= 0.23s | loss=1.0808 (cls=0.107, par=0.916, rel=0.057) | elapsed=9.9min\n",
      "  step  785/1000 | step_time= 0.33s | loss=0.2994 (cls=0.040, par=0.253, rel=0.006) | elapsed=9.9min\n",
      "  step  790/1000 | step_time= 0.84s | loss=0.6909 (cls=0.035, par=0.645, rel=0.011) | elapsed=10.0min\n",
      "  step  795/1000 | step_time= 0.88s | loss=0.5811 (cls=0.030, par=0.535, rel=0.017) | elapsed=10.1min\n",
      "  step  800/1000 | step_time= 0.79s | loss=0.6903 (cls=0.076, par=0.593, rel=0.022) | elapsed=10.2min\n",
      "  step  805/1000 | step_time= 0.87s | loss=0.4537 (cls=0.040, par=0.401, rel=0.014) | elapsed=10.2min\n",
      "  step  810/1000 | step_time= 0.87s | loss=0.6681 (cls=0.050, par=0.603, rel=0.015) | elapsed=10.3min\n",
      "  step  815/1000 | step_time= 0.94s | loss=1.1358 (cls=0.063, par=1.045, rel=0.027) | elapsed=10.3min\n",
      "  step  820/1000 | step_time= 0.69s | loss=0.3791 (cls=0.039, par=0.327, rel=0.013) | elapsed=10.4min\n",
      "  step  825/1000 | step_time= 0.83s | loss=0.3681 (cls=0.032, par=0.321, rel=0.016) | elapsed=10.5min\n",
      "  step  830/1000 | step_time= 0.80s | loss=0.4911 (cls=0.032, par=0.449, rel=0.010) | elapsed=10.5min\n",
      "  step  835/1000 | step_time= 0.79s | loss=0.5850 (cls=0.060, par=0.510, rel=0.015) | elapsed=10.6min\n",
      "  step  840/1000 | step_time= 0.89s | loss=0.2218 (cls=0.012, par=0.206, rel=0.004) | elapsed=10.7min\n",
      "  step  845/1000 | step_time= 0.79s | loss=0.5897 (cls=0.076, par=0.493, rel=0.021) | elapsed=10.7min\n",
      "  step  850/1000 | step_time= 0.81s | loss=0.4232 (cls=0.015, par=0.403, rel=0.005) | elapsed=10.8min\n",
      "  step  855/1000 | step_time= 0.56s | loss=0.4738 (cls=0.041, par=0.418, rel=0.015) | elapsed=10.8min\n",
      "  step  860/1000 | step_time= 0.31s | loss=1.0020 (cls=0.083, par=0.883, rel=0.035) | elapsed=10.9min\n",
      "  step  865/1000 | step_time= 0.80s | loss=0.5056 (cls=0.046, par=0.448, rel=0.012) | elapsed=11.0min\n",
      "  step  870/1000 | step_time= 0.79s | loss=1.0576 (cls=0.081, par=0.941, rel=0.035) | elapsed=11.0min\n",
      "  step  875/1000 | step_time= 0.82s | loss=0.3059 (cls=0.016, par=0.286, rel=0.003) | elapsed=11.1min\n",
      "  step  880/1000 | step_time= 0.80s | loss=0.6144 (cls=0.055, par=0.547, rel=0.012) | elapsed=11.2min\n",
      "  step  885/1000 | step_time= 0.80s | loss=0.4606 (cls=0.036, par=0.416, rel=0.009) | elapsed=11.2min\n",
      "  step  890/1000 | step_time= 0.80s | loss=0.4595 (cls=0.034, par=0.415, rel=0.010) | elapsed=11.3min\n",
      "  step  895/1000 | step_time= 0.47s | loss=0.9576 (cls=0.069, par=0.868, rel=0.021) | elapsed=11.3min\n",
      "  step  900/1000 | step_time= 0.82s | loss=0.9196 (cls=0.054, par=0.840, rel=0.025) | elapsed=11.4min\n",
      "  step  905/1000 | step_time= 0.83s | loss=0.4566 (cls=0.030, par=0.418, rel=0.009) | elapsed=11.5min\n",
      "  step  910/1000 | step_time= 0.74s | loss=0.8994 (cls=0.056, par=0.814, rel=0.029) | elapsed=11.5min\n",
      "  step  915/1000 | step_time= 0.37s | loss=1.0913 (cls=0.072, par=0.980, rel=0.039) | elapsed=11.6min\n",
      "  step  920/1000 | step_time= 0.61s | loss=0.5918 (cls=0.033, par=0.546, rel=0.013) | elapsed=11.7min\n",
      "  step  925/1000 | step_time= 0.54s | loss=0.8879 (cls=0.083, par=0.773, rel=0.032) | elapsed=11.7min\n",
      "  step  930/1000 | step_time= 0.82s | loss=0.7826 (cls=0.083, par=0.682, rel=0.018) | elapsed=11.8min\n",
      "  step  935/1000 | step_time= 0.41s | loss=0.5762 (cls=0.082, par=0.476, rel=0.018) | elapsed=11.8min\n",
      "  step  940/1000 | step_time= 0.83s | loss=0.5380 (cls=0.038, par=0.480, rel=0.020) | elapsed=11.9min\n",
      "  step  945/1000 | step_time= 0.69s | loss=0.4306 (cls=0.046, par=0.366, rel=0.019) | elapsed=12.0min\n",
      "  step  950/1000 | step_time= 0.66s | loss=0.3714 (cls=0.047, par=0.313, rel=0.011) | elapsed=12.0min\n",
      "  step  955/1000 | step_time= 0.90s | loss=0.4559 (cls=0.028, par=0.414, rel=0.014) | elapsed=12.1min\n",
      "  step  960/1000 | step_time= 0.92s | loss=0.6873 (cls=0.053, par=0.618, rel=0.016) | elapsed=12.2min\n",
      "  step  965/1000 | step_time= 0.70s | loss=0.6421 (cls=0.071, par=0.544, rel=0.028) | elapsed=12.2min\n",
      "  step  970/1000 | step_time= 0.56s | loss=0.4502 (cls=0.046, par=0.394, rel=0.011) | elapsed=12.3min\n",
      "  step  975/1000 | step_time= 0.64s | loss=0.3385 (cls=0.043, par=0.290, rel=0.005) | elapsed=12.4min\n",
      "  step  980/1000 | step_time= 0.81s | loss=0.4556 (cls=0.039, par=0.410, rel=0.007) | elapsed=12.4min\n",
      "  step  985/1000 | step_time= 0.91s | loss=1.0242 (cls=0.063, par=0.930, rel=0.032) | elapsed=12.5min\n",
      "  step  990/1000 | step_time= 0.95s | loss=0.2577 (cls=0.030, par=0.221, rel=0.007) | elapsed=12.5min\n",
      "  step  995/1000 | step_time= 0.94s | loss=0.4120 (cls=0.044, par=0.357, rel=0.012) | elapsed=12.6min\n",
      "  step 1000/1000 | step_time= 0.95s | loss=0.6165 (cls=0.053, par=0.545, rel=0.018) | elapsed=12.7min\n",
      "[Train] epoch done in 12.69 min\n",
      "[ep 2] train: {'loss': 0.5633891981691123, 'loss_cls': 0.04787850289605558, 'loss_par': 0.4994902092143893, 'loss_rel': 0.01602048568474129}  | test: {'loss': 0.5911250470876693, 'loss_cls': 0.04330951202660799, 'loss_par': 0.533139784425497, 'loss_rel': 0.01467575163883157}\n",
      "saved: dsps_hrdh_best.pt\n"
     ]
    }
   ],
   "source": [
    "best = 1e9\n",
    "for ep in range(1, cfg.epochs+1):\n",
    "    tr = train_one_epoch(model, train_loader, optimizer, cfg, focal_cls, focal_rel)\n",
    "    te = eval_one_epoch(model, test_loader, cfg, focal_cls, focal_rel)\n",
    "    print(f\"[ep {ep}] train:\", tr, \" | test:\", te)\n",
    "\n",
    "    if te[\"loss\"] < best:\n",
    "        best = te[\"loss\"]\n",
    "        torch.save(model.state_dict(), \"dsps_hrdh_best.pt\")\n",
    "        print(\"saved: dsps_hrdh_best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa9bca",
   "metadata": {},
   "source": [
    "## 9. 推理与预测导出\n",
    "\n",
    "- 单文档推理\n",
    "- 带关系重组/后处理的推理\n",
    "- 批量导出 split 预测结果到目录\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6a51892",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_doc(model: DSPSModel, doc: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    model.eval()\n",
    "    out = model(doc)\n",
    "\n",
    "    cls = out[\"cls_logits\"].argmax(dim=-1).cpu().tolist()\n",
    "\n",
    "    # parent: each i logits over [ROOT]+past\n",
    "    par = []\n",
    "    for i, logits_i in enumerate(out[\"par_logits\"]):\n",
    "        idx = int(torch.argmax(logits_i).cpu())\n",
    "        p = -1 if idx == 0 else (idx - 1)\n",
    "        par.append(p)\n",
    "\n",
    "    # relation: 这里的 rel_logits 是用 GT parent 生成的；\n",
    "    # 推理时更严格做法：用预测 parent 重算 rel（下面给你重算版本）\n",
    "    # 先给简版：\n",
    "    rel = out[\"rel_logits\"].argmax(dim=-1).cpu().tolist()\n",
    "\n",
    "    return {\"pred_cls\": cls, \"pred_parent\": par, \"pred_rel\": rel}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "597ea42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_doc_with_rel_recompute(model, doc, device):\n",
    "    \"\"\"\n",
    "    返回：\n",
    "      pred_cls: (L,)\n",
    "      pred_parent: (L,)  -1 表示 ROOT\n",
    "      pred_rel: (L,)\n",
    "      also return raw probs/logits if needed\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    out = model(doc)\n",
    "\n",
    "    cls_logits = out[\"cls_logits\"].to(device)          # (L,C)\n",
    "    par_logits_list = out[\"par_logits\"]                # list of (i+1,)\n",
    "    L = cls_logits.shape[0]\n",
    "\n",
    "    pred_cls = cls_logits.argmax(dim=-1).detach().cpu().tolist()\n",
    "\n",
    "    # parent decode: candidates = [ROOT] + [0..i-1]\n",
    "    pred_parent = []\n",
    "    for i, logits_i in enumerate(par_logits_list):\n",
    "        idx = int(torch.argmax(logits_i).item())\n",
    "        p = -1 if idx == 0 else (idx - 1)\n",
    "        pred_parent.append(p)\n",
    "\n",
    "    # recompute relation logits using predicted parent\n",
    "    # need access to h_seq and root inside the model; easiest: re-run minimal pieces here\n",
    "    # We can reconstruct h_seq by running embeddings+encoder+gru again using model modules\n",
    "    units = doc[\"units\"]\n",
    "    doc_id = doc[\"doc_id\"]\n",
    "    page_images = doc[\"page_images\"]\n",
    "\n",
    "    # embeddings sum (same as forward)\n",
    "    x = model.layout_pos_page(units, page_images)\n",
    "    if model.use_text:\n",
    "        texts = [u.get(\"text\",\"\") for u in units]\n",
    "        x = x + model.text_emb(doc_id, texts)\n",
    "    if model.use_visual:\n",
    "        x = x + model.vis_emb(units, page_images)\n",
    "    x = model.fuse_ln(x)                       # (L,d)\n",
    "    x_star = model.encoder(x.unsqueeze(0)).squeeze(0)  # (L,d)\n",
    "    root = x_star.mean(dim=0, keepdim=True)    # (1,d)\n",
    "    h_seq = model.gru(x_star.unsqueeze(0))[0].squeeze(0)  # (L,d)\n",
    "\n",
    "    rel_logits = []\n",
    "    for i in range(L):\n",
    "        p = pred_parent[i]\n",
    "        # --- safe parent index for relation recompute ---\n",
    "        # p could be invalid (>=len) during early training; fallback to root for safety\n",
    "        L = h_seq.size(0)\n",
    "        if (p is None) or (p < 0) or (p >= L):\n",
    "            parent_vec = root.squeeze(0)\n",
    "            # 可选：记录一次非法 parent（不影响逻辑）\n",
    "            # doc.setdefault(\"_bad_parent_count\", 0)\n",
    "                    # doc[\"_bad_parent_count\"] += 1\n",
    "        else:\n",
    "            parent_vec = h_seq[p]\n",
    "\n",
    "        feat = torch.cat([h_seq[i], parent_vec], dim=-1)\n",
    "        rel_logits.append(model.rel_head(feat))\n",
    "\n",
    "    rel_logits = torch.stack(rel_logits, dim=0)  # (L,R)\n",
    "\n",
    "    pred_rel = rel_logits.argmax(dim=-1).detach().cpu().tolist()\n",
    "\n",
    "    return {\n",
    "        \"pred_cls\": pred_cls,\n",
    "        \"pred_parent\": pred_parent,\n",
    "        \"pred_rel\": pred_rel,\n",
    "        \"cls_logits\": cls_logits.detach().cpu(),\n",
    "        \"rel_logits\": rel_logits.detach().cpu(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc28d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _id2name(mapping, idx: int) -> str:\n",
    "    \"\"\"\n",
    "    mapping 可以是：\n",
    "      - list: mapping[idx]\n",
    "      - dict: mapping.get(idx)\n",
    "      - None: 返回 idx 字符串\n",
    "    \"\"\"\n",
    "    if mapping is None:\n",
    "        return str(idx)\n",
    "    if isinstance(mapping, dict):\n",
    "        return mapping.get(idx, str(idx))\n",
    "    if isinstance(mapping, (list, tuple)):\n",
    "        if 0 <= idx < len(mapping):\n",
    "            return str(mapping[idx])\n",
    "        return str(idx)\n",
    "    return str(idx)\n",
    "\n",
    "\n",
    "def export_tree_json(doc, pred=None):\n",
    "    \"\"\"\n",
    "    doc: dataset 返回的 doc dict\n",
    "    pred: None => 导出 GT\n",
    "          dict => 导出 pred（需要含 pred_cls/pred_parent/pred_rel）\n",
    "    输出格式：\n",
    "      {\n",
    "        doc_id,\n",
    "        nodes: [\n",
    "          {id, text, label_id, label_name, is_meta, parent, rel_id, rel_name, page_id, box},\n",
    "          ...\n",
    "        ]\n",
    "      }\n",
    "    \"\"\"\n",
    "    doc_id = doc[\"doc_id\"]\n",
    "    units = doc[\"units\"]\n",
    "    L = len(units)\n",
    "\n",
    "    if pred is None:\n",
    "        cls_ids = doc[\"y_cls\"]\n",
    "        parent = doc[\"y_parent\"]\n",
    "        rel_ids = doc[\"y_rel\"]\n",
    "    else:\n",
    "        cls_ids = pred[\"pred_cls\"]\n",
    "        parent = pred[\"pred_parent\"]\n",
    "        rel_ids = pred[\"pred_rel\"]\n",
    "\n",
    "    # 兼容你 notebook 里 ID2LABEL_14 / ID2REL 的类型（list 或 dict）\n",
    "    label_map = globals().get(\"ID2LABEL_14\", None)\n",
    "    rel_map = globals().get(\"ID2REL\", None)\n",
    "\n",
    "    out = {\"doc_id\": doc_id, \"nodes\": []}\n",
    "    is_meta_list = doc.get(\"is_meta\", [False]*L)\n",
    "\n",
    "    for i in range(L):\n",
    "        u = units[i]\n",
    "        is_meta = bool(is_meta_list[i])\n",
    "        cls_id = int(cls_ids[i])\n",
    "        rel_id = int(rel_ids[i]) if rel_ids is not None else -1\n",
    "\n",
    "        out[\"nodes\"].append({\n",
    "            \"id\": i,\n",
    "            \"text\": u.get(\"text\", \"\"),\n",
    "            \"label_id\": cls_id,\n",
    "            \"label_name\": _id2name(label_map, cls_id),\n",
    "            \"is_meta\": is_meta,\n",
    "            \"parent\": int(parent[i]) if parent[i] is not None else -1,\n",
    "            \"rel_id\": rel_id,\n",
    "            \"rel_name\": _id2name(rel_map, rel_id),\n",
    "            \"page_id\": int(u.get(\"page_id\", 0)),\n",
    "            \"box\": [int(x) for x in u.get(\"box\", [0,0,0,0])]\n",
    "        })\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "531fe758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "@torch.no_grad()\n",
    "def export_split_predictions(model, loader, save_dir, device):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for doc in tqdm(loader, desc=f\"export -> {save_dir}\"):\n",
    "        pred = predict_doc_with_rel_recompute(model, doc, device)\n",
    "\n",
    "        gt_json = export_tree_json(doc, pred=None)\n",
    "        pr_json = export_tree_json(doc, pred=pred)\n",
    "\n",
    "        doc_id = doc[\"doc_id\"].replace(\"/\", \"_\")\n",
    "        with open(os.path.join(save_dir, f\"{doc_id}.gt.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(gt_json, f, ensure_ascii=False, indent=2)\n",
    "        with open(os.path.join(save_dir, f\"{doc_id}.pred.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(pr_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed5b87ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomra\\AppData\\Local\\Temp\\ipykernel_8292\\631854839.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"dsps_hrdh_best.pt\", map_location=device))\n",
      "export -> exports_hrdh_test: 100%|███████████████████████████████████████████████████| 500/500 [03:31<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载最优模型\n",
    "model.load_state_dict(torch.load(\"dsps_hrdh_best.pt\", map_location=device))\n",
    "\n",
    "export_split_predictions(model, test_loader, save_dir=\"exports_hrdh_test\", device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad150067",
   "metadata": {},
   "source": [
    "## 10. STEDS / TED 评估实现\n",
    "\n",
    "- TNode 数据结构\n",
    "- Zhang-Shasha Tree Edit Distance\n",
    "- compute_steds 与导出目录评估\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f9577dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,\n",
       " ('exports_hrdh_test\\\\1401.3699.gt.json',\n",
       "  'exports_hrdh_test\\\\1401.3699.pred.json'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, glob\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "def load_pair_files(export_dir: str) -> List[Tuple[str, str]]:\n",
    "    gt_files = sorted(glob.glob(os.path.join(export_dir, \"*.gt.json\")))\n",
    "    pairs = []\n",
    "    for gt in gt_files:\n",
    "        pred = gt.replace(\".gt.json\", \".pred.json\")\n",
    "        if os.path.exists(pred):\n",
    "            pairs.append((gt, pred))\n",
    "    return pairs\n",
    "\n",
    "pairs = load_pair_files(\"exports_hrdh_test\")\n",
    "len(pairs), pairs[0] if pairs else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df8b4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "@dataclass\n",
    "class TNode:\n",
    "    label: int\n",
    "    text: str\n",
    "    oid: int                 # original id，用来排序\n",
    "    children: List[\"TNode\"]\n",
    "\n",
    "    def __init__(self, label: int, text: str, oid: int):\n",
    "        self.label = label\n",
    "        self.text = text\n",
    "        self.oid = oid\n",
    "        self.children = []\n",
    "\n",
    "def find_effective_parent(i: int) -> int:\n",
    "    \"\"\"\n",
    "    返回 i 的有效父节点：\n",
    "    - 若 parent 指向越界/非法：视为无父节点（-1），挂到 ROOT\n",
    "    - 若 parent 指向被排除节点（meta 或不在 kept 集）：沿 parent 链向上跳\n",
    "    - 增加循环保护，避免死循环\n",
    "    \"\"\"\n",
    "    n = len(parent)\n",
    "\n",
    "    def _is_valid_idx(x: int) -> bool:\n",
    "        return isinstance(x, int) and (0 <= x < n)\n",
    "\n",
    "    p = parent[i] if 0 <= i < n else -1\n",
    "\n",
    "    # 1) 直接非法或 root\n",
    "    if (p is None) or (not isinstance(p, int)) or (p < 0):\n",
    "        return -1\n",
    "    if not _is_valid_idx(p):\n",
    "        return -1\n",
    "\n",
    "    # 2) 向上跳，直到遇到 kept 的父节点或 root/非法\n",
    "    seen = set()\n",
    "    while True:\n",
    "        if p in kept_set:\n",
    "            return p\n",
    "\n",
    "        if p in seen:\n",
    "            # 出现环，按无父节点处理\n",
    "            return -1\n",
    "        seen.add(p)\n",
    "\n",
    "        # 向上跳\n",
    "        pp = parent[p]\n",
    "        if (pp is None) or (not isinstance(pp, int)) or (pp < 0):\n",
    "            return -1\n",
    "        if not _is_valid_idx(pp):\n",
    "            return -1\n",
    "        p = pp\n",
    "\n",
    "    # 找“跳过 meta 后的最近祖先”\n",
    "    def find_effective_parent(i: int) -> int:\n",
    "        p = parent[i]\n",
    "        seen = set()\n",
    "        while True:\n",
    "            if p < 0:\n",
    "                return -1\n",
    "            if p in seen:         # 防环保护\n",
    "                return -1\n",
    "            seen.add(p)\n",
    "            if kept(p):\n",
    "                return p\n",
    "            # p 是 meta 或被排除节点，继续向上跳\n",
    "            p = parent[p]\n",
    "\n",
    "    # 连接\n",
    "    for i in kept_ids:\n",
    "        ep = find_effective_parent(i)\n",
    "        if ep < 0:\n",
    "            root.children.append(obj[i])\n",
    "        else:\n",
    "            obj[ep].children.append(obj[i])\n",
    "\n",
    "    # children 按原始 reading-order id 排序，确保“有序树”一致\n",
    "    def sort_rec(x: TNode):\n",
    "        x.children.sort(key=lambda c: c.oid)\n",
    "        for ch in x.children:\n",
    "            sort_rec(ch)\n",
    "\n",
    "    sort_rec(root)\n",
    "    return root, len(kept_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aadfc478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ted_zhang_shasha(t1: TNode, t2: TNode, match_mode: str = \"strict\") -> int:\n",
    "    \"\"\"\n",
    "    Ordered Tree Edit Distance (Zhang-Shasha)\n",
    "    Costs: ins=1, del=1\n",
    "    match_mode:\n",
    "      - \"strict\": label + text 完全一致 cost=0，否则 1\n",
    "      - \"label\": 只要 label 一致 cost=0，否则 1\n",
    "    \"\"\"\n",
    "    def postorder(root: TNode):\n",
    "        nodes = []\n",
    "        def dfs(x: TNode):\n",
    "            for ch in x.children:\n",
    "                dfs(ch)\n",
    "            nodes.append(x)\n",
    "        dfs(root)\n",
    "        return nodes\n",
    "\n",
    "    A = postorder(t1)\n",
    "    B = postorder(t2)\n",
    "\n",
    "    idxA = {id(node): i+1 for i, node in enumerate(A)}\n",
    "    idxB = {id(node): i+1 for i, node in enumerate(B)}\n",
    "\n",
    "    n, m = len(A), len(B)\n",
    "\n",
    "    def leftmost_indices(nodes, idx_map):\n",
    "        l = [0] * (len(nodes) + 1)  # 1-based\n",
    "        for i, node in enumerate(nodes, start=1):\n",
    "            cur = node\n",
    "            while cur.children:\n",
    "                cur = cur.children[0]\n",
    "            l[i] = idx_map[id(cur)]\n",
    "        return l\n",
    "\n",
    "    l1 = leftmost_indices(A, idxA)\n",
    "    l2 = leftmost_indices(B, idxB)\n",
    "\n",
    "    def keyroots(leftmost):\n",
    "        seen = {}\n",
    "        for i in range(1, len(leftmost)):\n",
    "            seen[leftmost[i]] = i\n",
    "        return sorted(seen.values())\n",
    "\n",
    "    kr1 = keyroots(l1)\n",
    "    kr2 = keyroots(l2)\n",
    "\n",
    "    treedist = [[0] * (m + 1) for _ in range(n + 1)]\n",
    "\n",
    "    def relabel_cost(i: int, j: int) -> int:\n",
    "        a = A[i-1]\n",
    "        b = B[j-1]\n",
    "        if match_mode == \"label\":\n",
    "            return 0 if (a.label == b.label) else 1\n",
    "        return 0 if (a.label == b.label and a.text == b.text) else 1\n",
    "\n",
    "    for i in kr1:\n",
    "        for j in kr2:\n",
    "            i0 = l1[i]\n",
    "            j0 = l2[j]\n",
    "            fd = [[0] * (j - j0 + 2) for _ in range(i - i0 + 2)]\n",
    "\n",
    "            for di in range(1, i - i0 + 2):\n",
    "                fd[di][0] = fd[di-1][0] + 1  # delete\n",
    "            for dj in range(1, j - j0 + 2):\n",
    "                fd[0][dj] = fd[0][dj-1] + 1  # insert\n",
    "\n",
    "            for di in range(1, i - i0 + 2):\n",
    "                for dj in range(1, j - j0 + 2):\n",
    "                    ii = i0 + di - 1\n",
    "                    jj = j0 + dj - 1\n",
    "                    if l1[ii] == i0 and l2[jj] == j0:\n",
    "                        fd[di][dj] = min(\n",
    "                            fd[di-1][dj] + 1,\n",
    "                            fd[di][dj-1] + 1,\n",
    "                            fd[di-1][dj-1] + relabel_cost(ii, jj)\n",
    "                        )\n",
    "                        treedist[ii][jj] = fd[di][dj]\n",
    "                    else:\n",
    "                        fd[di][dj] = min(\n",
    "                            fd[di-1][dj] + 1,\n",
    "                            fd[di][dj-1] + 1,\n",
    "                            fd[di-1][dj-1] + treedist[ii][jj]\n",
    "                        )\n",
    "\n",
    "    return treedist[n][m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff268db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_steds(gt_js: Dict[str,Any], pr_js: Dict[str,Any], exclude_meta: bool = True, match_mode: str = \"strict\") -> float:\n",
    "    gt_root, gt_n = build_tree_from_export(gt_js, exclude_meta=exclude_meta)\n",
    "    pr_root, pr_n = build_tree_from_export(pr_js, exclude_meta=exclude_meta)\n",
    "    dist = ted_zhang_shasha(gt_root, pr_root, match_mode=match_mode)\n",
    "    denom = max(gt_n, pr_n, 1)\n",
    "    return 1.0 - dist / denom\n",
    "\n",
    "def compute_aux_metrics(gt_js: Dict[str,Any], pr_js: Dict[str,Any], exclude_meta: bool = True) -> Dict[str,float]:\n",
    "    gt_nodes = gt_js[\"nodes\"]\n",
    "    pr_nodes = pr_js[\"nodes\"]\n",
    "    L = min(len(gt_nodes), len(pr_nodes))\n",
    "\n",
    "    mask = []\n",
    "    for i in range(L):\n",
    "        is_meta = bool(gt_nodes[i].get(\"is_meta\", False))\n",
    "        mask.append((not is_meta) if exclude_meta else True)\n",
    "\n",
    "    def acc(key: str) -> float:\n",
    "        tot = 0\n",
    "        hit = 0\n",
    "        for i in range(L):\n",
    "            if not mask[i]:\n",
    "                continue\n",
    "            tot += 1\n",
    "            hit += int(gt_nodes[i][key] == pr_nodes[i][key])\n",
    "        return hit / tot if tot else 0.0\n",
    "\n",
    "    # label_id / parent / rel_id\n",
    "    return {\n",
    "        \"cls_acc\": acc(\"label_id\"),\n",
    "        \"parent_acc\": acc(\"parent\"),\n",
    "        \"rel_acc\": acc(\"rel_id\"),\n",
    "        \"n_eval\": float(sum(mask)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2ee463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval exports_hrdh_test: 100%|████████████████████████████████████████████████████████| 500/500 [45:03<00:00,  5.41s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'num_docs': 500,\n",
       " 'exclude_meta': True,\n",
       " 'STEDS_mean': -0.18628876550536422,\n",
       " 'STEDS_std': 0.2921664398838005,\n",
       " 'STEDS_median': -0.20092412009013827,\n",
       " 'cls_acc_mean': 0.8558382271757636,\n",
       " 'parent_acc_mean': 0.8717278160283322,\n",
       " 'rel_acc_mean': 0.8896526009448044}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def eval_export_dir(export_dir: str, exclude_meta: bool = True):\n",
    "    pairs = load_pair_files(export_dir)\n",
    "    steds_list = []\n",
    "    aux_list = []\n",
    "\n",
    "    for gt_path, pr_path in tqdm(pairs, desc=f\"eval {export_dir}\"):\n",
    "        with open(gt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            gt_js = json.load(f)\n",
    "        with open(pr_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            pr_js = json.load(f)\n",
    "\n",
    "        st = compute_steds(gt_js, pr_js, exclude_meta=exclude_meta)\n",
    "        aux = compute_aux_metrics(gt_js, pr_js, exclude_meta=exclude_meta)\n",
    "\n",
    "        steds_list.append(st)\n",
    "        aux_list.append(aux)\n",
    "\n",
    "    steds_arr = np.array(steds_list, dtype=float)\n",
    "    cls_acc = np.array([a[\"cls_acc\"] for a in aux_list], dtype=float)\n",
    "    par_acc = np.array([a[\"parent_acc\"] for a in aux_list], dtype=float)\n",
    "    rel_acc = np.array([a[\"rel_acc\"] for a in aux_list], dtype=float)\n",
    "\n",
    "    report = {\n",
    "        \"num_docs\": len(pairs),\n",
    "        \"exclude_meta\": exclude_meta,\n",
    "        \"STEDS_mean\": float(steds_arr.mean()) if len(steds_arr) else 0.0,\n",
    "        \"STEDS_std\": float(steds_arr.std()) if len(steds_arr) else 0.0,\n",
    "        \"STEDS_median\": float(np.median(steds_arr)) if len(steds_arr) else 0.0,\n",
    "        \"cls_acc_mean\": float(cls_acc.mean()) if len(cls_acc) else 0.0,\n",
    "        \"parent_acc_mean\": float(par_acc.mean()) if len(par_acc) else 0.0,\n",
    "        \"rel_acc_mean\": float(rel_acc.mean()) if len(rel_acc) else 0.0,\n",
    "    }\n",
    "    return report, steds_list, aux_list\n",
    "\n",
    "report, steds_list, aux_list = eval_export_dir(\"exports_hrdh_test\", exclude_meta=True)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f99a52",
   "metadata": {},
   "source": [
    "## 11. 文本规范化与阅读顺序（Reading Order）辅助\n",
    "\n",
    "- 文本归一化/匹配\n",
    "- 多列布局排序与文档级排序\n",
    "\n",
    "说明：原 Notebook 中此部分包含多段独立工具函数，均按原顺序保留。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b670de16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STEDS variants: 100%|██████████████████████████████████████████████████████████████| 500/500 [2:24:20<00:00, 17.32s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'original': {'mean': -0.18628876550536422,\n",
       "  'median': -0.20092412009013827,\n",
       "  'std': 0.2921664398838005,\n",
       "  'min': -0.858744394618834,\n",
       "  'max': 0.5529661016949152},\n",
       " 'label_only': {'mean': 0.12433016122204182,\n",
       "  'median': 0.173594878284821,\n",
       "  'std': 0.24902520475892204,\n",
       "  'min': -0.756701030927835,\n",
       "  'max': 0.6278586278586278},\n",
       " 'text_norm': {'mean': -0.18628876550536422,\n",
       "  'median': -0.20092412009013827,\n",
       "  'std': 0.2921664398838005,\n",
       "  'min': -0.858744394618834,\n",
       "  'max': 0.5529661016949152}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, os, re, numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- text normalization ----------\n",
    "def _norm_text(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = s.replace(\"\\n\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.replace(\"- \", \"-\")\n",
    "    return s\n",
    "\n",
    "\n",
    "# ---------- STEDS variants ----------\n",
    "def steds_original(gt_js, pr_js):\n",
    "    return compute_steds(gt_js, pr_js, exclude_meta=True)\n",
    "\n",
    "\n",
    "def steds_label_only(gt_js, pr_js):\n",
    "    gt_root, gt_n = build_tree_from_export(gt_js, exclude_meta=True)\n",
    "    pr_root, pr_n = build_tree_from_export(pr_js, exclude_meta=True)\n",
    "\n",
    "    def wipe_text(x):\n",
    "        x.text = \"\"\n",
    "        for c in x.children:\n",
    "            wipe_text(c)\n",
    "\n",
    "    wipe_text(gt_root)\n",
    "    wipe_text(pr_root)\n",
    "\n",
    "    dist = ted_zhang_shasha(gt_root, pr_root)\n",
    "    denom = max(gt_n, pr_n, 1)\n",
    "    return 1.0 - dist / denom\n",
    "\n",
    "\n",
    "def steds_text_norm(gt_js, pr_js):\n",
    "    gt_root, gt_n = build_tree_from_export(gt_js, exclude_meta=True)\n",
    "    pr_root, pr_n = build_tree_from_export(pr_js, exclude_meta=True)\n",
    "\n",
    "    def apply_norm(x):\n",
    "        x.text = _norm_text(x.text)\n",
    "        for c in x.children:\n",
    "            apply_norm(c)\n",
    "\n",
    "    apply_norm(gt_root)\n",
    "    apply_norm(pr_root)\n",
    "\n",
    "    dist = ted_zhang_shasha(gt_root, pr_root)\n",
    "    denom = max(gt_n, pr_n, 1)\n",
    "    return 1.0 - dist / denom\n",
    "\n",
    "\n",
    "# ---------- batch evaluation ----------\n",
    "def eval_steds_variants(export_dir: str):\n",
    "    pairs = load_pair_files(export_dir)\n",
    "\n",
    "    res = {\n",
    "        \"original\": [],\n",
    "        \"label_only\": [],\n",
    "        \"text_norm\": [],\n",
    "    }\n",
    "\n",
    "    for gt_path, pr_path in tqdm(pairs, desc=\"STEDS variants\"):\n",
    "        with open(gt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            gt = json.load(f)\n",
    "        with open(pr_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            pr = json.load(f)\n",
    "\n",
    "        res[\"original\"].append(steds_original(gt, pr))\n",
    "        res[\"label_only\"].append(steds_label_only(gt, pr))\n",
    "        res[\"text_norm\"].append(steds_text_norm(gt, pr))\n",
    "\n",
    "    def summary(arr):\n",
    "        arr = np.asarray(arr)\n",
    "        return {\n",
    "            \"mean\": float(arr.mean()),\n",
    "            \"median\": float(np.median(arr)),\n",
    "            \"std\": float(arr.std()),\n",
    "            \"min\": float(arr.min()),\n",
    "            \"max\": float(arr.max()),\n",
    "        }\n",
    "\n",
    "    report = {k: summary(v) for k, v in res.items()}\n",
    "    return report\n",
    "\n",
    "\n",
    "# ---------- run ----------\n",
    "report = eval_steds_variants(\"exports_hrdh_test\")\n",
    "report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3d3eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kmeans_1d(x, k, iters=30):\n",
    "    \"\"\"简单 1D kmeans。x: (n,) float\"\"\"\n",
    "    x = x.astype(np.float32)\n",
    "    # init: 均匀取 k 个分位点\n",
    "    qs = np.linspace(0.1, 0.9, k)\n",
    "    centers = np.quantile(x, qs)\n",
    "    for _ in range(iters):\n",
    "        # assign\n",
    "        d = np.abs(x[:, None] - centers[None, :])\n",
    "        labels = d.argmin(axis=1)\n",
    "        new_centers = centers.copy()\n",
    "        for j in range(k):\n",
    "            mask = labels == j\n",
    "            if mask.any():\n",
    "                new_centers[j] = x[mask].mean()\n",
    "        if np.allclose(new_centers, centers, atol=1e-4):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    # inertia\n",
    "    inertia = ((x - centers[labels]) ** 2).sum()\n",
    "    # sort clusters by center (left->right)\n",
    "    order = np.argsort(centers)\n",
    "    remap = np.zeros_like(order)\n",
    "    remap[order] = np.arange(k)\n",
    "    labels = remap[labels]\n",
    "    centers = centers[order]\n",
    "    return labels, centers, inertia\n",
    "\n",
    "def choose_k_1d(x, k_min=1, k_max=3, min_cluster_size=5):\n",
    "    \"\"\"\n",
    "    自动选择列数 k：\n",
    "    - 基于“惯性下降幅度”（elbow-ish）\n",
    "    - 同时避免产生很小的列\n",
    "    \"\"\"\n",
    "    best = None\n",
    "    prev_inertia = None\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        labels, centers, inertia = kmeans_1d(x, k)\n",
    "        # 小簇过滤\n",
    "        ok = True\n",
    "        for j in range(k):\n",
    "            if (labels == j).sum() < min_cluster_size and len(x) >= min_cluster_size * k:\n",
    "                ok = False\n",
    "                break\n",
    "        if not ok:\n",
    "            continue\n",
    "\n",
    "        if prev_inertia is None:\n",
    "            score = 0.0\n",
    "        else:\n",
    "            # inertia 下降比例越大越好\n",
    "            score = (prev_inertia - inertia) / max(prev_inertia, 1e-6)\n",
    "\n",
    "        # 记录：优先更大“下降”，其次更小 inertia\n",
    "        cand = (score, -inertia, k, labels, centers, inertia)\n",
    "        if best is None or cand > best:\n",
    "            best = cand\n",
    "        prev_inertia = inertia\n",
    "\n",
    "    if best is None:\n",
    "        labels, centers, inertia = kmeans_1d(x, 1)\n",
    "        return 1, labels, centers\n",
    "    _, _, k, labels, centers, _ = best\n",
    "    return k, labels, centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96a015ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_units_multi_column(units, page_w, page_h, max_cols=3):\n",
    "    \"\"\"\n",
    "    units: list of dict with 'box'=[x0,y0,x1,y1]\n",
    "    返回：按阅读顺序排序后的 index 列表\n",
    "    \"\"\"\n",
    "    n = len(units)\n",
    "    if n <= 1:\n",
    "        return list(range(n))\n",
    "\n",
    "    boxes = np.array([u[\"box\"] for u in units], dtype=np.float32)\n",
    "    x0, y0, x1, y1 = boxes[:,0], boxes[:,1], boxes[:,2], boxes[:,3]\n",
    "    xc = 0.5 * (x0 + x1)\n",
    "    yt = y0\n",
    "    bw = np.clip(x1 - x0, 1.0, None)\n",
    "\n",
    "    # 识别“跨栏大框”：宽度超过页面的一定比例（标题/大图/大表等），单独处理\n",
    "    wide = bw > (0.70 * page_w)\n",
    "\n",
    "    # 对非 wide 的做列聚类\n",
    "    idx_normal = np.where(~wide)[0]\n",
    "    idx_wide = np.where(wide)[0]\n",
    "\n",
    "    ordered = []\n",
    "\n",
    "    # 先把“宽大框”按 y 排到合适位置：通常它们是跨栏标题/大图，阅读上仍按 y 走\n",
    "    # 策略：把 wide 与 normal 一起排序时，wide 作为“单独列”插入：这里用更稳的两阶段合并\n",
    "    if len(idx_normal) > 0:\n",
    "        x_norm = xc[idx_normal]\n",
    "        k, labels, centers = choose_k_1d(x_norm, 1, max_cols, min_cluster_size=4)\n",
    "\n",
    "        # 每一列内部按 y_top 排序\n",
    "        cols = []\n",
    "        for c in range(k):\n",
    "            members = idx_normal[labels == c]\n",
    "            members = members[np.argsort(yt[members])]\n",
    "            cols.append(list(members))\n",
    "\n",
    "        # 列从左到右拼接\n",
    "        seq_normal = [i for col in cols for i in col]\n",
    "    else:\n",
    "        seq_normal = []\n",
    "\n",
    "    # wide 按 y_top 排序\n",
    "    seq_wide = list(idx_wide[np.argsort(yt[idx_wide])]) if len(idx_wide) > 0 else []\n",
    "\n",
    "    # 合并 wide 与 normal：按 y_top 进行稳定插入（wide 的 y 决定位置）\n",
    "    # 做法：对两序列按 y_top 归并，但当 y 接近时优先 wide（跨栏标题通常应先读）\n",
    "    def merge_by_y(a, b):\n",
    "        ia = ib = 0\n",
    "        out = []\n",
    "        while ia < len(a) and ib < len(b):\n",
    "            ya = yt[a[ia]]\n",
    "            yb = yt[b[ib]]\n",
    "            if yb <= ya + 2:  # 容忍 2px 抖动，wide 略优先\n",
    "                out.append(b[ib]); ib += 1\n",
    "            else:\n",
    "                out.append(a[ia]); ia += 1\n",
    "        out.extend(a[ia:])\n",
    "        out.extend(b[ib:])\n",
    "        return out\n",
    "\n",
    "    ordered = merge_by_y(seq_normal, seq_wide)\n",
    "    return ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "649604bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_doc_units_by_reading_order(doc_units, page_images):\n",
    "    \"\"\"\n",
    "    doc_units: list[dict], each has page_id, box, text, ...\n",
    "    page_images: dict {page_id: image_path}\n",
    "    return sorted_units\n",
    "    \"\"\"\n",
    "    # 1) 按页收集\n",
    "    by_page = {}\n",
    "    for u in doc_units:\n",
    "        pid = int(u[\"page_id\"])\n",
    "        by_page.setdefault(pid, []).append(u)\n",
    "\n",
    "    # 2) 页按 pid 排序\n",
    "    sorted_units = []\n",
    "    for pid in sorted(by_page.keys()):\n",
    "        # 读 page 尺寸\n",
    "        from PIL import Image\n",
    "        img = Image.open(page_images[pid])\n",
    "        W, H = img.width, img.height\n",
    "\n",
    "        units_p = by_page[pid]\n",
    "        order_idx = order_units_multi_column(units_p, W, H, max_cols=3)\n",
    "        sorted_units.extend([units_p[i] for i in order_idx])\n",
    "\n",
    "    return sorted_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "504e50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def _kmeans_1d(x, k, iters=30):\n",
    "    x = x.astype(np.float32)\n",
    "    qs = np.linspace(0.1, 0.9, k)\n",
    "    centers = np.quantile(x, qs)\n",
    "\n",
    "    for _ in range(iters):\n",
    "        d = np.abs(x[:, None] - centers[None, :])\n",
    "        labels = d.argmin(axis=1)\n",
    "        new_centers = centers.copy()\n",
    "        for j in range(k):\n",
    "            m = labels == j\n",
    "            if m.any():\n",
    "                new_centers[j] = x[m].mean()\n",
    "        if np.allclose(new_centers, centers, atol=1e-4):\n",
    "            break\n",
    "        centers = new_centers\n",
    "\n",
    "    inertia = ((x - centers[labels]) ** 2).sum()\n",
    "\n",
    "    order = np.argsort(centers)\n",
    "    remap = np.zeros_like(order)\n",
    "    remap[order] = np.arange(k)\n",
    "    labels = remap[labels]\n",
    "    centers = centers[order]\n",
    "    return labels, centers, inertia\n",
    "\n",
    "\n",
    "def _choose_k_1d(x, k_min=1, k_max=3, min_cluster_size=4):\n",
    "    best = None\n",
    "    prev_inertia = None\n",
    "\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        labels, centers, inertia = _kmeans_1d(x, k)\n",
    "        ok = True\n",
    "        if len(x) >= min_cluster_size * k:\n",
    "            for j in range(k):\n",
    "                if (labels == j).sum() < min_cluster_size:\n",
    "                    ok = False\n",
    "                    break\n",
    "        if not ok:\n",
    "            continue\n",
    "\n",
    "        score = 0.0 if prev_inertia is None else (prev_inertia - inertia) / max(prev_inertia, 1e-6)\n",
    "        cand = (score, -inertia, k, labels, centers)\n",
    "        if best is None or cand > best:\n",
    "            best = cand\n",
    "        prev_inertia = inertia\n",
    "\n",
    "    if best is None:\n",
    "        labels, centers, _ = _kmeans_1d(x, 1)\n",
    "        return 1, labels, centers\n",
    "    _, _, k, labels, centers = best\n",
    "    return k, labels, centers\n",
    "\n",
    "\n",
    "def order_units_multi_column(units, page_w, page_h, max_cols=3):\n",
    "    \"\"\"\n",
    "    units: list of dict with 'box'=[x0,y0,x1,y1]\n",
    "    return: list of indices in reading order\n",
    "    \"\"\"\n",
    "    n = len(units)\n",
    "    if n <= 1:\n",
    "        return list(range(n))\n",
    "\n",
    "    boxes = np.array([u[\"box\"] for u in units], dtype=np.float32)\n",
    "    x0, y0, x1, y1 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    xc = 0.5 * (x0 + x1)\n",
    "    yt = y0\n",
    "    bw = np.clip(x1 - x0, 1.0, None)\n",
    "\n",
    "    # “跨栏大框”识别：标题/大图/大表常见\n",
    "    wide = bw > (0.70 * page_w)\n",
    "\n",
    "    idx_normal = np.where(~wide)[0]\n",
    "    idx_wide = np.where(wide)[0]\n",
    "\n",
    "    seq_normal = []\n",
    "    if len(idx_normal) > 0:\n",
    "        x_norm = xc[idx_normal]\n",
    "        k, labels, _ = _choose_k_1d(x_norm, 1, max_cols, min_cluster_size=4)\n",
    "\n",
    "        cols = []\n",
    "        for c in range(k):\n",
    "            members = idx_normal[labels == c]\n",
    "            members = members[np.argsort(yt[members])]\n",
    "            cols.append(list(members))\n",
    "\n",
    "        seq_normal = [i for col in cols for i in col]\n",
    "\n",
    "    seq_wide = list(idx_wide[np.argsort(yt[idx_wide])]) if len(idx_wide) > 0 else []\n",
    "\n",
    "    # 按 y 归并：wide 在接近同一 y 时略优先（跨栏标题通常应先读）\n",
    "    def merge_by_y(a, b):\n",
    "        ia = ib = 0\n",
    "        out = []\n",
    "        while ia < len(a) and ib < len(b):\n",
    "            ya = yt[a[ia]]\n",
    "            yb = yt[b[ib]]\n",
    "            if yb <= ya + 2:\n",
    "                out.append(b[ib]); ib += 1\n",
    "            else:\n",
    "                out.append(a[ia]); ia += 1\n",
    "        out.extend(a[ia:])\n",
    "        out.extend(b[ib:])\n",
    "        return out\n",
    "\n",
    "    return merge_by_y(seq_normal, seq_wide)\n",
    "\n",
    "\n",
    "def sort_doc_units_by_reading_order(doc_units, page_images, max_cols=3):\n",
    "    \"\"\"\n",
    "    doc_units: list[dict] each has 'page_id','box',...\n",
    "    page_images: dict[int -> image_path]\n",
    "    return: new_units_sorted\n",
    "    \"\"\"\n",
    "    by_page = {}\n",
    "    for u in doc_units:\n",
    "        pid = int(u[\"page_id\"])\n",
    "        by_page.setdefault(pid, []).append(u)\n",
    "\n",
    "    sorted_units = []\n",
    "    for pid in sorted(by_page.keys()):\n",
    "        img = Image.open(page_images[pid])\n",
    "        W, H = img.width, img.height\n",
    "\n",
    "        units_p = by_page[pid]\n",
    "        order_idx = order_units_multi_column(units_p, W, H, max_cols=max_cols)\n",
    "        sorted_units.extend([units_p[i] for i in order_idx])\n",
    "\n",
    "    return sorted_units\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a08b68",
   "metadata": {},
   "source": [
    "## 12. 诊断、对比评估与实验封装\n",
    "\n",
    "- check_causal_violation\n",
    "- eval_export_dir_dual\n",
    "- run_experiment：实验跑批封装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "913d80eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal violation rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "def check_causal_violation(dataset, n=100):\n",
    "    cnt = 0\n",
    "    viol = 0\n",
    "    for i in range(min(n, len(dataset))):\n",
    "        d = dataset[i]\n",
    "        yp = d[\"y_parent\"]\n",
    "        for j,p in enumerate(yp):\n",
    "            if p is not None and p >= j:\n",
    "                viol += 1\n",
    "            cnt += 1\n",
    "    return viol / max(cnt,1)\n",
    "\n",
    "print(\"causal violation rate:\", check_causal_violation(train_ds, n=100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "180eb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json, os, glob\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_export_dir_dual(export_dir: str, exclude_meta: bool = True,show_progress: bool = True):\n",
    "    pairs = load_pair_files(export_dir)\n",
    "\n",
    "    st_strict, st_label = [], []\n",
    "    cls_acc, par_acc, rel_acc = [], [], []\n",
    "\n",
    "    for gt_path, pr_path in pairs:\n",
    "        with open(gt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            gt_js = json.load(f)\n",
    "        with open(pr_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            pr_js = json.load(f)\n",
    "\n",
    "        st_strict.append(compute_steds(gt_js, pr_js, exclude_meta=exclude_meta, match_mode=\"strict\"))\n",
    "        st_label.append(compute_steds(gt_js, pr_js, exclude_meta=exclude_meta, match_mode=\"label\"))\n",
    "\n",
    "        aux = compute_aux_metrics(gt_js, pr_js, exclude_meta=exclude_meta)\n",
    "        cls_acc.append(aux[\"cls_acc\"])\n",
    "        par_acc.append(aux[\"parent_acc\"])\n",
    "        rel_acc.append(aux[\"rel_acc\"])\n",
    "\n",
    "    def agg(x):\n",
    "        x = np.array(x, dtype=float)\n",
    "        return float(x.mean()), float(x.std()), float(np.median(x))\n",
    "\n",
    "    s_mean, s_std, s_med = agg(st_strict)\n",
    "    l_mean, l_std, l_med = agg(st_label)\n",
    "\n",
    "    return {\n",
    "        \"num_docs\": len(pairs),\n",
    "        \"STEDS_strict_mean\": s_mean,\n",
    "        \"STEDS_strict_std\": s_std,\n",
    "        \"STEDS_strict_median\": s_med,\n",
    "        \"STEDS_label_mean\": l_mean,\n",
    "        \"STEDS_label_std\": l_std,\n",
    "        \"STEDS_label_median\": l_med,\n",
    "        \"cls_acc_mean\": float(np.mean(cls_acc)) if cls_acc else 0.0,\n",
    "        \"parent_acc_mean\": float(np.mean(par_acc)) if par_acc else 0.0,\n",
    "        \"rel_acc_mean\": float(np.mean(rel_acc)) if rel_acc else 0.0,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f3052e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train] start epoch, num_docs = 1000\n",
      "  step    1/1000 | step_time= 2.73s | loss=5.1416 (cls=0.674, par=4.268, rel=0.199) | elapsed=0.0min\n",
      "    sanity: seq_len=132, avg_parent_candidates=66.5\n",
      "    sanity: seq_len=161, avg_parent_candidates=81.0\n",
      "  step    5/1000 | step_time= 0.97s | loss=4.3868 (cls=0.275, par=3.947, rel=0.165) | elapsed=0.1min\n",
      "  step   10/1000 | step_time= 0.81s | loss=4.6751 (cls=0.287, par=4.229, rel=0.159) | elapsed=0.2min\n",
      "  step   15/1000 | step_time= 0.99s | loss=3.8214 (cls=0.105, par=3.621, rel=0.095) | elapsed=0.2min\n",
      "  step   20/1000 | step_time= 0.92s | loss=3.6946 (cls=0.157, par=3.439, rel=0.099) | elapsed=0.3min\n",
      "  step   25/1000 | step_time= 0.97s | loss=3.6264 (cls=0.096, par=3.455, rel=0.075) | elapsed=0.4min\n",
      "  step   30/1000 | step_time= 0.94s | loss=3.2549 (cls=0.075, par=3.134, rel=0.045) | elapsed=0.5min\n",
      "  step   35/1000 | step_time= 0.94s | loss=3.0228 (cls=0.087, par=2.885, rel=0.051) | elapsed=0.5min\n",
      "  step   40/1000 | step_time= 0.96s | loss=3.2079 (cls=0.094, par=3.039, rel=0.075) | elapsed=0.6min\n",
      "  step   45/1000 | step_time= 0.98s | loss=2.9140 (cls=0.101, par=2.747, rel=0.066) | elapsed=0.7min\n",
      "  step   50/1000 | step_time= 0.98s | loss=2.7697 (cls=0.091, par=2.618, rel=0.061) | elapsed=0.8min\n",
      "  step   55/1000 | step_time= 0.95s | loss=2.2305 (cls=0.112, par=2.055, rel=0.064) | elapsed=0.8min\n",
      "  step   60/1000 | step_time= 0.96s | loss=2.0115 (cls=0.115, par=1.842, rel=0.054) | elapsed=0.9min\n",
      "  step   65/1000 | step_time= 0.84s | loss=2.0749 (cls=0.159, par=1.855, rel=0.061) | elapsed=1.0min\n",
      "  step   70/1000 | step_time= 0.44s | loss=1.8634 (cls=0.095, par=1.695, rel=0.073) | elapsed=1.1min\n",
      "  step   75/1000 | step_time= 0.47s | loss=1.9689 (cls=0.140, par=1.771, rel=0.058) | elapsed=1.1min\n",
      "  step   80/1000 | step_time= 0.95s | loss=2.2019 (cls=0.111, par=2.008, rel=0.083) | elapsed=1.2min\n",
      "  step   85/1000 | step_time= 0.60s | loss=1.8412 (cls=0.107, par=1.661, rel=0.073) | elapsed=1.3min\n",
      "  step   90/1000 | step_time= 0.94s | loss=1.9998 (cls=0.100, par=1.839, rel=0.061) | elapsed=1.3min\n",
      "  step   95/1000 | step_time= 0.97s | loss=2.4188 (cls=0.124, par=2.237, rel=0.058) | elapsed=1.4min\n",
      "  step  100/1000 | step_time= 0.63s | loss=1.8882 (cls=0.139, par=1.659, rel=0.090) | elapsed=1.5min\n",
      "  step  105/1000 | step_time= 0.93s | loss=1.9945 (cls=0.063, par=1.890, rel=0.042) | elapsed=1.6min\n",
      "  step  110/1000 | step_time= 0.94s | loss=2.2637 (cls=0.091, par=2.114, rel=0.059) | elapsed=1.6min\n",
      "  step  115/1000 | step_time= 0.68s | loss=2.2710 (cls=0.085, par=2.131, rel=0.055) | elapsed=1.7min\n",
      "  step  120/1000 | step_time= 0.49s | loss=1.9432 (cls=0.143, par=1.699, rel=0.101) | elapsed=1.8min\n",
      "  step  125/1000 | step_time= 0.99s | loss=1.8577 (cls=0.052, par=1.766, rel=0.040) | elapsed=1.9min\n",
      "  step  130/1000 | step_time= 0.27s | loss=1.7085 (cls=0.120, par=1.508, rel=0.081) | elapsed=1.9min\n",
      "  step  135/1000 | step_time= 0.86s | loss=2.1444 (cls=0.074, par=2.009, rel=0.062) | elapsed=2.0min\n",
      "  step  140/1000 | step_time= 0.51s | loss=1.7094 (cls=0.128, par=1.502, rel=0.079) | elapsed=2.0min\n",
      "  step  145/1000 | step_time= 0.95s | loss=2.2416 (cls=0.060, par=2.131, rel=0.051) | elapsed=2.1min\n",
      "  step  150/1000 | step_time= 0.95s | loss=2.2833 (cls=0.078, par=2.159, rel=0.046) | elapsed=2.2min\n",
      "  step  155/1000 | step_time= 0.74s | loss=2.2965 (cls=0.085, par=2.131, rel=0.080) | elapsed=2.3min\n",
      "  step  160/1000 | step_time= 0.68s | loss=1.8532 (cls=0.121, par=1.659, rel=0.073) | elapsed=2.3min\n",
      "  step  165/1000 | step_time= 0.99s | loss=1.5238 (cls=0.087, par=1.372, rel=0.064) | elapsed=2.4min\n",
      "  step  170/1000 | step_time= 0.95s | loss=1.7112 (cls=0.108, par=1.550, rel=0.053) | elapsed=2.5min\n",
      "  step  175/1000 | step_time= 0.63s | loss=1.8965 (cls=0.127, par=1.674, rel=0.095) | elapsed=2.6min\n",
      "  step  180/1000 | step_time= 0.74s | loss=1.8234 (cls=0.080, par=1.671, rel=0.073) | elapsed=2.6min\n",
      "  step  185/1000 | step_time= 0.95s | loss=2.0639 (cls=0.075, par=1.943, rel=0.046) | elapsed=2.7min\n",
      "  step  190/1000 | step_time= 1.03s | loss=2.0820 (cls=0.076, par=1.954, rel=0.052) | elapsed=2.8min\n",
      "  step  195/1000 | step_time= 0.96s | loss=1.7594 (cls=0.049, par=1.676, rel=0.035) | elapsed=2.9min\n",
      "  step  200/1000 | step_time= 0.97s | loss=1.6896 (cls=0.077, par=1.550, rel=0.062) | elapsed=3.0min\n",
      "  step  205/1000 | step_time= 0.97s | loss=1.7639 (cls=0.051, par=1.671, rel=0.042) | elapsed=3.0min\n",
      "  step  210/1000 | step_time= 0.95s | loss=1.9907 (cls=0.062, par=1.884, rel=0.045) | elapsed=3.1min\n",
      "  step  215/1000 | step_time= 1.01s | loss=1.9697 (cls=0.059, par=1.861, rel=0.049) | elapsed=3.2min\n",
      "  step  220/1000 | step_time= 1.00s | loss=1.6032 (cls=0.082, par=1.446, rel=0.075) | elapsed=3.2min\n",
      "  step  225/1000 | step_time= 0.96s | loss=1.9558 (cls=0.064, par=1.853, rel=0.039) | elapsed=3.3min\n",
      "  step  230/1000 | step_time= 0.96s | loss=1.9920 (cls=0.067, par=1.875, rel=0.050) | elapsed=3.4min\n",
      "  step  235/1000 | step_time= 0.39s | loss=1.7036 (cls=0.145, par=1.486, rel=0.073) | elapsed=3.5min\n",
      "  step  240/1000 | step_time= 0.95s | loss=1.9509 (cls=0.076, par=1.812, rel=0.062) | elapsed=3.5min\n",
      "  step  245/1000 | step_time= 0.99s | loss=1.5622 (cls=0.075, par=1.431, rel=0.056) | elapsed=3.6min\n",
      "  step  250/1000 | step_time= 0.98s | loss=1.6146 (cls=0.084, par=1.466, rel=0.065) | elapsed=3.7min\n",
      "  step  255/1000 | step_time= 0.88s | loss=1.8352 (cls=0.063, par=1.724, rel=0.048) | elapsed=3.8min\n",
      "  step  260/1000 | step_time= 0.95s | loss=1.6988 (cls=0.073, par=1.571, rel=0.055) | elapsed=3.8min\n",
      "  step  265/1000 | step_time= 0.77s | loss=1.3406 (cls=0.059, par=1.236, rel=0.046) | elapsed=3.9min\n",
      "  step  270/1000 | step_time= 0.60s | loss=1.6011 (cls=0.105, par=1.419, rel=0.077) | elapsed=4.0min\n",
      "  step  275/1000 | step_time= 0.96s | loss=2.2566 (cls=0.064, par=2.146, rel=0.046) | elapsed=4.1min\n",
      "  step  280/1000 | step_time= 0.98s | loss=1.7255 (cls=0.061, par=1.620, rel=0.044) | elapsed=4.1min\n",
      "  step  285/1000 | step_time= 0.97s | loss=1.7491 (cls=0.078, par=1.606, rel=0.065) | elapsed=4.2min\n",
      "  step  290/1000 | step_time= 0.44s | loss=1.7075 (cls=0.116, par=1.490, rel=0.102) | elapsed=4.3min\n",
      "  step  295/1000 | step_time= 0.93s | loss=1.9816 (cls=0.063, par=1.857, rel=0.061) | elapsed=4.4min\n",
      "  step  300/1000 | step_time= 1.61s | loss=1.5556 (cls=0.105, par=1.392, rel=0.058) | elapsed=4.4min\n",
      "  step  305/1000 | step_time= 0.74s | loss=1.6967 (cls=0.089, par=1.537, rel=0.070) | elapsed=4.5min\n",
      "  step  310/1000 | step_time= 0.97s | loss=1.7919 (cls=0.099, par=1.618, rel=0.075) | elapsed=4.6min\n",
      "  step  315/1000 | step_time= 0.96s | loss=1.6976 (cls=0.059, par=1.601, rel=0.037) | elapsed=4.7min\n",
      "  step  320/1000 | step_time= 0.98s | loss=1.4101 (cls=0.070, par=1.291, rel=0.048) | elapsed=4.7min\n",
      "  step  325/1000 | step_time= 0.97s | loss=1.4534 (cls=0.050, par=1.353, rel=0.050) | elapsed=4.8min\n",
      "  step  330/1000 | step_time= 0.99s | loss=1.6427 (cls=0.061, par=1.542, rel=0.040) | elapsed=4.9min\n",
      "  step  335/1000 | step_time= 0.98s | loss=1.4856 (cls=0.069, par=1.384, rel=0.033) | elapsed=4.9min\n",
      "  step  340/1000 | step_time= 0.95s | loss=1.6063 (cls=0.098, par=1.448, rel=0.060) | elapsed=5.0min\n",
      "  step  345/1000 | step_time= 0.82s | loss=1.6280 (cls=0.107, par=1.452, rel=0.069) | elapsed=5.1min\n",
      "  step  350/1000 | step_time= 0.97s | loss=1.3589 (cls=0.083, par=1.225, rel=0.051) | elapsed=5.2min\n",
      "  step  355/1000 | step_time= 0.98s | loss=1.6018 (cls=0.122, par=1.424, rel=0.056) | elapsed=5.2min\n",
      "  step  360/1000 | step_time= 0.93s | loss=1.6161 (cls=0.065, par=1.505, rel=0.046) | elapsed=5.3min\n",
      "  step  365/1000 | step_time= 0.97s | loss=1.5682 (cls=0.078, par=1.427, rel=0.064) | elapsed=5.4min\n",
      "  step  370/1000 | step_time= 0.96s | loss=1.6452 (cls=0.052, par=1.554, rel=0.039) | elapsed=5.5min\n",
      "  step  375/1000 | step_time= 1.07s | loss=1.5998 (cls=0.103, par=1.440, rel=0.057) | elapsed=5.5min\n",
      "  step  380/1000 | step_time= 1.69s | loss=1.6302 (cls=0.064, par=1.523, rel=0.043) | elapsed=5.6min\n",
      "  step  385/1000 | step_time= 1.06s | loss=1.7069 (cls=0.062, par=1.594, rel=0.051) | elapsed=5.7min\n",
      "  step  390/1000 | step_time= 0.87s | loss=1.8523 (cls=0.073, par=1.722, rel=0.057) | elapsed=5.8min\n",
      "  step  395/1000 | step_time= 0.96s | loss=1.6220 (cls=0.060, par=1.519, rel=0.043) | elapsed=5.8min\n",
      "  step  400/1000 | step_time= 0.75s | loss=1.2919 (cls=0.071, par=1.165, rel=0.057) | elapsed=5.9min\n",
      "  step  405/1000 | step_time= 0.92s | loss=1.5576 (cls=0.100, par=1.405, rel=0.052) | elapsed=6.0min\n",
      "  step  410/1000 | step_time= 0.60s | loss=1.6112 (cls=0.103, par=1.429, rel=0.079) | elapsed=6.1min\n",
      "  step  415/1000 | step_time= 0.96s | loss=1.5446 (cls=0.149, par=1.351, rel=0.045) | elapsed=6.1min\n",
      "  step  420/1000 | step_time= 0.88s | loss=1.2767 (cls=0.061, par=1.160, rel=0.056) | elapsed=6.2min\n",
      "  step  425/1000 | step_time= 0.99s | loss=2.1908 (cls=0.082, par=2.053, rel=0.056) | elapsed=6.3min\n",
      "  step  430/1000 | step_time= 0.96s | loss=1.3753 (cls=0.088, par=1.239, rel=0.048) | elapsed=6.4min\n",
      "  step  435/1000 | step_time= 0.68s | loss=1.3971 (cls=0.063, par=1.271, rel=0.064) | elapsed=6.5min\n",
      "  step  440/1000 | step_time= 0.97s | loss=1.3741 (cls=0.061, par=1.267, rel=0.046) | elapsed=6.5min\n",
      "  step  445/1000 | step_time= 0.97s | loss=1.4841 (cls=0.095, par=1.331, rel=0.058) | elapsed=6.6min\n",
      "  step  450/1000 | step_time= 0.98s | loss=1.5858 (cls=0.054, par=1.494, rel=0.038) | elapsed=6.7min\n",
      "  step  455/1000 | step_time= 1.60s | loss=1.7330 (cls=0.084, par=1.601, rel=0.048) | elapsed=6.8min\n",
      "  step  460/1000 | step_time= 0.72s | loss=1.3225 (cls=0.100, par=1.165, rel=0.058) | elapsed=6.9min\n",
      "  step  465/1000 | step_time= 0.88s | loss=1.6136 (cls=0.059, par=1.515, rel=0.040) | elapsed=6.9min\n",
      "  step  470/1000 | step_time= 0.72s | loss=1.4210 (cls=0.105, par=1.257, rel=0.059) | elapsed=7.0min\n",
      "  step  475/1000 | step_time= 0.98s | loss=1.7815 (cls=0.066, par=1.676, rel=0.040) | elapsed=7.1min\n",
      "  step  480/1000 | step_time= 0.68s | loss=1.5723 (cls=0.090, par=1.417, rel=0.065) | elapsed=7.1min\n",
      "  step  485/1000 | step_time= 0.56s | loss=1.1989 (cls=0.090, par=1.061, rel=0.048) | elapsed=7.2min\n",
      "  step  490/1000 | step_time= 0.97s | loss=1.3345 (cls=0.067, par=1.210, rel=0.058) | elapsed=7.3min\n",
      "  step  495/1000 | step_time= 0.70s | loss=1.1620 (cls=0.068, par=1.030, rel=0.064) | elapsed=7.4min\n",
      "  step  500/1000 | step_time= 0.96s | loss=1.3480 (cls=0.050, par=1.256, rel=0.042) | elapsed=7.4min\n",
      "  step  505/1000 | step_time= 0.94s | loss=1.6539 (cls=0.059, par=1.560, rel=0.035) | elapsed=7.5min\n",
      "  step  510/1000 | step_time= 0.98s | loss=1.2190 (cls=0.040, par=1.152, rel=0.026) | elapsed=7.6min\n",
      "  step  515/1000 | step_time= 0.96s | loss=1.3529 (cls=0.063, par=1.258, rel=0.032) | elapsed=7.7min\n",
      "  step  520/1000 | step_time= 0.69s | loss=1.7464 (cls=0.066, par=1.627, rel=0.053) | elapsed=7.8min\n",
      "  step  525/1000 | step_time= 0.57s | loss=1.5483 (cls=0.082, par=1.400, rel=0.066) | elapsed=7.8min\n",
      "  step  530/1000 | step_time= 0.93s | loss=1.4270 (cls=0.078, par=1.310, rel=0.039) | elapsed=7.9min\n",
      "  step  535/1000 | step_time= 0.95s | loss=1.2645 (cls=0.065, par=1.167, rel=0.032) | elapsed=8.0min\n",
      "  step  540/1000 | step_time= 1.01s | loss=1.2179 (cls=0.064, par=1.106, rel=0.048) | elapsed=8.1min\n",
      "  step  545/1000 | step_time= 0.98s | loss=1.3241 (cls=0.043, par=1.253, rel=0.028) | elapsed=8.1min\n",
      "  step  550/1000 | step_time= 0.90s | loss=1.2519 (cls=0.050, par=1.158, rel=0.044) | elapsed=8.2min\n",
      "  step  555/1000 | step_time= 0.96s | loss=1.5724 (cls=0.045, par=1.500, rel=0.027) | elapsed=8.3min\n",
      "  step  560/1000 | step_time= 0.94s | loss=1.1400 (cls=0.066, par=1.037, rel=0.037) | elapsed=8.3min\n",
      "  step  565/1000 | step_time= 1.00s | loss=1.1601 (cls=0.043, par=1.082, rel=0.035) | elapsed=8.4min\n",
      "  step  570/1000 | step_time= 0.95s | loss=1.1905 (cls=0.080, par=1.073, rel=0.038) | elapsed=8.5min\n",
      "  step  575/1000 | step_time= 0.77s | loss=1.0412 (cls=0.056, par=0.946, rel=0.039) | elapsed=8.6min\n",
      "  step  580/1000 | step_time= 0.84s | loss=1.3829 (cls=0.051, par=1.292, rel=0.040) | elapsed=8.6min\n",
      "  step  585/1000 | step_time= 0.95s | loss=1.0988 (cls=0.053, par=1.006, rel=0.040) | elapsed=8.7min\n",
      "  step  590/1000 | step_time= 0.94s | loss=1.3277 (cls=0.088, par=1.182, rel=0.058) | elapsed=8.8min\n",
      "  step  595/1000 | step_time= 0.85s | loss=1.7708 (cls=0.071, par=1.656, rel=0.044) | elapsed=8.9min\n",
      "  step  600/1000 | step_time= 0.72s | loss=1.5144 (cls=0.107, par=1.337, rel=0.070) | elapsed=8.9min\n",
      "  step  605/1000 | step_time= 0.99s | loss=1.0398 (cls=0.037, par=0.984, rel=0.019) | elapsed=9.0min\n",
      "  step  610/1000 | step_time= 0.96s | loss=1.0804 (cls=0.043, par=1.011, rel=0.027) | elapsed=9.1min\n",
      "  step  615/1000 | step_time= 0.96s | loss=0.8508 (cls=0.036, par=0.796, rel=0.019) | elapsed=9.2min\n",
      "  step  620/1000 | step_time= 1.67s | loss=1.3152 (cls=0.113, par=1.162, rel=0.040) | elapsed=9.2min\n",
      "  step  625/1000 | step_time= 0.94s | loss=0.9285 (cls=0.056, par=0.843, rel=0.030) | elapsed=9.3min\n",
      "  step  630/1000 | step_time= 0.69s | loss=0.9645 (cls=0.061, par=0.860, rel=0.043) | elapsed=9.4min\n",
      "  step  635/1000 | step_time= 0.94s | loss=1.2577 (cls=0.056, par=1.174, rel=0.027) | elapsed=9.5min\n",
      "  step  640/1000 | step_time= 0.94s | loss=0.9458 (cls=0.071, par=0.841, rel=0.034) | elapsed=9.5min\n",
      "  step  645/1000 | step_time= 0.68s | loss=1.1459 (cls=0.053, par=1.050, rel=0.043) | elapsed=9.6min\n",
      "  step  650/1000 | step_time= 0.82s | loss=1.3290 (cls=0.067, par=1.202, rel=0.060) | elapsed=9.7min\n",
      "  step  655/1000 | step_time= 0.83s | loss=1.1269 (cls=0.085, par=1.002, rel=0.040) | elapsed=9.8min\n",
      "  step  660/1000 | step_time= 0.98s | loss=1.5085 (cls=0.105, par=1.346, rel=0.058) | elapsed=9.8min\n",
      "  step  665/1000 | step_time= 0.98s | loss=1.4923 (cls=0.064, par=1.395, rel=0.034) | elapsed=9.9min\n",
      "  step  670/1000 | step_time= 1.00s | loss=1.3456 (cls=0.065, par=1.238, rel=0.042) | elapsed=10.0min\n",
      "  step  675/1000 | step_time= 0.53s | loss=1.4604 (cls=0.089, par=1.295, rel=0.076) | elapsed=10.1min\n",
      "  step  680/1000 | step_time= 0.98s | loss=1.3319 (cls=0.051, par=1.245, rel=0.036) | elapsed=10.1min\n",
      "  step  685/1000 | step_time= 0.95s | loss=1.1907 (cls=0.056, par=1.109, rel=0.026) | elapsed=10.2min\n",
      "  step  690/1000 | step_time= 0.95s | loss=1.2187 (cls=0.051, par=1.133, rel=0.035) | elapsed=10.3min\n",
      "  step  695/1000 | step_time= 1.00s | loss=1.2941 (cls=0.042, par=1.229, rel=0.023) | elapsed=10.4min\n",
      "  step  700/1000 | step_time= 1.00s | loss=1.3506 (cls=0.077, par=1.225, rel=0.049) | elapsed=10.5min\n",
      "  step  705/1000 | step_time= 0.94s | loss=0.6568 (cls=0.043, par=0.596, rel=0.018) | elapsed=10.5min\n",
      "  step  710/1000 | step_time= 1.06s | loss=1.1919 (cls=0.091, par=1.053, rel=0.048) | elapsed=10.6min\n",
      "  step  715/1000 | step_time= 0.99s | loss=0.7323 (cls=0.036, par=0.680, rel=0.017) | elapsed=10.7min\n",
      "  step  720/1000 | step_time= 0.99s | loss=0.9485 (cls=0.040, par=0.884, rel=0.024) | elapsed=10.8min\n",
      "  step  725/1000 | step_time= 1.03s | loss=1.1101 (cls=0.062, par=0.988, rel=0.060) | elapsed=10.9min\n",
      "  step  730/1000 | step_time= 1.02s | loss=1.2636 (cls=0.054, par=1.172, rel=0.037) | elapsed=10.9min\n",
      "  step  735/1000 | step_time= 1.02s | loss=1.0660 (cls=0.044, par=0.987, rel=0.035) | elapsed=11.0min\n",
      "  step  740/1000 | step_time= 1.02s | loss=1.5361 (cls=0.077, par=1.384, rel=0.076) | elapsed=11.1min\n",
      "  step  745/1000 | step_time= 1.00s | loss=1.4634 (cls=0.040, par=1.392, rel=0.031) | elapsed=11.2min\n",
      "  step  750/1000 | step_time= 0.99s | loss=0.9557 (cls=0.036, par=0.900, rel=0.020) | elapsed=11.2min\n",
      "  step  755/1000 | step_time= 0.69s | loss=1.2269 (cls=0.068, par=1.116, rel=0.043) | elapsed=11.3min\n",
      "  step  760/1000 | step_time= 0.23s | loss=1.3161 (cls=0.192, par=1.071, rel=0.053) | elapsed=11.4min\n",
      "  step  765/1000 | step_time= 0.97s | loss=1.5652 (cls=0.070, par=1.464, rel=0.031) | elapsed=11.4min\n",
      "  step  770/1000 | step_time= 0.97s | loss=1.1362 (cls=0.061, par=1.053, rel=0.022) | elapsed=11.5min\n",
      "  step  775/1000 | step_time= 0.69s | loss=0.7822 (cls=0.071, par=0.688, rel=0.023) | elapsed=11.6min\n",
      "  step  780/1000 | step_time= 0.97s | loss=1.3160 (cls=0.044, par=1.241, rel=0.031) | elapsed=11.7min\n",
      "  step  785/1000 | step_time= 1.02s | loss=0.9127 (cls=0.049, par=0.843, rel=0.020) | elapsed=11.7min\n",
      "  step  790/1000 | step_time= 1.00s | loss=0.8092 (cls=0.035, par=0.758, rel=0.016) | elapsed=11.8min\n",
      "  step  795/1000 | step_time= 0.96s | loss=1.4050 (cls=0.070, par=1.296, rel=0.039) | elapsed=11.9min\n",
      "  step  800/1000 | step_time= 0.80s | loss=0.8657 (cls=0.036, par=0.803, rel=0.027) | elapsed=12.0min\n",
      "  step  805/1000 | step_time= 0.52s | loss=0.6989 (cls=0.060, par=0.617, rel=0.022) | elapsed=12.0min\n",
      "  step  810/1000 | step_time= 1.00s | loss=1.0117 (cls=0.064, par=0.917, rel=0.030) | elapsed=12.1min\n",
      "  step  815/1000 | step_time= 0.93s | loss=1.5368 (cls=0.098, par=1.377, rel=0.062) | elapsed=12.2min\n",
      "  step  820/1000 | step_time= 1.00s | loss=1.0966 (cls=0.043, par=1.032, rel=0.022) | elapsed=12.2min\n",
      "  step  825/1000 | step_time= 1.02s | loss=0.9669 (cls=0.032, par=0.918, rel=0.017) | elapsed=12.3min\n",
      "  step  830/1000 | step_time= 0.96s | loss=1.0385 (cls=0.060, par=0.940, rel=0.039) | elapsed=12.4min\n",
      "  step  835/1000 | step_time= 1.00s | loss=1.2239 (cls=0.046, par=1.145, rel=0.033) | elapsed=12.5min\n",
      "  step  840/1000 | step_time= 1.02s | loss=0.9213 (cls=0.072, par=0.827, rel=0.023) | elapsed=12.5min\n",
      "  step  845/1000 | step_time= 0.41s | loss=1.0776 (cls=0.049, par=1.002, rel=0.027) | elapsed=12.6min\n",
      "  step  850/1000 | step_time= 1.02s | loss=1.0273 (cls=0.037, par=0.970, rel=0.021) | elapsed=12.7min\n",
      "  step  855/1000 | step_time= 1.01s | loss=0.7711 (cls=0.045, par=0.702, rel=0.024) | elapsed=12.8min\n",
      "  step  860/1000 | step_time= 0.98s | loss=0.8173 (cls=0.028, par=0.769, rel=0.020) | elapsed=12.8min\n",
      "  step  865/1000 | step_time= 0.41s | loss=0.8827 (cls=0.077, par=0.765, rel=0.040) | elapsed=12.9min\n",
      "  step  870/1000 | step_time= 1.83s | loss=0.9256 (cls=0.067, par=0.814, rel=0.045) | elapsed=13.0min\n",
      "  step  875/1000 | step_time= 1.01s | loss=0.8066 (cls=0.026, par=0.769, rel=0.012) | elapsed=13.1min\n",
      "  step  880/1000 | step_time= 0.99s | loss=0.9053 (cls=0.069, par=0.812, rel=0.025) | elapsed=13.1min\n",
      "  step  885/1000 | step_time= 0.37s | loss=1.5727 (cls=0.096, par=1.416, rel=0.061) | elapsed=13.2min\n",
      "  step  890/1000 | step_time= 0.98s | loss=0.8717 (cls=0.064, par=0.782, rel=0.026) | elapsed=13.3min\n",
      "  step  895/1000 | step_time= 0.72s | loss=1.0144 (cls=0.070, par=0.919, rel=0.026) | elapsed=13.4min\n",
      "  step  900/1000 | step_time= 1.00s | loss=0.9198 (cls=0.052, par=0.843, rel=0.024) | elapsed=13.4min\n",
      "  step  905/1000 | step_time= 0.82s | loss=1.1902 (cls=0.051, par=1.105, rel=0.034) | elapsed=13.5min\n",
      "  step  910/1000 | step_time= 1.00s | loss=0.6805 (cls=0.031, par=0.629, rel=0.020) | elapsed=13.6min\n",
      "  step  915/1000 | step_time= 0.98s | loss=0.6057 (cls=0.060, par=0.526, rel=0.020) | elapsed=13.7min\n",
      "  step  920/1000 | step_time= 1.01s | loss=1.0680 (cls=0.050, par=0.981, rel=0.038) | elapsed=13.7min\n",
      "  step  925/1000 | step_time= 1.01s | loss=1.0322 (cls=0.038, par=0.971, rel=0.024) | elapsed=13.8min\n",
      "  step  930/1000 | step_time= 1.00s | loss=1.1852 (cls=0.059, par=1.094, rel=0.033) | elapsed=13.9min\n",
      "  step  935/1000 | step_time= 0.85s | loss=0.8323 (cls=0.048, par=0.763, rel=0.022) | elapsed=14.0min\n",
      "  step  940/1000 | step_time= 1.01s | loss=1.1116 (cls=0.060, par=1.007, rel=0.045) | elapsed=14.0min\n",
      "  step  945/1000 | step_time= 0.92s | loss=1.3906 (cls=0.106, par=1.229, rel=0.055) | elapsed=14.1min\n",
      "  step  950/1000 | step_time= 0.99s | loss=1.1714 (cls=0.078, par=1.062, rel=0.032) | elapsed=14.2min\n",
      "  step  955/1000 | step_time= 1.01s | loss=1.0898 (cls=0.071, par=0.965, rel=0.055) | elapsed=14.3min\n",
      "  step  960/1000 | step_time= 1.00s | loss=1.3484 (cls=0.083, par=1.210, rel=0.055) | elapsed=14.3min\n",
      "  step  965/1000 | step_time= 0.85s | loss=1.1600 (cls=0.056, par=1.065, rel=0.039) | elapsed=14.4min\n",
      "  step  970/1000 | step_time= 1.00s | loss=0.9092 (cls=0.039, par=0.846, rel=0.024) | elapsed=14.5min\n",
      "  step  975/1000 | step_time= 0.79s | loss=0.8484 (cls=0.028, par=0.801, rel=0.020) | elapsed=14.6min\n",
      "  step  980/1000 | step_time= 0.77s | loss=1.0661 (cls=0.087, par=0.949, rel=0.030) | elapsed=14.6min\n",
      "  step  985/1000 | step_time= 1.08s | loss=0.7605 (cls=0.068, par=0.660, rel=0.033) | elapsed=14.7min\n",
      "  step  990/1000 | step_time= 0.77s | loss=1.1869 (cls=0.060, par=1.088, rel=0.038) | elapsed=14.8min\n",
      "  step  995/1000 | step_time= 0.59s | loss=0.8724 (cls=0.076, par=0.774, rel=0.023) | elapsed=14.9min\n",
      "  step 1000/1000 | step_time= 0.99s | loss=1.0771 (cls=0.041, par=1.007, rel=0.029) | elapsed=14.9min\n",
      "[Train] epoch done in 14.94 min\n",
      "[baseline_softmask_on] ep=1 train={'loss': 1.5398720849752425, 'loss_cls': 0.08018663603626192, 'loss_par': 1.4103576460182667, 'loss_rel': 0.04932779945340007} test={'loss': 0.9649344018101692, 'loss_cls': 0.05517185503616929, 'loss_par': 0.8815332287549973, 'loss_rel': 0.02822931897640228}\n",
      "[baseline_softmask_on] saved -> ablation_runs\\baseline_softmask_on\\best.pt\n",
      "\n",
      "[Train] start epoch, num_docs = 1000\n",
      "  step    1/1000 | step_time= 0.84s | loss=1.0905 (cls=0.104, par=0.952, rel=0.034) | elapsed=0.0min\n",
      "    sanity: seq_len=512, avg_parent_candidates=256.5\n",
      "    sanity: seq_len=512, avg_parent_candidates=256.5\n",
      "  step    5/1000 | step_time= 0.84s | loss=0.8152 (cls=0.051, par=0.743, rel=0.022) | elapsed=0.1min\n",
      "  step   10/1000 | step_time= 0.85s | loss=1.1589 (cls=0.056, par=1.075, rel=0.028) | elapsed=0.1min\n",
      "  step   15/1000 | step_time= 0.84s | loss=0.7554 (cls=0.052, par=0.686, rel=0.018) | elapsed=0.2min\n",
      "  step   20/1000 | step_time= 0.70s | loss=1.6073 (cls=0.094, par=1.436, rel=0.077) | elapsed=0.3min\n",
      "  step   25/1000 | step_time= 0.83s | loss=1.0597 (cls=0.083, par=0.939, rel=0.038) | elapsed=0.3min\n",
      "  step   30/1000 | step_time= 0.82s | loss=0.5284 (cls=0.028, par=0.485, rel=0.016) | elapsed=0.4min\n",
      "  step   35/1000 | step_time= 0.61s | loss=0.8974 (cls=0.069, par=0.800, rel=0.028) | elapsed=0.4min\n",
      "  step   40/1000 | step_time= 0.81s | loss=1.4271 (cls=0.054, par=1.351, rel=0.023) | elapsed=0.5min\n",
      "  step   45/1000 | step_time= 0.55s | loss=0.6979 (cls=0.066, par=0.610, rel=0.021) | elapsed=0.6min\n",
      "  step   50/1000 | step_time= 0.82s | loss=0.6361 (cls=0.028, par=0.599, rel=0.008) | elapsed=0.6min\n",
      "  step   55/1000 | step_time= 0.82s | loss=1.0977 (cls=0.060, par=1.007, rel=0.032) | elapsed=0.7min\n",
      "  step   60/1000 | step_time= 0.80s | loss=0.8166 (cls=0.043, par=0.749, rel=0.025) | elapsed=0.7min\n",
      "  step   65/1000 | step_time= 0.82s | loss=0.7519 (cls=0.028, par=0.708, rel=0.016) | elapsed=0.8min\n",
      "  step   70/1000 | step_time= 0.82s | loss=1.1767 (cls=0.067, par=1.059, rel=0.051) | elapsed=0.9min\n",
      "  step   75/1000 | step_time= 0.80s | loss=0.7796 (cls=0.055, par=0.697, rel=0.028) | elapsed=0.9min\n",
      "  step   80/1000 | step_time= 0.80s | loss=0.7895 (cls=0.047, par=0.721, rel=0.021) | elapsed=1.0min\n",
      "  step   85/1000 | step_time= 0.81s | loss=0.6350 (cls=0.040, par=0.579, rel=0.016) | elapsed=1.0min\n",
      "  step   90/1000 | step_time= 0.80s | loss=1.0145 (cls=0.056, par=0.928, rel=0.030) | elapsed=1.1min\n",
      "  step   95/1000 | step_time= 0.82s | loss=0.9727 (cls=0.065, par=0.874, rel=0.034) | elapsed=1.2min\n",
      "  step  100/1000 | step_time= 0.82s | loss=1.0531 (cls=0.068, par=0.950, rel=0.035) | elapsed=1.2min\n",
      "  step  105/1000 | step_time= 0.83s | loss=1.1227 (cls=0.071, par=1.021, rel=0.031) | elapsed=1.3min\n",
      "  step  110/1000 | step_time= 0.58s | loss=0.9087 (cls=0.049, par=0.837, rel=0.024) | elapsed=1.4min\n",
      "  step  115/1000 | step_time= 0.37s | loss=1.4360 (cls=0.085, par=1.275, rel=0.076) | elapsed=1.4min\n",
      "  step  120/1000 | step_time= 0.82s | loss=0.7881 (cls=0.035, par=0.736, rel=0.017) | elapsed=1.5min\n",
      "  step  125/1000 | step_time= 0.60s | loss=0.9863 (cls=0.049, par=0.910, rel=0.028) | elapsed=1.5min\n",
      "  step  130/1000 | step_time= 0.76s | loss=1.0893 (cls=0.076, par=0.977, rel=0.037) | elapsed=1.6min\n",
      "  step  135/1000 | step_time= 0.81s | loss=0.8455 (cls=0.061, par=0.768, rel=0.017) | elapsed=1.7min\n",
      "  step  140/1000 | step_time= 0.83s | loss=0.7537 (cls=0.036, par=0.698, rel=0.020) | elapsed=1.7min\n",
      "  step  145/1000 | step_time= 0.82s | loss=0.6732 (cls=0.038, par=0.619, rel=0.017) | elapsed=1.8min\n",
      "  step  150/1000 | step_time= 0.46s | loss=1.3029 (cls=0.073, par=1.152, rel=0.079) | elapsed=1.9min\n",
      "  step  155/1000 | step_time= 0.84s | loss=0.9438 (cls=0.060, par=0.844, rel=0.040) | elapsed=1.9min\n",
      "  step  160/1000 | step_time= 0.74s | loss=0.6936 (cls=0.046, par=0.630, rel=0.018) | elapsed=2.0min\n",
      "  step  165/1000 | step_time= 0.81s | loss=1.0018 (cls=0.056, par=0.918, rel=0.028) | elapsed=2.1min\n",
      "  step  170/1000 | step_time= 0.84s | loss=0.9233 (cls=0.055, par=0.839, rel=0.029) | elapsed=2.1min\n",
      "  step  175/1000 | step_time= 0.82s | loss=0.5591 (cls=0.050, par=0.492, rel=0.017) | elapsed=2.2min\n",
      "  step  180/1000 | step_time= 0.41s | loss=1.0886 (cls=0.078, par=0.961, rel=0.049) | elapsed=2.2min\n",
      "  step  185/1000 | step_time= 0.33s | loss=0.6768 (cls=0.043, par=0.616, rel=0.018) | elapsed=2.3min\n",
      "  step  190/1000 | step_time= 0.84s | loss=0.9219 (cls=0.057, par=0.839, rel=0.025) | elapsed=2.3min\n",
      "  step  195/1000 | step_time= 0.80s | loss=0.6706 (cls=0.054, par=0.595, rel=0.022) | elapsed=2.4min\n",
      "  step  200/1000 | step_time= 0.81s | loss=1.0333 (cls=0.053, par=0.944, rel=0.036) | elapsed=2.5min\n",
      "  step  205/1000 | step_time= 0.82s | loss=0.6821 (cls=0.035, par=0.629, rel=0.018) | elapsed=2.5min\n",
      "  step  210/1000 | step_time= 0.84s | loss=0.5540 (cls=0.021, par=0.521, rel=0.012) | elapsed=2.6min\n",
      "  step  215/1000 | step_time= 0.60s | loss=0.7544 (cls=0.045, par=0.698, rel=0.011) | elapsed=2.6min\n",
      "  step  220/1000 | step_time= 0.86s | loss=0.9005 (cls=0.059, par=0.813, rel=0.029) | elapsed=2.7min\n",
      "  step  225/1000 | step_time= 0.86s | loss=0.7638 (cls=0.023, par=0.729, rel=0.013) | elapsed=2.8min\n",
      "  step  230/1000 | step_time= 0.83s | loss=0.7525 (cls=0.037, par=0.691, rel=0.024) | elapsed=2.8min\n",
      "  step  235/1000 | step_time= 0.84s | loss=0.9852 (cls=0.058, par=0.891, rel=0.036) | elapsed=2.9min\n",
      "  step  240/1000 | step_time= 0.90s | loss=0.7063 (cls=0.043, par=0.649, rel=0.014) | elapsed=3.0min\n",
      "  step  245/1000 | step_time= 0.83s | loss=0.7508 (cls=0.050, par=0.674, rel=0.027) | elapsed=3.0min\n",
      "  step  250/1000 | step_time= 0.70s | loss=1.0272 (cls=0.086, par=0.884, rel=0.057) | elapsed=3.1min\n",
      "  step  255/1000 | step_time= 0.84s | loss=0.9183 (cls=0.103, par=0.789, rel=0.027) | elapsed=3.2min\n",
      "  step  260/1000 | step_time= 0.84s | loss=0.7619 (cls=0.051, par=0.693, rel=0.018) | elapsed=3.2min\n",
      "  step  265/1000 | step_time= 0.28s | loss=0.7747 (cls=0.082, par=0.664, rel=0.029) | elapsed=3.3min\n",
      "  step  270/1000 | step_time= 0.83s | loss=1.2072 (cls=0.094, par=1.075, rel=0.038) | elapsed=3.4min\n",
      "  step  275/1000 | step_time= 0.64s | loss=0.8746 (cls=0.088, par=0.762, rel=0.024) | elapsed=3.4min\n",
      "  step  280/1000 | step_time= 0.44s | loss=0.4579 (cls=0.059, par=0.378, rel=0.021) | elapsed=3.5min\n",
      "  step  285/1000 | step_time= 0.83s | loss=0.3282 (cls=0.022, par=0.297, rel=0.009) | elapsed=3.6min\n",
      "  step  290/1000 | step_time= 0.85s | loss=0.7443 (cls=0.063, par=0.652, rel=0.029) | elapsed=3.6min\n",
      "  step  295/1000 | step_time= 0.44s | loss=0.9197 (cls=0.053, par=0.845, rel=0.022) | elapsed=3.7min\n",
      "  step  300/1000 | step_time= 0.84s | loss=1.1318 (cls=0.070, par=1.032, rel=0.030) | elapsed=3.7min\n",
      "  step  305/1000 | step_time= 0.84s | loss=0.9449 (cls=0.055, par=0.867, rel=0.023) | elapsed=3.8min\n",
      "  step  310/1000 | step_time= 0.84s | loss=0.7896 (cls=0.041, par=0.729, rel=0.019) | elapsed=3.9min\n",
      "  step  315/1000 | step_time= 0.84s | loss=1.1445 (cls=0.087, par=1.019, rel=0.038) | elapsed=3.9min\n",
      "  step  320/1000 | step_time= 0.77s | loss=1.5195 (cls=0.078, par=1.386, rel=0.056) | elapsed=4.0min\n",
      "  step  325/1000 | step_time= 0.72s | loss=1.2230 (cls=0.074, par=1.088, rel=0.061) | elapsed=4.0min\n",
      "  step  330/1000 | step_time= 0.85s | loss=0.8025 (cls=0.029, par=0.760, rel=0.014) | elapsed=4.1min\n",
      "  step  335/1000 | step_time= 0.49s | loss=0.7887 (cls=0.053, par=0.709, rel=0.027) | elapsed=4.2min\n",
      "  step  340/1000 | step_time= 0.79s | loss=0.3956 (cls=0.040, par=0.344, rel=0.012) | elapsed=4.2min\n",
      "  step  345/1000 | step_time= 0.82s | loss=0.5507 (cls=0.059, par=0.481, rel=0.011) | elapsed=4.3min\n",
      "  step  350/1000 | step_time= 0.70s | loss=0.4204 (cls=0.043, par=0.364, rel=0.014) | elapsed=4.4min\n",
      "  step  355/1000 | step_time= 0.83s | loss=0.8483 (cls=0.058, par=0.769, rel=0.020) | elapsed=4.4min\n",
      "  step  360/1000 | step_time= 0.61s | loss=0.5747 (cls=0.052, par=0.506, rel=0.017) | elapsed=4.5min\n",
      "  step  365/1000 | step_time= 0.95s | loss=0.9238 (cls=0.047, par=0.848, rel=0.028) | elapsed=4.5min\n",
      "  step  370/1000 | step_time= 0.47s | loss=1.1303 (cls=0.106, par=0.978, rel=0.045) | elapsed=4.6min\n",
      "  step  375/1000 | step_time= 0.67s | loss=1.4212 (cls=0.679, par=0.722, rel=0.021) | elapsed=4.7min\n",
      "  step  380/1000 | step_time= 0.82s | loss=0.9811 (cls=0.059, par=0.897, rel=0.025) | elapsed=4.7min\n",
      "  step  385/1000 | step_time= 0.51s | loss=0.6099 (cls=0.035, par=0.553, rel=0.022) | elapsed=4.8min\n",
      "  step  390/1000 | step_time= 0.85s | loss=0.5815 (cls=0.049, par=0.509, rel=0.024) | elapsed=4.9min\n",
      "  step  395/1000 | step_time= 0.78s | loss=0.9948 (cls=0.066, par=0.888, rel=0.041) | elapsed=4.9min\n",
      "  step  400/1000 | step_time= 0.40s | loss=1.0747 (cls=0.136, par=0.902, rel=0.037) | elapsed=5.0min\n",
      "  step  405/1000 | step_time= 0.79s | loss=0.4941 (cls=0.029, par=0.454, rel=0.012) | elapsed=5.1min\n",
      "  step  410/1000 | step_time= 0.68s | loss=1.1221 (cls=0.044, par=1.042, rel=0.036) | elapsed=5.1min\n",
      "  step  415/1000 | step_time= 0.44s | loss=1.2309 (cls=0.045, par=1.117, rel=0.069) | elapsed=5.2min\n",
      "  step  420/1000 | step_time= 0.50s | loss=0.3278 (cls=0.030, par=0.286, rel=0.011) | elapsed=5.2min\n",
      "  step  425/1000 | step_time= 0.52s | loss=1.0340 (cls=0.066, par=0.938, rel=0.030) | elapsed=5.3min\n",
      "  step  430/1000 | step_time= 0.83s | loss=1.1291 (cls=0.052, par=1.037, rel=0.039) | elapsed=5.4min\n",
      "  step  435/1000 | step_time= 0.83s | loss=1.1921 (cls=0.068, par=1.098, rel=0.026) | elapsed=5.4min\n",
      "  step  440/1000 | step_time= 0.81s | loss=0.9200 (cls=0.055, par=0.837, rel=0.028) | elapsed=5.5min\n",
      "  step  445/1000 | step_time= 0.83s | loss=1.1996 (cls=0.084, par=1.075, rel=0.041) | elapsed=5.5min\n",
      "  step  450/1000 | step_time= 0.86s | loss=0.7892 (cls=0.039, par=0.728, rel=0.022) | elapsed=5.6min\n",
      "  step  455/1000 | step_time= 0.84s | loss=1.4409 (cls=0.074, par=1.327, rel=0.040) | elapsed=5.7min\n",
      "  step  460/1000 | step_time= 0.31s | loss=1.0498 (cls=0.079, par=0.923, rel=0.048) | elapsed=5.7min\n",
      "  step  465/1000 | step_time= 0.83s | loss=1.3653 (cls=0.082, par=1.238, rel=0.045) | elapsed=5.8min\n",
      "  step  470/1000 | step_time= 0.81s | loss=1.2608 (cls=0.100, par=1.123, rel=0.038) | elapsed=5.9min\n",
      "  step  475/1000 | step_time= 0.84s | loss=0.7784 (cls=0.047, par=0.713, rel=0.019) | elapsed=5.9min\n",
      "  step  480/1000 | step_time= 0.84s | loss=1.0927 (cls=0.041, par=1.020, rel=0.031) | elapsed=6.0min\n",
      "  step  485/1000 | step_time= 0.84s | loss=1.0119 (cls=0.038, par=0.946, rel=0.028) | elapsed=6.1min\n",
      "  step  490/1000 | step_time= 0.57s | loss=1.2196 (cls=0.057, par=1.125, rel=0.038) | elapsed=6.1min\n",
      "  step  495/1000 | step_time= 0.59s | loss=1.0625 (cls=0.053, par=0.964, rel=0.045) | elapsed=6.2min\n",
      "  step  500/1000 | step_time= 0.84s | loss=0.7290 (cls=0.034, par=0.676, rel=0.019) | elapsed=6.2min\n",
      "  step  505/1000 | step_time= 0.85s | loss=0.8511 (cls=0.041, par=0.787, rel=0.023) | elapsed=6.3min\n",
      "  step  510/1000 | step_time= 0.83s | loss=0.8906 (cls=0.056, par=0.817, rel=0.018) | elapsed=6.4min\n",
      "  step  515/1000 | step_time= 0.83s | loss=0.5554 (cls=0.021, par=0.525, rel=0.010) | elapsed=6.5min\n",
      "  step  520/1000 | step_time= 0.66s | loss=1.1804 (cls=0.070, par=1.063, rel=0.047) | elapsed=6.5min\n",
      "  step  525/1000 | step_time= 0.68s | loss=0.7723 (cls=0.050, par=0.703, rel=0.019) | elapsed=6.6min\n",
      "  step  530/1000 | step_time= 0.84s | loss=0.7259 (cls=0.033, par=0.676, rel=0.016) | elapsed=6.6min\n",
      "  step  535/1000 | step_time= 0.57s | loss=0.4124 (cls=0.057, par=0.346, rel=0.009) | elapsed=6.7min\n",
      "  step  540/1000 | step_time= 0.83s | loss=0.7637 (cls=0.041, par=0.708, rel=0.014) | elapsed=6.8min\n",
      "  step  545/1000 | step_time= 0.84s | loss=0.9528 (cls=0.026, par=0.906, rel=0.021) | elapsed=6.8min\n",
      "  step  550/1000 | step_time= 0.39s | loss=1.2352 (cls=0.088, par=1.102, rel=0.045) | elapsed=6.9min\n",
      "  step  555/1000 | step_time= 0.84s | loss=1.1582 (cls=0.066, par=1.035, rel=0.058) | elapsed=7.0min\n",
      "  step  560/1000 | step_time= 0.58s | loss=1.1249 (cls=0.072, par=1.016, rel=0.037) | elapsed=7.0min\n",
      "  step  565/1000 | step_time= 0.83s | loss=0.8759 (cls=0.050, par=0.801, rel=0.025) | elapsed=7.1min\n",
      "  step  570/1000 | step_time= 0.38s | loss=0.9129 (cls=0.067, par=0.809, rel=0.037) | elapsed=7.1min\n",
      "  step  575/1000 | step_time= 0.81s | loss=0.8696 (cls=0.067, par=0.781, rel=0.021) | elapsed=7.2min\n",
      "  step  580/1000 | step_time= 0.63s | loss=0.7660 (cls=0.050, par=0.703, rel=0.012) | elapsed=7.2min\n",
      "  step  585/1000 | step_time= 0.65s | loss=0.7187 (cls=0.056, par=0.644, rel=0.019) | elapsed=7.3min\n",
      "  step  590/1000 | step_time= 0.76s | loss=0.5633 (cls=0.035, par=0.514, rel=0.014) | elapsed=7.4min\n",
      "  step  595/1000 | step_time= 0.83s | loss=0.9245 (cls=0.043, par=0.862, rel=0.019) | elapsed=7.4min\n",
      "  step  600/1000 | step_time= 0.84s | loss=0.7838 (cls=0.028, par=0.736, rel=0.019) | elapsed=7.5min\n",
      "  step  605/1000 | step_time= 0.49s | loss=0.7331 (cls=0.071, par=0.642, rel=0.020) | elapsed=7.5min\n",
      "  step  610/1000 | step_time= 0.47s | loss=0.5780 (cls=0.103, par=0.465, rel=0.011) | elapsed=7.6min\n",
      "  step  615/1000 | step_time= 0.65s | loss=0.7239 (cls=0.043, par=0.654, rel=0.027) | elapsed=7.7min\n",
      "  step  620/1000 | step_time= 0.84s | loss=0.7531 (cls=0.025, par=0.718, rel=0.011) | elapsed=7.7min\n",
      "  step  625/1000 | step_time= 0.59s | loss=1.0517 (cls=0.074, par=0.936, rel=0.042) | elapsed=7.8min\n",
      "  step  630/1000 | step_time= 0.84s | loss=0.6043 (cls=0.056, par=0.528, rel=0.020) | elapsed=7.8min\n",
      "  step  635/1000 | step_time= 0.86s | loss=1.0129 (cls=0.067, par=0.917, rel=0.029) | elapsed=7.9min\n",
      "  step  640/1000 | step_time= 0.79s | loss=0.6663 (cls=0.043, par=0.611, rel=0.012) | elapsed=8.0min\n",
      "  step  645/1000 | step_time= 0.83s | loss=0.6554 (cls=0.029, par=0.611, rel=0.015) | elapsed=8.0min\n",
      "  step  650/1000 | step_time= 0.82s | loss=1.1378 (cls=0.063, par=1.026, rel=0.049) | elapsed=8.1min\n",
      "  step  655/1000 | step_time= 0.82s | loss=0.9025 (cls=0.036, par=0.851, rel=0.016) | elapsed=8.2min\n",
      "  step  660/1000 | step_time= 0.84s | loss=0.5012 (cls=0.025, par=0.467, rel=0.009) | elapsed=8.3min\n",
      "  step  665/1000 | step_time= 0.82s | loss=1.2017 (cls=0.094, par=1.054, rel=0.053) | elapsed=8.3min\n",
      "  step  670/1000 | step_time= 0.81s | loss=1.1323 (cls=0.075, par=1.018, rel=0.039) | elapsed=8.4min\n",
      "  step  675/1000 | step_time= 1.05s | loss=0.9110 (cls=0.051, par=0.835, rel=0.025) | elapsed=8.5min\n",
      "  step  680/1000 | step_time= 0.84s | loss=0.6022 (cls=0.042, par=0.549, rel=0.011) | elapsed=8.5min\n",
      "  step  685/1000 | step_time= 0.84s | loss=1.0465 (cls=0.066, par=0.954, rel=0.026) | elapsed=8.6min\n",
      "  step  690/1000 | step_time= 0.84s | loss=0.9180 (cls=0.063, par=0.823, rel=0.032) | elapsed=8.7min\n",
      "  step  695/1000 | step_time= 0.74s | loss=0.6039 (cls=0.055, par=0.522, rel=0.027) | elapsed=8.7min\n",
      "  step  700/1000 | step_time= 0.76s | loss=0.7555 (cls=0.073, par=0.657, rel=0.025) | elapsed=8.8min\n",
      "  step  705/1000 | step_time= 0.84s | loss=0.6389 (cls=0.052, par=0.570, rel=0.016) | elapsed=8.9min\n",
      "  step  710/1000 | step_time= 0.49s | loss=0.6027 (cls=0.045, par=0.529, rel=0.029) | elapsed=8.9min\n",
      "  step  715/1000 | step_time= 0.85s | loss=0.6564 (cls=0.025, par=0.618, rel=0.013) | elapsed=9.0min\n",
      "  step  720/1000 | step_time= 0.84s | loss=0.8554 (cls=0.057, par=0.780, rel=0.019) | elapsed=9.0min\n",
      "  step  725/1000 | step_time= 0.83s | loss=1.0943 (cls=0.077, par=0.987, rel=0.030) | elapsed=9.1min\n",
      "  step  730/1000 | step_time= 0.84s | loss=1.1195 (cls=0.054, par=1.041, rel=0.024) | elapsed=9.2min\n",
      "  step  735/1000 | step_time= 0.84s | loss=1.2521 (cls=0.092, par=1.109, rel=0.051) | elapsed=9.2min\n",
      "  step  740/1000 | step_time= 0.29s | loss=0.7568 (cls=0.085, par=0.650, rel=0.022) | elapsed=9.3min\n",
      "  step  745/1000 | step_time= 0.82s | loss=0.9702 (cls=0.080, par=0.864, rel=0.026) | elapsed=9.4min\n",
      "  step  750/1000 | step_time= 0.30s | loss=0.7343 (cls=0.066, par=0.638, rel=0.030) | elapsed=9.4min\n",
      "  step  755/1000 | step_time= 0.58s | loss=0.9266 (cls=0.074, par=0.822, rel=0.031) | elapsed=9.5min\n",
      "  step  760/1000 | step_time= 0.84s | loss=0.9677 (cls=0.060, par=0.873, rel=0.034) | elapsed=9.5min\n",
      "  step  765/1000 | step_time= 0.69s | loss=1.0052 (cls=0.088, par=0.884, rel=0.033) | elapsed=9.6min\n",
      "  step  770/1000 | step_time= 0.48s | loss=0.5256 (cls=0.061, par=0.444, rel=0.021) | elapsed=9.7min\n",
      "  step  775/1000 | step_time= 0.84s | loss=0.5734 (cls=0.055, par=0.502, rel=0.017) | elapsed=9.7min\n",
      "  step  780/1000 | step_time= 0.84s | loss=0.4023 (cls=0.013, par=0.383, rel=0.006) | elapsed=9.8min\n",
      "  step  785/1000 | step_time= 0.84s | loss=1.0758 (cls=0.071, par=0.976, rel=0.028) | elapsed=9.9min\n",
      "  step  790/1000 | step_time= 0.84s | loss=0.6667 (cls=0.054, par=0.600, rel=0.013) | elapsed=10.0min\n",
      "  step  795/1000 | step_time= 0.21s | loss=0.7275 (cls=0.088, par=0.607, rel=0.032) | elapsed=10.0min\n",
      "  step  800/1000 | step_time= 0.83s | loss=0.8354 (cls=0.056, par=0.747, rel=0.033) | elapsed=10.1min\n",
      "  step  805/1000 | step_time= 0.74s | loss=1.2886 (cls=0.084, par=1.165, rel=0.040) | elapsed=10.2min\n",
      "  step  810/1000 | step_time= 0.81s | loss=0.7921 (cls=0.043, par=0.733, rel=0.017) | elapsed=10.2min\n",
      "  step  815/1000 | step_time= 0.85s | loss=0.9759 (cls=0.038, par=0.918, rel=0.020) | elapsed=10.3min\n",
      "  step  820/1000 | step_time= 0.84s | loss=0.5562 (cls=0.027, par=0.519, rel=0.010) | elapsed=10.3min\n",
      "  step  825/1000 | step_time= 0.71s | loss=0.6819 (cls=0.040, par=0.625, rel=0.017) | elapsed=10.4min\n",
      "  step  830/1000 | step_time= 0.84s | loss=0.8765 (cls=0.060, par=0.798, rel=0.018) | elapsed=10.5min\n",
      "  step  835/1000 | step_time= 0.74s | loss=1.1104 (cls=0.087, par=0.995, rel=0.028) | elapsed=10.5min\n",
      "  step  840/1000 | step_time= 0.84s | loss=0.8263 (cls=0.048, par=0.761, rel=0.018) | elapsed=10.6min\n",
      "  step  845/1000 | step_time= 0.83s | loss=0.9828 (cls=0.087, par=0.873, rel=0.023) | elapsed=10.7min\n",
      "  step  850/1000 | step_time= 0.83s | loss=0.9683 (cls=0.078, par=0.868, rel=0.022) | elapsed=10.7min\n",
      "  step  855/1000 | step_time= 0.83s | loss=0.6993 (cls=0.045, par=0.634, rel=0.020) | elapsed=10.8min\n",
      "  step  860/1000 | step_time= 0.84s | loss=0.9195 (cls=0.053, par=0.851, rel=0.016) | elapsed=10.9min\n",
      "  step  865/1000 | step_time= 0.82s | loss=0.5767 (cls=0.052, par=0.517, rel=0.008) | elapsed=10.9min\n",
      "  step  870/1000 | step_time= 0.82s | loss=1.1553 (cls=0.081, par=1.029, rel=0.046) | elapsed=11.0min\n",
      "  step  875/1000 | step_time= 0.81s | loss=0.6405 (cls=0.055, par=0.567, rel=0.019) | elapsed=11.1min\n",
      "  step  880/1000 | step_time= 0.84s | loss=0.4866 (cls=0.028, par=0.451, rel=0.008) | elapsed=11.1min\n",
      "  step  885/1000 | step_time= 0.40s | loss=1.1135 (cls=0.082, par=0.990, rel=0.042) | elapsed=11.2min\n",
      "  step  890/1000 | step_time= 0.84s | loss=1.5548 (cls=0.084, par=1.397, rel=0.074) | elapsed=11.2min\n",
      "  step  895/1000 | step_time= 0.85s | loss=1.0022 (cls=0.053, par=0.914, rel=0.035) | elapsed=11.3min\n",
      "  step  900/1000 | step_time= 0.85s | loss=0.6251 (cls=0.027, par=0.586, rel=0.012) | elapsed=11.4min\n",
      "  step  905/1000 | step_time= 0.63s | loss=0.5467 (cls=0.059, par=0.472, rel=0.016) | elapsed=11.4min\n",
      "  step  910/1000 | step_time= 0.84s | loss=0.6763 (cls=0.040, par=0.623, rel=0.014) | elapsed=11.5min\n",
      "  step  915/1000 | step_time= 0.28s | loss=0.5383 (cls=0.048, par=0.476, rel=0.015) | elapsed=11.6min\n",
      "  step  920/1000 | step_time= 0.85s | loss=0.7798 (cls=0.035, par=0.732, rel=0.013) | elapsed=11.7min\n",
      "  step  925/1000 | step_time= 0.50s | loss=0.7203 (cls=0.050, par=0.651, rel=0.020) | elapsed=11.7min\n",
      "  step  930/1000 | step_time= 0.86s | loss=0.7857 (cls=0.046, par=0.711, rel=0.029) | elapsed=11.8min\n",
      "  step  935/1000 | step_time= 0.65s | loss=0.7120 (cls=0.077, par=0.558, rel=0.077) | elapsed=11.9min\n",
      "  step  940/1000 | step_time= 0.87s | loss=0.7459 (cls=0.029, par=0.701, rel=0.016) | elapsed=11.9min\n",
      "  step  945/1000 | step_time= 0.56s | loss=0.6012 (cls=0.030, par=0.555, rel=0.016) | elapsed=12.0min\n",
      "  step  950/1000 | step_time= 0.31s | loss=0.8477 (cls=0.065, par=0.758, rel=0.025) | elapsed=12.0min\n",
      "  step  955/1000 | step_time= 0.75s | loss=0.7335 (cls=0.046, par=0.669, rel=0.019) | elapsed=12.1min\n",
      "  step  960/1000 | step_time= 0.62s | loss=0.4532 (cls=0.035, par=0.401, rel=0.018) | elapsed=12.1min\n",
      "  step  965/1000 | step_time= 0.84s | loss=0.8141 (cls=0.071, par=0.728, rel=0.015) | elapsed=12.2min\n",
      "  step  970/1000 | step_time= 0.76s | loss=0.8039 (cls=0.047, par=0.730, rel=0.026) | elapsed=12.2min\n",
      "  step  975/1000 | step_time= 0.85s | loss=1.3998 (cls=0.051, par=1.311, rel=0.038) | elapsed=12.3min\n",
      "  step  980/1000 | step_time= 0.85s | loss=0.7663 (cls=0.031, par=0.721, rel=0.014) | elapsed=12.4min\n",
      "  step  985/1000 | step_time= 0.44s | loss=1.1310 (cls=0.070, par=1.007, rel=0.054) | elapsed=12.5min\n",
      "  step  990/1000 | step_time= 0.76s | loss=0.7031 (cls=0.043, par=0.646, rel=0.014) | elapsed=12.5min\n",
      "  step  995/1000 | step_time= 0.86s | loss=0.4817 (cls=0.021, par=0.452, rel=0.009) | elapsed=12.6min\n",
      "  step 1000/1000 | step_time= 0.86s | loss=0.8177 (cls=0.058, par=0.733, rel=0.026) | elapsed=12.7min\n",
      "[Train] epoch done in 12.66 min\n",
      "[baseline_softmask_on] ep=2 train={'loss': 0.8887081586122513, 'loss_cls': 0.059795586314052344, 'loss_par': 0.8010726687610149, 'loss_rel': 0.027839903366286306} test={'loss': 0.7245140718817711, 'loss_cls': 0.04792706891708076, 'loss_par': 0.6567082076072693, 'loss_rel': 0.019878796488512308}\n",
      "[baseline_softmask_on] saved -> ablation_runs\\baseline_softmask_on\\best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "export -> ablation_runs\\baseline_softmask_on\\exports_test: 100%|█████████████████████| 500/500 [03:23<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "[baseline_softmask_on] export done, start eval...\n",
      "[baseline_softmask_on] eval done.\n",
      "\n",
      "[Train] start epoch, num_docs = 1000\n",
      "  step    1/1000 | step_time= 3.17s | loss=5.1444 (cls=0.674, par=4.271, rel=0.199) | elapsed=0.1min\n",
      "    sanity: seq_len=132, avg_parent_candidates=66.5\n",
      "    sanity: seq_len=161, avg_parent_candidates=81.0\n",
      "  step    5/1000 | step_time= 0.82s | loss=4.4172 (cls=0.282, par=3.964, rel=0.171) | elapsed=0.1min\n",
      "  step   10/1000 | step_time= 0.73s | loss=4.6317 (cls=0.299, par=4.173, rel=0.160) | elapsed=0.2min\n",
      "  step   15/1000 | step_time= 0.83s | loss=3.8238 (cls=0.109, par=3.622, rel=0.092) | elapsed=0.2min\n",
      "  step   20/1000 | step_time= 0.79s | loss=3.6184 (cls=0.153, par=3.367, rel=0.098) | elapsed=0.3min\n",
      "  step   25/1000 | step_time= 0.83s | loss=3.6194 (cls=0.096, par=3.449, rel=0.075) | elapsed=0.3min\n",
      "  step   30/1000 | step_time= 0.81s | loss=3.2543 (cls=0.074, par=3.136, rel=0.045) | elapsed=0.4min\n",
      "  step   35/1000 | step_time= 0.83s | loss=3.0171 (cls=0.081, par=2.885, rel=0.050) | elapsed=0.5min\n",
      "  step   40/1000 | step_time= 0.85s | loss=3.1965 (cls=0.094, par=3.028, rel=0.074) | elapsed=0.5min\n",
      "  step   45/1000 | step_time= 0.84s | loss=2.9305 (cls=0.100, par=2.766, rel=0.064) | elapsed=0.6min\n",
      "  step   50/1000 | step_time= 0.84s | loss=2.7895 (cls=0.088, par=2.641, rel=0.061) | elapsed=0.7min\n",
      "  step   55/1000 | step_time= 0.83s | loss=2.2753 (cls=0.114, par=2.097, rel=0.065) | elapsed=0.7min\n",
      "  step   60/1000 | step_time= 0.85s | loss=1.9871 (cls=0.114, par=1.818, rel=0.055) | elapsed=0.8min\n",
      "  step   65/1000 | step_time= 0.73s | loss=2.0778 (cls=0.156, par=1.861, rel=0.061) | elapsed=0.9min\n",
      "  step   70/1000 | step_time= 0.36s | loss=1.8484 (cls=0.095, par=1.680, rel=0.073) | elapsed=0.9min\n",
      "  step   75/1000 | step_time= 0.40s | loss=1.9474 (cls=0.140, par=1.750, rel=0.058) | elapsed=1.0min\n",
      "  step   80/1000 | step_time= 0.82s | loss=2.2403 (cls=0.111, par=2.048, rel=0.082) | elapsed=1.1min\n",
      "  step   85/1000 | step_time= 0.51s | loss=1.8967 (cls=0.104, par=1.720, rel=0.072) | elapsed=1.1min\n",
      "  step   90/1000 | step_time= 0.82s | loss=1.9297 (cls=0.101, par=1.769, rel=0.060) | elapsed=1.2min\n",
      "  step   95/1000 | step_time= 0.84s | loss=2.2202 (cls=0.126, par=2.036, rel=0.058) | elapsed=1.3min\n",
      "  step  100/1000 | step_time= 0.55s | loss=1.8928 (cls=0.134, par=1.669, rel=0.089) | elapsed=1.3min\n",
      "  step  105/1000 | step_time= 0.82s | loss=2.0877 (cls=0.060, par=1.986, rel=0.042) | elapsed=1.4min\n",
      "  step  110/1000 | step_time= 0.84s | loss=2.1992 (cls=0.084, par=2.058, rel=0.057) | elapsed=1.4min\n",
      "  step  115/1000 | step_time= 0.59s | loss=2.2777 (cls=0.084, par=2.138, rel=0.055) | elapsed=1.5min\n",
      "  step  120/1000 | step_time= 0.43s | loss=1.9270 (cls=0.141, par=1.688, rel=0.098) | elapsed=1.6min\n",
      "  step  125/1000 | step_time= 0.89s | loss=1.8601 (cls=0.052, par=1.769, rel=0.039) | elapsed=1.6min\n",
      "  step  130/1000 | step_time= 0.24s | loss=1.7509 (cls=0.117, par=1.551, rel=0.083) | elapsed=1.7min\n",
      "  step  135/1000 | step_time= 0.81s | loss=2.1396 (cls=0.073, par=2.004, rel=0.063) | elapsed=1.7min\n",
      "  step  140/1000 | step_time= 0.50s | loss=1.7202 (cls=0.121, par=1.523, rel=0.076) | elapsed=1.8min\n",
      "  step  145/1000 | step_time= 0.84s | loss=2.2137 (cls=0.060, par=2.102, rel=0.052) | elapsed=1.9min\n",
      "  step  150/1000 | step_time= 0.87s | loss=2.1925 (cls=0.078, par=2.069, rel=0.046) | elapsed=1.9min\n",
      "  step  155/1000 | step_time= 0.68s | loss=2.3414 (cls=0.088, par=2.173, rel=0.080) | elapsed=2.0min\n",
      "  step  160/1000 | step_time= 0.69s | loss=1.8949 (cls=0.121, par=1.699, rel=0.075) | elapsed=2.1min\n",
      "  step  165/1000 | step_time= 0.91s | loss=1.5190 (cls=0.083, par=1.371, rel=0.065) | elapsed=2.1min\n",
      "  step  170/1000 | step_time= 0.81s | loss=1.6952 (cls=0.108, par=1.533, rel=0.054) | elapsed=2.2min\n",
      "  step  175/1000 | step_time= 0.56s | loss=1.9026 (cls=0.128, par=1.677, rel=0.098) | elapsed=2.3min\n",
      "  step  180/1000 | step_time= 0.61s | loss=1.7899 (cls=0.079, par=1.638, rel=0.073) | elapsed=2.3min\n",
      "  step  185/1000 | step_time= 0.85s | loss=2.0706 (cls=0.075, par=1.949, rel=0.046) | elapsed=2.4min\n",
      "  step  190/1000 | step_time= 0.83s | loss=2.0707 (cls=0.076, par=1.942, rel=0.053) | elapsed=2.5min\n",
      "  step  195/1000 | step_time= 0.83s | loss=1.7927 (cls=0.049, par=1.709, rel=0.035) | elapsed=2.6min\n",
      "  step  200/1000 | step_time= 0.84s | loss=1.7112 (cls=0.077, par=1.572, rel=0.062) | elapsed=2.6min\n",
      "  step  205/1000 | step_time= 0.86s | loss=1.7573 (cls=0.050, par=1.664, rel=0.043) | elapsed=2.7min\n",
      "  step  210/1000 | step_time= 0.82s | loss=2.0398 (cls=0.062, par=1.935, rel=0.044) | elapsed=2.7min\n",
      "  step  215/1000 | step_time= 0.82s | loss=1.9457 (cls=0.060, par=1.838, rel=0.049) | elapsed=2.8min\n",
      "  step  220/1000 | step_time= 0.86s | loss=1.6439 (cls=0.085, par=1.480, rel=0.079) | elapsed=2.9min\n",
      "  step  225/1000 | step_time= 0.83s | loss=1.9660 (cls=0.064, par=1.863, rel=0.039) | elapsed=2.9min\n",
      "  step  230/1000 | step_time= 0.83s | loss=1.9600 (cls=0.066, par=1.843, rel=0.051) | elapsed=3.0min\n",
      "  step  235/1000 | step_time= 0.34s | loss=1.6325 (cls=0.145, par=1.416, rel=0.072) | elapsed=3.1min\n",
      "  step  240/1000 | step_time= 0.86s | loss=1.8781 (cls=0.072, par=1.745, rel=0.062) | elapsed=3.1min\n",
      "  step  245/1000 | step_time= 0.85s | loss=1.5509 (cls=0.073, par=1.422, rel=0.056) | elapsed=3.2min\n",
      "  step  250/1000 | step_time= 0.86s | loss=1.6162 (cls=0.085, par=1.468, rel=0.064) | elapsed=3.3min\n",
      "  step  255/1000 | step_time= 0.75s | loss=1.8297 (cls=0.065, par=1.715, rel=0.049) | elapsed=3.3min\n",
      "  step  260/1000 | step_time= 0.88s | loss=1.7156 (cls=0.073, par=1.585, rel=0.058) | elapsed=3.4min\n",
      "  step  265/1000 | step_time= 0.67s | loss=1.3266 (cls=0.057, par=1.224, rel=0.046) | elapsed=3.5min\n",
      "  step  270/1000 | step_time= 0.52s | loss=1.6101 (cls=0.106, par=1.426, rel=0.078) | elapsed=3.5min\n",
      "  step  275/1000 | step_time= 0.83s | loss=1.9995 (cls=0.062, par=1.892, rel=0.046) | elapsed=3.6min\n",
      "  step  280/1000 | step_time= 0.83s | loss=1.7062 (cls=0.059, par=1.604, rel=0.044) | elapsed=3.7min\n",
      "  step  285/1000 | step_time= 0.86s | loss=1.7067 (cls=0.073, par=1.568, rel=0.065) | elapsed=3.7min\n",
      "  step  290/1000 | step_time= 0.39s | loss=1.6688 (cls=0.112, par=1.459, rel=0.098) | elapsed=3.8min\n",
      "  step  295/1000 | step_time= 0.83s | loss=1.9707 (cls=0.064, par=1.845, rel=0.062) | elapsed=3.8min\n",
      "  step  300/1000 | step_time= 0.88s | loss=1.5603 (cls=0.104, par=1.400, rel=0.056) | elapsed=3.9min\n",
      "  step  305/1000 | step_time= 0.65s | loss=1.6651 (cls=0.089, par=1.506, rel=0.070) | elapsed=4.0min\n",
      "  step  310/1000 | step_time= 0.83s | loss=1.7497 (cls=0.096, par=1.578, rel=0.075) | elapsed=4.0min\n",
      "  step  315/1000 | step_time= 0.87s | loss=1.6490 (cls=0.053, par=1.559, rel=0.037) | elapsed=4.1min\n",
      "  step  320/1000 | step_time= 0.85s | loss=1.4353 (cls=0.070, par=1.317, rel=0.048) | elapsed=4.2min\n",
      "  step  325/1000 | step_time= 0.84s | loss=1.5148 (cls=0.049, par=1.417, rel=0.049) | elapsed=4.2min\n",
      "  step  330/1000 | step_time= 0.85s | loss=1.6688 (cls=0.062, par=1.566, rel=0.041) | elapsed=4.3min\n",
      "  step  335/1000 | step_time= 0.85s | loss=1.4784 (cls=0.069, par=1.374, rel=0.035) | elapsed=4.4min\n",
      "  step  340/1000 | step_time= 0.84s | loss=1.5960 (cls=0.099, par=1.436, rel=0.061) | elapsed=4.4min\n",
      "  step  345/1000 | step_time= 0.72s | loss=1.7117 (cls=0.109, par=1.533, rel=0.069) | elapsed=4.5min\n",
      "  step  350/1000 | step_time= 0.85s | loss=1.3535 (cls=0.085, par=1.219, rel=0.050) | elapsed=4.6min\n",
      "  step  355/1000 | step_time= 0.82s | loss=1.6053 (cls=0.118, par=1.431, rel=0.056) | elapsed=4.6min\n",
      "  step  360/1000 | step_time= 0.89s | loss=1.5280 (cls=0.066, par=1.415, rel=0.047) | elapsed=4.7min\n",
      "  step  365/1000 | step_time= 0.84s | loss=1.5858 (cls=0.077, par=1.445, rel=0.064) | elapsed=4.8min\n",
      "  step  370/1000 | step_time= 0.84s | loss=1.5833 (cls=0.054, par=1.491, rel=0.038) | elapsed=4.8min\n",
      "  step  375/1000 | step_time= 0.86s | loss=1.6030 (cls=0.100, par=1.447, rel=0.056) | elapsed=4.9min\n",
      "  step  380/1000 | step_time= 0.81s | loss=1.6227 (cls=0.065, par=1.512, rel=0.045) | elapsed=4.9min\n",
      "  step  385/1000 | step_time= 0.85s | loss=1.7320 (cls=0.058, par=1.624, rel=0.050) | elapsed=5.0min\n",
      "  step  390/1000 | step_time= 0.74s | loss=1.8095 (cls=0.071, par=1.681, rel=0.057) | elapsed=5.1min\n",
      "  step  395/1000 | step_time= 0.89s | loss=1.5641 (cls=0.061, par=1.459, rel=0.044) | elapsed=5.1min\n",
      "  step  400/1000 | step_time= 0.73s | loss=1.3141 (cls=0.067, par=1.189, rel=0.058) | elapsed=5.2min\n",
      "  step  405/1000 | step_time= 0.94s | loss=1.5222 (cls=0.103, par=1.367, rel=0.052) | elapsed=5.3min\n",
      "  step  410/1000 | step_time= 0.65s | loss=1.5823 (cls=0.099, par=1.404, rel=0.079) | elapsed=5.4min\n",
      "  step  415/1000 | step_time= 0.86s | loss=1.4855 (cls=0.144, par=1.296, rel=0.046) | elapsed=5.5min\n",
      "  step  420/1000 | step_time= 0.78s | loss=1.2922 (cls=0.062, par=1.171, rel=0.059) | elapsed=5.5min\n",
      "  step  425/1000 | step_time= 1.05s | loss=2.1614 (cls=0.083, par=2.022, rel=0.056) | elapsed=5.6min\n",
      "  step  430/1000 | step_time= 0.88s | loss=1.3846 (cls=0.090, par=1.247, rel=0.047) | elapsed=5.7min\n",
      "  step  435/1000 | step_time= 0.66s | loss=1.3781 (cls=0.061, par=1.254, rel=0.064) | elapsed=5.8min\n",
      "  step  440/1000 | step_time= 0.84s | loss=1.3961 (cls=0.058, par=1.290, rel=0.048) | elapsed=5.8min\n",
      "  step  445/1000 | step_time= 0.88s | loss=1.5152 (cls=0.098, par=1.359, rel=0.059) | elapsed=5.9min\n",
      "  step  450/1000 | step_time= 0.84s | loss=1.5652 (cls=0.059, par=1.469, rel=0.038) | elapsed=6.0min\n",
      "  step  455/1000 | step_time= 0.77s | loss=1.6925 (cls=0.086, par=1.558, rel=0.048) | elapsed=6.0min\n",
      "  step  460/1000 | step_time= 0.61s | loss=1.3669 (cls=0.107, par=1.201, rel=0.059) | elapsed=6.1min\n",
      "  step  465/1000 | step_time= 0.75s | loss=1.6249 (cls=0.059, par=1.526, rel=0.040) | elapsed=6.2min\n",
      "  step  470/1000 | step_time= 0.61s | loss=1.4623 (cls=0.105, par=1.295, rel=0.063) | elapsed=6.2min\n",
      "  step  475/1000 | step_time= 0.83s | loss=1.7907 (cls=0.067, par=1.682, rel=0.041) | elapsed=6.3min\n",
      "  step  480/1000 | step_time= 0.58s | loss=1.5529 (cls=0.091, par=1.397, rel=0.065) | elapsed=6.3min\n",
      "  step  485/1000 | step_time= 0.49s | loss=1.1969 (cls=0.089, par=1.058, rel=0.050) | elapsed=6.4min\n",
      "  step  490/1000 | step_time= 0.85s | loss=1.3607 (cls=0.064, par=1.238, rel=0.058) | elapsed=6.5min\n",
      "  step  495/1000 | step_time= 0.61s | loss=1.1639 (cls=0.066, par=1.031, rel=0.066) | elapsed=6.6min\n",
      "  step  500/1000 | step_time= 0.80s | loss=1.3685 (cls=0.049, par=1.277, rel=0.042) | elapsed=6.6min\n",
      "  step  505/1000 | step_time= 0.82s | loss=1.6563 (cls=0.061, par=1.560, rel=0.035) | elapsed=6.7min\n",
      "  step  510/1000 | step_time= 0.84s | loss=1.2150 (cls=0.042, par=1.146, rel=0.027) | elapsed=6.8min\n",
      "  step  515/1000 | step_time= 0.82s | loss=1.3374 (cls=0.065, par=1.240, rel=0.033) | elapsed=6.8min\n",
      "  step  520/1000 | step_time= 0.60s | loss=1.7519 (cls=0.065, par=1.633, rel=0.054) | elapsed=6.9min\n",
      "  step  525/1000 | step_time= 0.50s | loss=1.5497 (cls=0.082, par=1.402, rel=0.066) | elapsed=7.0min\n",
      "  step  530/1000 | step_time= 0.82s | loss=1.3733 (cls=0.077, par=1.257, rel=0.040) | elapsed=7.0min\n",
      "  step  535/1000 | step_time= 0.82s | loss=1.2672 (cls=0.065, par=1.171, rel=0.032) | elapsed=7.1min\n",
      "  step  540/1000 | step_time= 0.86s | loss=1.2065 (cls=0.060, par=1.095, rel=0.051) | elapsed=7.1min\n",
      "  step  545/1000 | step_time= 0.85s | loss=1.2899 (cls=0.044, par=1.217, rel=0.029) | elapsed=7.2min\n",
      "  step  550/1000 | step_time= 0.75s | loss=1.2401 (cls=0.051, par=1.144, rel=0.045) | elapsed=7.3min\n",
      "  step  555/1000 | step_time= 0.84s | loss=1.5020 (cls=0.043, par=1.430, rel=0.029) | elapsed=7.3min\n",
      "  step  560/1000 | step_time= 0.81s | loss=1.1131 (cls=0.061, par=1.016, rel=0.036) | elapsed=7.4min\n",
      "  step  565/1000 | step_time= 0.85s | loss=1.1815 (cls=0.044, par=1.102, rel=0.036) | elapsed=7.5min\n",
      "  step  570/1000 | step_time= 0.83s | loss=1.1927 (cls=0.082, par=1.071, rel=0.039) | elapsed=7.5min\n",
      "  step  575/1000 | step_time= 0.67s | loss=1.0075 (cls=0.056, par=0.913, rel=0.039) | elapsed=7.6min\n",
      "  step  580/1000 | step_time= 0.73s | loss=1.2460 (cls=0.052, par=1.155, rel=0.039) | elapsed=7.7min\n",
      "  step  585/1000 | step_time= 0.81s | loss=1.0849 (cls=0.053, par=0.991, rel=0.041) | elapsed=7.7min\n",
      "  step  590/1000 | step_time= 0.98s | loss=1.3134 (cls=0.086, par=1.170, rel=0.058) | elapsed=7.8min\n",
      "  step  595/1000 | step_time= 0.75s | loss=1.8047 (cls=0.070, par=1.689, rel=0.046) | elapsed=7.9min\n",
      "  step  600/1000 | step_time= 0.62s | loss=1.5165 (cls=0.106, par=1.340, rel=0.070) | elapsed=7.9min\n",
      "  step  605/1000 | step_time= 0.84s | loss=1.0459 (cls=0.037, par=0.990, rel=0.019) | elapsed=8.0min\n",
      "  step  610/1000 | step_time= 0.83s | loss=1.0775 (cls=0.044, par=1.006, rel=0.027) | elapsed=8.0min\n",
      "  step  615/1000 | step_time= 0.83s | loss=0.8707 (cls=0.033, par=0.820, rel=0.018) | elapsed=8.1min\n",
      "  step  620/1000 | step_time= 0.82s | loss=1.3219 (cls=0.112, par=1.170, rel=0.040) | elapsed=8.2min\n",
      "  step  625/1000 | step_time= 0.81s | loss=0.9499 (cls=0.057, par=0.862, rel=0.030) | elapsed=8.2min\n",
      "  step  630/1000 | step_time= 0.58s | loss=0.9750 (cls=0.059, par=0.873, rel=0.044) | elapsed=8.3min\n",
      "  step  635/1000 | step_time= 0.82s | loss=1.2584 (cls=0.055, par=1.176, rel=0.027) | elapsed=8.3min\n",
      "  step  640/1000 | step_time= 0.81s | loss=0.9669 (cls=0.071, par=0.864, rel=0.032) | elapsed=8.4min\n",
      "  step  645/1000 | step_time= 0.59s | loss=1.1269 (cls=0.054, par=1.031, rel=0.042) | elapsed=8.5min\n",
      "  step  650/1000 | step_time= 0.72s | loss=1.3591 (cls=0.065, par=1.235, rel=0.059) | elapsed=8.6min\n",
      "  step  655/1000 | step_time= 0.71s | loss=1.1410 (cls=0.084, par=1.017, rel=0.039) | elapsed=8.6min\n",
      "  step  660/1000 | step_time= 0.86s | loss=1.4971 (cls=0.102, par=1.338, rel=0.057) | elapsed=8.7min\n",
      "  step  665/1000 | step_time= 0.84s | loss=1.4731 (cls=0.064, par=1.375, rel=0.033) | elapsed=8.8min\n",
      "  step  670/1000 | step_time= 0.85s | loss=1.3438 (cls=0.061, par=1.241, rel=0.042) | elapsed=8.8min\n",
      "  step  675/1000 | step_time= 0.46s | loss=1.4781 (cls=0.086, par=1.313, rel=0.079) | elapsed=8.9min\n",
      "  step  680/1000 | step_time= 0.86s | loss=1.2527 (cls=0.049, par=1.169, rel=0.035) | elapsed=9.0min\n",
      "  step  685/1000 | step_time= 0.82s | loss=1.1667 (cls=0.057, par=1.084, rel=0.026) | elapsed=9.0min\n",
      "  step  690/1000 | step_time= 0.86s | loss=1.2181 (cls=0.049, par=1.135, rel=0.034) | elapsed=9.1min\n",
      "  step  695/1000 | step_time= 0.88s | loss=1.2767 (cls=0.041, par=1.212, rel=0.023) | elapsed=9.2min\n",
      "  step  700/1000 | step_time= 0.93s | loss=1.3637 (cls=0.080, par=1.236, rel=0.048) | elapsed=9.2min\n",
      "  step  705/1000 | step_time= 0.84s | loss=0.6599 (cls=0.045, par=0.597, rel=0.018) | elapsed=9.3min\n",
      "  step  710/1000 | step_time= 0.78s | loss=1.2139 (cls=0.093, par=1.069, rel=0.052) | elapsed=9.4min\n",
      "  step  715/1000 | step_time= 0.87s | loss=0.7230 (cls=0.037, par=0.668, rel=0.017) | elapsed=9.4min\n",
      "  step  720/1000 | step_time= 0.87s | loss=0.9448 (cls=0.039, par=0.883, rel=0.024) | elapsed=9.5min\n",
      "  step  725/1000 | step_time= 0.92s | loss=1.1336 (cls=0.062, par=1.011, rel=0.060) | elapsed=9.6min\n",
      "  step  730/1000 | step_time= 0.91s | loss=1.2539 (cls=0.053, par=1.164, rel=0.037) | elapsed=9.6min\n",
      "  step  735/1000 | step_time= 0.87s | loss=1.0325 (cls=0.046, par=0.952, rel=0.035) | elapsed=9.7min\n",
      "  step  740/1000 | step_time= 0.96s | loss=1.5691 (cls=0.076, par=1.414, rel=0.080) | elapsed=9.8min\n",
      "  step  745/1000 | step_time= 0.88s | loss=1.3135 (cls=0.038, par=1.244, rel=0.031) | elapsed=9.9min\n",
      "  step  750/1000 | step_time= 0.87s | loss=0.9588 (cls=0.032, par=0.906, rel=0.021) | elapsed=9.9min\n",
      "  step  755/1000 | step_time= 0.63s | loss=1.2266 (cls=0.067, par=1.114, rel=0.046) | elapsed=10.0min\n",
      "  step  760/1000 | step_time= 0.20s | loss=1.3400 (cls=0.197, par=1.086, rel=0.057) | elapsed=10.0min\n",
      "  step  765/1000 | step_time= 0.85s | loss=1.6116 (cls=0.071, par=1.507, rel=0.033) | elapsed=10.1min\n",
      "  step  770/1000 | step_time= 0.85s | loss=1.1376 (cls=0.062, par=1.054, rel=0.022) | elapsed=10.2min\n",
      "  step  775/1000 | step_time= 0.60s | loss=0.7771 (cls=0.069, par=0.684, rel=0.024) | elapsed=10.2min\n",
      "  step  780/1000 | step_time= 0.85s | loss=1.2515 (cls=0.042, par=1.178, rel=0.032) | elapsed=10.3min\n",
      "  step  785/1000 | step_time= 0.91s | loss=0.9206 (cls=0.050, par=0.850, rel=0.020) | elapsed=10.4min\n",
      "  step  790/1000 | step_time= 0.85s | loss=0.8492 (cls=0.034, par=0.798, rel=0.017) | elapsed=10.4min\n",
      "  step  795/1000 | step_time= 0.84s | loss=1.3683 (cls=0.071, par=1.257, rel=0.041) | elapsed=10.5min\n",
      "  step  800/1000 | step_time= 0.69s | loss=0.8634 (cls=0.039, par=0.797, rel=0.027) | elapsed=10.5min\n",
      "  step  805/1000 | step_time= 0.45s | loss=0.7066 (cls=0.058, par=0.626, rel=0.022) | elapsed=10.6min\n",
      "  step  810/1000 | step_time= 0.88s | loss=1.0042 (cls=0.068, par=0.908, rel=0.028) | elapsed=10.7min\n",
      "  step  815/1000 | step_time= 0.78s | loss=1.5328 (cls=0.100, par=1.373, rel=0.060) | elapsed=10.7min\n",
      "  step  820/1000 | step_time= 0.88s | loss=1.1265 (cls=0.040, par=1.064, rel=0.022) | elapsed=10.8min\n",
      "  step  825/1000 | step_time= 0.87s | loss=0.9561 (cls=0.032, par=0.907, rel=0.017) | elapsed=10.9min\n",
      "  step  830/1000 | step_time= 0.83s | loss=1.0299 (cls=0.059, par=0.933, rel=0.038) | elapsed=10.9min\n",
      "  step  835/1000 | step_time= 0.84s | loss=1.1551 (cls=0.046, par=1.078, rel=0.031) | elapsed=11.0min\n",
      "  step  840/1000 | step_time= 1.66s | loss=0.9515 (cls=0.070, par=0.858, rel=0.023) | elapsed=11.1min\n",
      "  step  845/1000 | step_time= 0.38s | loss=1.0748 (cls=0.049, par=0.998, rel=0.027) | elapsed=11.1min\n",
      "  step  850/1000 | step_time= 0.86s | loss=1.0134 (cls=0.037, par=0.955, rel=0.021) | elapsed=11.2min\n",
      "  step  855/1000 | step_time= 0.87s | loss=0.7889 (cls=0.046, par=0.718, rel=0.025) | elapsed=11.2min\n",
      "  step  860/1000 | step_time= 0.87s | loss=0.8023 (cls=0.033, par=0.747, rel=0.022) | elapsed=11.3min\n",
      "  step  865/1000 | step_time= 0.35s | loss=0.9366 (cls=0.084, par=0.810, rel=0.043) | elapsed=11.4min\n",
      "  step  870/1000 | step_time= 0.89s | loss=0.9137 (cls=0.063, par=0.805, rel=0.046) | elapsed=11.4min\n",
      "  step  875/1000 | step_time= 0.85s | loss=0.8231 (cls=0.027, par=0.785, rel=0.012) | elapsed=11.5min\n",
      "  step  880/1000 | step_time= 0.84s | loss=0.9203 (cls=0.069, par=0.825, rel=0.026) | elapsed=11.6min\n",
      "  step  885/1000 | step_time= 0.32s | loss=1.5278 (cls=0.093, par=1.375, rel=0.060) | elapsed=11.6min\n",
      "  step  890/1000 | step_time= 0.85s | loss=0.9293 (cls=0.067, par=0.836, rel=0.026) | elapsed=11.7min\n",
      "  step  895/1000 | step_time= 0.60s | loss=0.9750 (cls=0.067, par=0.883, rel=0.025) | elapsed=11.7min\n",
      "  step  900/1000 | step_time= 0.84s | loss=0.9492 (cls=0.053, par=0.872, rel=0.025) | elapsed=11.8min\n",
      "  step  905/1000 | step_time= 0.69s | loss=1.1757 (cls=0.053, par=1.087, rel=0.036) | elapsed=11.9min\n",
      "  step  910/1000 | step_time= 0.86s | loss=0.6464 (cls=0.031, par=0.597, rel=0.019) | elapsed=11.9min\n",
      "  step  915/1000 | step_time= 0.90s | loss=0.5995 (cls=0.060, par=0.519, rel=0.020) | elapsed=12.0min\n",
      "  step  920/1000 | step_time= 0.97s | loss=1.1017 (cls=0.051, par=1.011, rel=0.039) | elapsed=12.1min\n",
      "  step  925/1000 | step_time= 0.88s | loss=0.9780 (cls=0.038, par=0.916, rel=0.025) | elapsed=12.1min\n",
      "  step  930/1000 | step_time= 0.84s | loss=1.1358 (cls=0.056, par=1.046, rel=0.034) | elapsed=12.2min\n",
      "  step  935/1000 | step_time= 0.75s | loss=0.8178 (cls=0.043, par=0.752, rel=0.022) | elapsed=12.3min\n",
      "  step  940/1000 | step_time= 0.85s | loss=1.1590 (cls=0.057, par=1.054, rel=0.047) | elapsed=12.3min\n",
      "  step  945/1000 | step_time= 0.75s | loss=1.3813 (cls=0.111, par=1.218, rel=0.052) | elapsed=12.4min\n",
      "  step  950/1000 | step_time= 0.85s | loss=1.1577 (cls=0.085, par=1.041, rel=0.032) | elapsed=12.5min\n",
      "  step  955/1000 | step_time= 0.87s | loss=1.0748 (cls=0.069, par=0.953, rel=0.053) | elapsed=12.5min\n",
      "  step  960/1000 | step_time= 0.88s | loss=1.3125 (cls=0.082, par=1.172, rel=0.059) | elapsed=12.6min\n",
      "  step  965/1000 | step_time= 0.73s | loss=1.1803 (cls=0.057, par=1.083, rel=0.040) | elapsed=12.7min\n",
      "  step  970/1000 | step_time= 0.86s | loss=0.8933 (cls=0.041, par=0.827, rel=0.025) | elapsed=12.7min\n",
      "  step  975/1000 | step_time= 0.68s | loss=0.8616 (cls=0.028, par=0.813, rel=0.021) | elapsed=12.8min\n",
      "  step  980/1000 | step_time= 0.66s | loss=1.0959 (cls=0.090, par=0.973, rel=0.033) | elapsed=12.8min\n",
      "  step  985/1000 | step_time= 0.89s | loss=0.8044 (cls=0.069, par=0.703, rel=0.032) | elapsed=12.9min\n",
      "  step  990/1000 | step_time= 0.65s | loss=1.1890 (cls=0.060, par=1.091, rel=0.038) | elapsed=13.0min\n",
      "  step  995/1000 | step_time= 0.49s | loss=0.8788 (cls=0.076, par=0.779, rel=0.023) | elapsed=13.0min\n",
      "  step 1000/1000 | step_time= 0.88s | loss=1.0717 (cls=0.040, par=1.004, rel=0.028) | elapsed=13.1min\n",
      "[Train] epoch done in 13.11 min\n",
      "[ablation_softmask_off] ep=1 train={'loss': 1.5380560497045517, 'loss_cls': 0.07977738560177386, 'loss_par': 1.4086141094565392, 'loss_rel': 0.049664553590118884} test={'loss': 0.9628892549276352, 'loss_cls': 0.0551045241355896, 'loss_par': 0.8793782666921616, 'loss_rel': 0.028406465442851185}\n",
      "[ablation_softmask_off] saved -> ablation_runs\\ablation_softmask_off\\best.pt\n",
      "\n",
      "[Train] start epoch, num_docs = 1000\n",
      "  step    1/1000 | step_time= 0.68s | loss=1.1164 (cls=0.101, par=0.980, rel=0.035) | elapsed=0.0min\n",
      "    sanity: seq_len=512, avg_parent_candidates=256.5\n",
      "    sanity: seq_len=512, avg_parent_candidates=256.5\n",
      "  step    5/1000 | step_time= 0.70s | loss=0.8053 (cls=0.055, par=0.729, rel=0.022) | elapsed=0.1min\n",
      "  step   10/1000 | step_time= 0.67s | loss=1.2006 (cls=0.057, par=1.114, rel=0.030) | elapsed=0.1min\n",
      "  step   15/1000 | step_time= 0.68s | loss=0.7637 (cls=0.052, par=0.694, rel=0.018) | elapsed=0.2min\n",
      "  step   20/1000 | step_time= 0.59s | loss=1.5657 (cls=0.097, par=1.392, rel=0.077) | elapsed=0.2min\n",
      "  step   25/1000 | step_time= 0.67s | loss=1.0192 (cls=0.077, par=0.906, rel=0.037) | elapsed=0.3min\n",
      "  step   30/1000 | step_time= 0.70s | loss=0.5175 (cls=0.028, par=0.475, rel=0.014) | elapsed=0.3min\n",
      "  step   35/1000 | step_time= 0.53s | loss=0.9013 (cls=0.065, par=0.809, rel=0.027) | elapsed=0.4min\n",
      "  step   40/1000 | step_time= 0.67s | loss=1.4607 (cls=0.053, par=1.385, rel=0.023) | elapsed=0.4min\n",
      "  step   45/1000 | step_time= 0.47s | loss=0.7214 (cls=0.069, par=0.627, rel=0.025) | elapsed=0.5min\n",
      "  step   50/1000 | step_time= 0.69s | loss=0.6287 (cls=0.029, par=0.592, rel=0.008) | elapsed=0.5min\n",
      "  step   55/1000 | step_time= 0.69s | loss=1.0983 (cls=0.059, par=1.010, rel=0.029) | elapsed=0.6min\n",
      "  step   60/1000 | step_time= 0.74s | loss=0.8204 (cls=0.043, par=0.751, rel=0.027) | elapsed=0.6min\n",
      "  step   65/1000 | step_time= 0.80s | loss=0.7303 (cls=0.025, par=0.691, rel=0.015) | elapsed=0.7min\n",
      "  step   70/1000 | step_time= 0.71s | loss=1.2121 (cls=0.067, par=1.088, rel=0.058) | elapsed=0.7min\n",
      "  step   75/1000 | step_time= 0.72s | loss=0.7948 (cls=0.055, par=0.709, rel=0.030) | elapsed=0.8min\n",
      "  step   80/1000 | step_time= 0.70s | loss=0.8085 (cls=0.047, par=0.741, rel=0.020) | elapsed=0.8min\n",
      "  step   85/1000 | step_time= 0.70s | loss=0.6640 (cls=0.043, par=0.607, rel=0.015) | elapsed=0.9min\n",
      "  step   90/1000 | step_time= 0.72s | loss=1.0095 (cls=0.053, par=0.927, rel=0.029) | elapsed=0.9min\n",
      "  step   95/1000 | step_time= 0.69s | loss=1.0048 (cls=0.066, par=0.903, rel=0.036) | elapsed=1.0min\n",
      "  step  100/1000 | step_time= 0.70s | loss=1.0596 (cls=0.068, par=0.956, rel=0.035) | elapsed=1.0min\n",
      "  step  105/1000 | step_time= 0.73s | loss=1.1481 (cls=0.076, par=1.041, rel=0.031) | elapsed=1.1min\n",
      "  step  110/1000 | step_time= 0.49s | loss=0.9120 (cls=0.049, par=0.839, rel=0.025) | elapsed=1.2min\n",
      "  step  115/1000 | step_time= 0.30s | loss=1.4070 (cls=0.084, par=1.249, rel=0.075) | elapsed=1.2min\n",
      "  step  120/1000 | step_time= 0.73s | loss=0.7604 (cls=0.033, par=0.712, rel=0.015) | elapsed=1.3min\n",
      "  step  125/1000 | step_time= 0.51s | loss=1.0173 (cls=0.047, par=0.942, rel=0.029) | elapsed=1.3min\n",
      "  step  130/1000 | step_time= 1.66s | loss=1.1006 (cls=0.078, par=0.987, rel=0.036) | elapsed=1.4min\n",
      "  step  135/1000 | step_time= 0.71s | loss=0.8405 (cls=0.058, par=0.766, rel=0.017) | elapsed=1.4min\n",
      "  step  140/1000 | step_time= 0.75s | loss=0.7554 (cls=0.037, par=0.700, rel=0.019) | elapsed=1.5min\n",
      "  step  145/1000 | step_time= 0.75s | loss=0.6490 (cls=0.036, par=0.597, rel=0.016) | elapsed=1.5min\n",
      "  step  150/1000 | step_time= 0.39s | loss=1.3173 (cls=0.071, par=1.168, rel=0.079) | elapsed=1.6min\n",
      "  step  155/1000 | step_time= 0.93s | loss=0.9251 (cls=0.060, par=0.823, rel=0.041) | elapsed=1.7min\n",
      "  step  160/1000 | step_time= 0.70s | loss=0.6885 (cls=0.047, par=0.623, rel=0.018) | elapsed=1.7min\n",
      "  step  165/1000 | step_time= 0.75s | loss=1.0075 (cls=0.055, par=0.924, rel=0.028) | elapsed=1.8min\n",
      "  step  170/1000 | step_time= 0.87s | loss=0.9382 (cls=0.054, par=0.855, rel=0.029) | elapsed=1.9min\n",
      "  step  175/1000 | step_time= 0.85s | loss=0.5715 (cls=0.051, par=0.504, rel=0.017) | elapsed=1.9min\n",
      "  step  180/1000 | step_time= 0.39s | loss=1.1467 (cls=0.074, par=1.026, rel=0.047) | elapsed=2.0min\n",
      "  step  185/1000 | step_time= 0.32s | loss=0.6622 (cls=0.041, par=0.604, rel=0.018) | elapsed=2.0min\n",
      "  step  190/1000 | step_time= 0.76s | loss=0.9094 (cls=0.058, par=0.823, rel=0.028) | elapsed=2.1min\n",
      "  step  195/1000 | step_time= 0.87s | loss=0.6742 (cls=0.054, par=0.597, rel=0.023) | elapsed=2.1min\n",
      "  step  200/1000 | step_time= 0.78s | loss=0.9903 (cls=0.053, par=0.903, rel=0.034) | elapsed=2.2min\n",
      "  step  205/1000 | step_time= 0.80s | loss=0.6934 (cls=0.035, par=0.640, rel=0.018) | elapsed=2.3min\n",
      "  step  210/1000 | step_time= 0.80s | loss=0.5565 (cls=0.021, par=0.523, rel=0.012) | elapsed=2.3min\n",
      "  step  215/1000 | step_time= 0.67s | loss=0.7521 (cls=0.038, par=0.703, rel=0.011) | elapsed=2.4min\n",
      "  step  220/1000 | step_time= 0.98s | loss=0.9019 (cls=0.055, par=0.819, rel=0.028) | elapsed=2.4min\n",
      "  step  225/1000 | step_time= 0.71s | loss=0.7703 (cls=0.024, par=0.734, rel=0.012) | elapsed=2.5min\n",
      "  step  230/1000 | step_time= 0.71s | loss=0.7611 (cls=0.037, par=0.700, rel=0.024) | elapsed=2.5min\n",
      "  step  235/1000 | step_time= 0.71s | loss=0.9830 (cls=0.059, par=0.890, rel=0.034) | elapsed=2.6min\n",
      "  step  240/1000 | step_time= 0.70s | loss=0.6931 (cls=0.044, par=0.634, rel=0.015) | elapsed=2.6min\n",
      "  step  245/1000 | step_time= 0.81s | loss=0.7485 (cls=0.049, par=0.673, rel=0.027) | elapsed=2.7min\n",
      "  step  250/1000 | step_time= 0.58s | loss=1.0155 (cls=0.087, par=0.871, rel=0.058) | elapsed=2.8min\n",
      "  step  255/1000 | step_time= 0.67s | loss=0.8855 (cls=0.102, par=0.756, rel=0.027) | elapsed=2.8min\n",
      "  step  260/1000 | step_time= 0.71s | loss=0.7530 (cls=0.050, par=0.686, rel=0.018) | elapsed=2.9min\n",
      "  step  265/1000 | step_time= 0.26s | loss=0.7608 (cls=0.079, par=0.656, rel=0.027) | elapsed=2.9min\n",
      "  step  270/1000 | step_time= 0.72s | loss=1.2239 (cls=0.096, par=1.090, rel=0.038) | elapsed=3.0min\n",
      "  step  275/1000 | step_time= 0.53s | loss=0.9123 (cls=0.091, par=0.796, rel=0.026) | elapsed=3.0min\n",
      "  step  280/1000 | step_time= 0.36s | loss=0.4546 (cls=0.060, par=0.373, rel=0.022) | elapsed=3.1min\n",
      "  step  285/1000 | step_time= 0.68s | loss=0.3461 (cls=0.022, par=0.315, rel=0.009) | elapsed=3.2min\n",
      "  step  290/1000 | step_time= 0.71s | loss=0.7630 (cls=0.063, par=0.669, rel=0.031) | elapsed=3.2min\n",
      "  step  295/1000 | step_time= 0.38s | loss=0.9613 (cls=0.052, par=0.886, rel=0.023) | elapsed=3.3min\n",
      "  step  300/1000 | step_time= 0.74s | loss=1.1019 (cls=0.070, par=1.003, rel=0.029) | elapsed=3.3min\n",
      "  step  305/1000 | step_time= 0.71s | loss=0.9260 (cls=0.055, par=0.848, rel=0.024) | elapsed=3.4min\n",
      "  step  310/1000 | step_time= 0.71s | loss=0.7811 (cls=0.041, par=0.722, rel=0.019) | elapsed=3.4min\n",
      "  step  315/1000 | step_time= 0.70s | loss=1.1175 (cls=0.087, par=0.994, rel=0.037) | elapsed=3.5min\n",
      "  step  320/1000 | step_time= 0.67s | loss=1.4615 (cls=0.080, par=1.325, rel=0.057) | elapsed=3.5min\n",
      "  step  325/1000 | step_time= 0.63s | loss=1.2762 (cls=0.073, par=1.138, rel=0.065) | elapsed=3.6min\n",
      "  step  330/1000 | step_time= 0.72s | loss=0.7745 (cls=0.028, par=0.732, rel=0.014) | elapsed=3.6min\n",
      "  step  335/1000 | step_time= 0.41s | loss=0.8297 (cls=0.053, par=0.748, rel=0.028) | elapsed=3.7min\n",
      "  step  340/1000 | step_time= 0.67s | loss=0.3960 (cls=0.041, par=0.343, rel=0.012) | elapsed=3.7min\n",
      "  step  345/1000 | step_time= 0.73s | loss=0.5862 (cls=0.060, par=0.515, rel=0.011) | elapsed=3.8min\n",
      "  step  350/1000 | step_time= 0.57s | loss=0.4360 (cls=0.043, par=0.379, rel=0.014) | elapsed=3.8min\n",
      "  step  355/1000 | step_time= 0.69s | loss=0.8577 (cls=0.058, par=0.778, rel=0.021) | elapsed=3.9min\n",
      "  step  360/1000 | step_time= 0.52s | loss=0.5689 (cls=0.052, par=0.500, rel=0.017) | elapsed=3.9min\n",
      "  step  365/1000 | step_time= 0.71s | loss=0.9437 (cls=0.044, par=0.871, rel=0.028) | elapsed=4.0min\n",
      "  step  370/1000 | step_time= 0.39s | loss=1.1214 (cls=0.105, par=0.971, rel=0.045) | elapsed=4.0min\n",
      "  step  375/1000 | step_time= 0.59s | loss=1.4221 (cls=0.676, par=0.727, rel=0.019) | elapsed=4.1min\n",
      "  step  380/1000 | step_time= 0.73s | loss=0.9516 (cls=0.056, par=0.870, rel=0.026) | elapsed=4.2min\n",
      "  step  385/1000 | step_time= 0.43s | loss=0.6375 (cls=0.037, par=0.576, rel=0.024) | elapsed=4.2min\n",
      "  step  390/1000 | step_time= 0.71s | loss=0.5772 (cls=0.049, par=0.505, rel=0.024) | elapsed=4.3min\n",
      "  step  395/1000 | step_time= 0.69s | loss=1.0326 (cls=0.065, par=0.926, rel=0.042) | elapsed=4.3min\n",
      "  step  400/1000 | step_time= 0.35s | loss=1.0964 (cls=0.128, par=0.931, rel=0.037) | elapsed=4.3min\n",
      "  step  405/1000 | step_time= 0.64s | loss=0.4852 (cls=0.027, par=0.448, rel=0.010) | elapsed=4.4min\n",
      "  step  410/1000 | step_time= 0.55s | loss=1.1261 (cls=0.042, par=1.048, rel=0.036) | elapsed=4.4min\n",
      "  step  415/1000 | step_time= 0.36s | loss=1.1892 (cls=0.043, par=1.079, rel=0.067) | elapsed=4.5min\n",
      "  step  420/1000 | step_time= 0.42s | loss=0.3215 (cls=0.032, par=0.279, rel=0.011) | elapsed=4.6min\n",
      "  step  425/1000 | step_time= 0.43s | loss=1.0685 (cls=0.063, par=0.975, rel=0.030) | elapsed=4.6min\n",
      "  step  430/1000 | step_time= 0.71s | loss=1.1506 (cls=0.052, par=1.060, rel=0.039) | elapsed=4.6min\n",
      "  step  435/1000 | step_time= 0.72s | loss=1.1823 (cls=0.065, par=1.092, rel=0.026) | elapsed=4.7min\n",
      "  step  440/1000 | step_time= 0.69s | loss=0.9210 (cls=0.054, par=0.839, rel=0.028) | elapsed=4.8min\n",
      "  step  445/1000 | step_time= 0.70s | loss=1.2252 (cls=0.084, par=1.101, rel=0.040) | elapsed=4.8min\n",
      "  step  450/1000 | step_time= 0.71s | loss=0.8167 (cls=0.039, par=0.756, rel=0.022) | elapsed=4.9min\n",
      "  step  455/1000 | step_time= 0.71s | loss=1.4844 (cls=0.072, par=1.373, rel=0.039) | elapsed=4.9min\n",
      "  step  460/1000 | step_time= 0.27s | loss=1.0730 (cls=0.081, par=0.946, rel=0.046) | elapsed=5.0min\n",
      "  step  465/1000 | step_time= 0.70s | loss=1.3432 (cls=0.083, par=1.215, rel=0.045) | elapsed=5.0min\n",
      "  step  470/1000 | step_time= 0.72s | loss=1.2750 (cls=0.102, par=1.133, rel=0.040) | elapsed=5.1min\n",
      "  step  475/1000 | step_time= 0.70s | loss=0.7626 (cls=0.046, par=0.698, rel=0.019) | elapsed=5.2min\n",
      "  step  480/1000 | step_time= 0.70s | loss=1.1579 (cls=0.041, par=1.085, rel=0.032) | elapsed=5.2min\n",
      "  step  485/1000 | step_time= 0.73s | loss=0.9851 (cls=0.039, par=0.919, rel=0.027) | elapsed=5.3min\n",
      "  step  490/1000 | step_time= 0.46s | loss=1.2289 (cls=0.055, par=1.135, rel=0.039) | elapsed=5.3min\n",
      "  step  495/1000 | step_time= 0.49s | loss=1.0836 (cls=0.055, par=0.984, rel=0.045) | elapsed=5.4min\n",
      "  step  500/1000 | step_time= 0.70s | loss=0.7072 (cls=0.035, par=0.653, rel=0.019) | elapsed=5.4min\n",
      "  step  505/1000 | step_time= 0.72s | loss=0.8954 (cls=0.041, par=0.832, rel=0.022) | elapsed=5.5min\n",
      "  step  510/1000 | step_time= 0.70s | loss=0.8913 (cls=0.057, par=0.816, rel=0.019) | elapsed=5.5min\n",
      "  step  515/1000 | step_time= 0.75s | loss=0.5371 (cls=0.021, par=0.507, rel=0.009) | elapsed=5.6min\n",
      "  step  520/1000 | step_time= 0.55s | loss=1.1730 (cls=0.070, par=1.058, rel=0.045) | elapsed=5.6min\n",
      "  step  525/1000 | step_time= 0.66s | loss=0.7498 (cls=0.050, par=0.682, rel=0.018) | elapsed=5.7min\n",
      "  step  530/1000 | step_time= 0.75s | loss=0.7334 (cls=0.032, par=0.685, rel=0.016) | elapsed=5.8min\n",
      "  step  535/1000 | step_time= 0.50s | loss=0.3936 (cls=0.058, par=0.327, rel=0.009) | elapsed=5.8min\n",
      "  step  540/1000 | step_time= 0.69s | loss=0.7215 (cls=0.041, par=0.667, rel=0.014) | elapsed=5.9min\n",
      "  step  545/1000 | step_time= 0.71s | loss=0.9738 (cls=0.029, par=0.924, rel=0.022) | elapsed=5.9min\n",
      "  step  550/1000 | step_time= 0.33s | loss=1.2662 (cls=0.088, par=1.133, rel=0.046) | elapsed=6.0min\n",
      "  step  555/1000 | step_time= 0.72s | loss=1.1700 (cls=0.066, par=1.047, rel=0.057) | elapsed=6.0min\n",
      "  step  560/1000 | step_time= 0.49s | loss=1.1260 (cls=0.074, par=1.016, rel=0.036) | elapsed=6.1min\n",
      "  step  565/1000 | step_time= 0.68s | loss=0.8990 (cls=0.049, par=0.825, rel=0.025) | elapsed=6.1min\n",
      "  step  570/1000 | step_time= 0.38s | loss=0.9223 (cls=0.063, par=0.825, rel=0.034) | elapsed=6.2min\n",
      "  step  575/1000 | step_time= 0.72s | loss=0.8437 (cls=0.066, par=0.755, rel=0.023) | elapsed=6.2min\n",
      "  step  580/1000 | step_time= 0.54s | loss=0.7455 (cls=0.051, par=0.682, rel=0.013) | elapsed=6.3min\n",
      "  step  585/1000 | step_time= 0.54s | loss=0.7216 (cls=0.055, par=0.648, rel=0.019) | elapsed=6.3min\n",
      "  step  590/1000 | step_time= 0.66s | loss=0.5643 (cls=0.034, par=0.515, rel=0.015) | elapsed=6.4min\n",
      "  step  595/1000 | step_time= 0.71s | loss=0.9463 (cls=0.042, par=0.884, rel=0.020) | elapsed=6.4min\n",
      "  step  600/1000 | step_time= 0.70s | loss=0.7492 (cls=0.029, par=0.701, rel=0.019) | elapsed=6.5min\n",
      "  step  605/1000 | step_time= 0.40s | loss=0.7729 (cls=0.076, par=0.675, rel=0.022) | elapsed=6.5min\n",
      "  step  610/1000 | step_time= 0.44s | loss=0.6058 (cls=0.102, par=0.492, rel=0.012) | elapsed=6.6min\n",
      "  step  615/1000 | step_time= 0.61s | loss=0.7598 (cls=0.046, par=0.688, rel=0.026) | elapsed=6.7min\n",
      "  step  620/1000 | step_time= 0.73s | loss=0.7624 (cls=0.025, par=0.726, rel=0.011) | elapsed=6.7min\n",
      "  step  625/1000 | step_time= 0.49s | loss=1.0548 (cls=0.077, par=0.936, rel=0.042) | elapsed=6.8min\n",
      "  step  630/1000 | step_time= 0.71s | loss=0.6248 (cls=0.056, par=0.548, rel=0.021) | elapsed=6.8min\n",
      "  step  635/1000 | step_time= 0.67s | loss=1.0154 (cls=0.071, par=0.917, rel=0.028) | elapsed=6.9min\n",
      "  step  640/1000 | step_time= 0.69s | loss=0.6726 (cls=0.043, par=0.617, rel=0.012) | elapsed=6.9min\n",
      "  step  645/1000 | step_time= 0.73s | loss=0.6194 (cls=0.029, par=0.575, rel=0.015) | elapsed=7.0min\n",
      "  step  650/1000 | step_time= 0.75s | loss=1.1331 (cls=0.065, par=1.020, rel=0.049) | elapsed=7.0min\n",
      "  step  655/1000 | step_time= 0.71s | loss=0.8918 (cls=0.035, par=0.841, rel=0.015) | elapsed=7.1min\n",
      "  step  660/1000 | step_time= 0.71s | loss=0.4914 (cls=0.025, par=0.458, rel=0.009) | elapsed=7.1min\n",
      "  step  665/1000 | step_time= 0.69s | loss=1.2198 (cls=0.095, par=1.069, rel=0.055) | elapsed=7.2min\n",
      "  step  670/1000 | step_time= 0.71s | loss=1.1553 (cls=0.077, par=1.039, rel=0.040) | elapsed=7.3min\n",
      "  step  675/1000 | step_time= 0.73s | loss=0.9036 (cls=0.050, par=0.830, rel=0.024) | elapsed=7.3min\n",
      "  step  680/1000 | step_time= 0.70s | loss=0.6063 (cls=0.041, par=0.553, rel=0.012) | elapsed=7.4min\n",
      "  step  685/1000 | step_time= 0.72s | loss=0.9856 (cls=0.065, par=0.896, rel=0.025) | elapsed=7.4min\n",
      "  step  690/1000 | step_time= 0.70s | loss=0.9272 (cls=0.062, par=0.833, rel=0.033) | elapsed=7.5min\n",
      "  step  695/1000 | step_time= 0.62s | loss=0.6070 (cls=0.052, par=0.528, rel=0.028) | elapsed=7.5min\n",
      "  step  700/1000 | step_time= 0.62s | loss=0.7913 (cls=0.074, par=0.689, rel=0.028) | elapsed=7.6min\n",
      "  step  705/1000 | step_time= 0.70s | loss=0.6389 (cls=0.053, par=0.570, rel=0.016) | elapsed=7.6min\n",
      "  step  710/1000 | step_time= 0.43s | loss=0.5811 (cls=0.046, par=0.507, rel=0.028) | elapsed=7.7min\n",
      "  step  715/1000 | step_time= 0.88s | loss=0.6623 (cls=0.024, par=0.625, rel=0.013) | elapsed=7.8min\n",
      "  step  720/1000 | step_time= 0.88s | loss=0.8566 (cls=0.053, par=0.784, rel=0.020) | elapsed=7.8min\n",
      "  step  725/1000 | step_time= 0.70s | loss=1.0970 (cls=0.074, par=0.993, rel=0.030) | elapsed=7.9min\n",
      "  step  730/1000 | step_time= 0.70s | loss=1.1243 (cls=0.054, par=1.046, rel=0.025) | elapsed=8.0min\n",
      "  step  735/1000 | step_time= 0.71s | loss=1.2958 (cls=0.095, par=1.144, rel=0.057) | elapsed=8.0min\n",
      "  step  740/1000 | step_time= 0.25s | loss=0.7821 (cls=0.086, par=0.674, rel=0.022) | elapsed=8.1min\n",
      "  step  745/1000 | step_time= 0.70s | loss=0.9955 (cls=0.079, par=0.889, rel=0.027) | elapsed=8.1min\n",
      "  step  750/1000 | step_time= 0.24s | loss=0.6977 (cls=0.054, par=0.616, rel=0.028) | elapsed=8.2min\n",
      "  step  755/1000 | step_time= 0.49s | loss=0.9133 (cls=0.072, par=0.811, rel=0.030) | elapsed=8.2min\n",
      "  step  760/1000 | step_time= 0.70s | loss=0.9812 (cls=0.060, par=0.886, rel=0.035) | elapsed=8.3min\n",
      "  step  765/1000 | step_time= 0.57s | loss=1.0264 (cls=0.089, par=0.904, rel=0.034) | elapsed=8.3min\n",
      "  step  770/1000 | step_time= 0.40s | loss=0.5121 (cls=0.058, par=0.434, rel=0.021) | elapsed=8.4min\n",
      "  step  775/1000 | step_time= 0.70s | loss=0.5501 (cls=0.053, par=0.479, rel=0.017) | elapsed=8.4min\n",
      "  step  780/1000 | step_time= 0.72s | loss=0.3823 (cls=0.015, par=0.362, rel=0.006) | elapsed=8.5min\n",
      "  step  785/1000 | step_time= 0.69s | loss=1.0584 (cls=0.072, par=0.957, rel=0.029) | elapsed=8.6min\n",
      "  step  790/1000 | step_time= 0.69s | loss=0.6939 (cls=0.055, par=0.625, rel=0.014) | elapsed=8.6min\n",
      "  step  795/1000 | step_time= 0.17s | loss=0.7942 (cls=0.085, par=0.675, rel=0.035) | elapsed=8.7min\n",
      "  step  800/1000 | step_time= 0.69s | loss=0.8215 (cls=0.055, par=0.733, rel=0.033) | elapsed=8.7min\n",
      "  step  805/1000 | step_time= 0.61s | loss=1.3056 (cls=0.084, par=1.180, rel=0.042) | elapsed=8.8min\n",
      "  step  810/1000 | step_time= 0.69s | loss=0.7661 (cls=0.042, par=0.708, rel=0.017) | elapsed=8.8min\n",
      "  step  815/1000 | step_time= 0.71s | loss=0.9675 (cls=0.039, par=0.908, rel=0.021) | elapsed=8.9min\n",
      "  step  820/1000 | step_time= 0.70s | loss=0.5455 (cls=0.028, par=0.508, rel=0.009) | elapsed=8.9min\n",
      "  step  825/1000 | step_time= 0.58s | loss=0.6757 (cls=0.039, par=0.620, rel=0.017) | elapsed=9.0min\n",
      "  step  830/1000 | step_time= 0.71s | loss=0.8713 (cls=0.060, par=0.792, rel=0.020) | elapsed=9.0min\n",
      "  step  835/1000 | step_time= 0.61s | loss=1.0988 (cls=0.088, par=0.983, rel=0.028) | elapsed=9.1min\n",
      "  step  840/1000 | step_time= 0.70s | loss=0.8682 (cls=0.047, par=0.802, rel=0.018) | elapsed=9.1min\n",
      "  step  845/1000 | step_time= 0.68s | loss=0.9764 (cls=0.086, par=0.866, rel=0.024) | elapsed=9.2min\n",
      "  step  850/1000 | step_time= 0.69s | loss=0.9614 (cls=0.078, par=0.860, rel=0.023) | elapsed=9.3min\n",
      "  step  855/1000 | step_time= 0.71s | loss=0.7210 (cls=0.047, par=0.654, rel=0.021) | elapsed=9.3min\n",
      "  step  860/1000 | step_time= 0.71s | loss=0.9283 (cls=0.053, par=0.859, rel=0.016) | elapsed=9.4min\n",
      "  step  865/1000 | step_time= 0.69s | loss=0.5775 (cls=0.052, par=0.517, rel=0.009) | elapsed=9.4min\n",
      "  step  870/1000 | step_time= 0.70s | loss=1.1515 (cls=0.081, par=1.026, rel=0.044) | elapsed=9.5min\n",
      "  step  875/1000 | step_time= 0.68s | loss=0.6560 (cls=0.057, par=0.581, rel=0.018) | elapsed=9.5min\n",
      "  step  880/1000 | step_time= 0.70s | loss=0.4985 (cls=0.028, par=0.463, rel=0.008) | elapsed=9.6min\n",
      "  step  885/1000 | step_time= 0.32s | loss=1.0834 (cls=0.078, par=0.965, rel=0.040) | elapsed=9.7min\n",
      "  step  890/1000 | step_time= 0.74s | loss=1.5637 (cls=0.083, par=1.406, rel=0.075) | elapsed=9.7min\n",
      "  step  895/1000 | step_time= 0.72s | loss=0.9821 (cls=0.055, par=0.892, rel=0.036) | elapsed=9.7min\n",
      "  step  900/1000 | step_time= 0.70s | loss=0.6368 (cls=0.026, par=0.600, rel=0.011) | elapsed=9.8min\n",
      "  step  905/1000 | step_time= 0.52s | loss=0.5322 (cls=0.057, par=0.459, rel=0.016) | elapsed=9.9min\n",
      "  step  910/1000 | step_time= 0.71s | loss=0.6971 (cls=0.039, par=0.644, rel=0.014) | elapsed=9.9min\n",
      "  step  915/1000 | step_time= 0.23s | loss=0.5213 (cls=0.049, par=0.459, rel=0.014) | elapsed=10.0min\n",
      "  step  920/1000 | step_time= 0.71s | loss=0.7885 (cls=0.035, par=0.739, rel=0.015) | elapsed=10.0min\n",
      "  step  925/1000 | step_time= 0.42s | loss=0.7345 (cls=0.048, par=0.666, rel=0.021) | elapsed=10.1min\n",
      "  step  930/1000 | step_time= 0.70s | loss=0.7904 (cls=0.044, par=0.718, rel=0.029) | elapsed=10.1min\n",
      "  step  935/1000 | step_time= 0.54s | loss=0.7498 (cls=0.082, par=0.589, rel=0.079) | elapsed=10.2min\n",
      "  step  940/1000 | step_time= 0.70s | loss=0.7688 (cls=0.029, par=0.723, rel=0.017) | elapsed=10.2min\n",
      "  step  945/1000 | step_time= 0.46s | loss=0.5841 (cls=0.030, par=0.540, rel=0.015) | elapsed=10.3min\n",
      "  step  950/1000 | step_time= 0.25s | loss=0.8721 (cls=0.065, par=0.782, rel=0.025) | elapsed=10.3min\n",
      "  step  955/1000 | step_time= 0.61s | loss=0.7131 (cls=0.044, par=0.650, rel=0.019) | elapsed=10.4min\n",
      "  step  960/1000 | step_time= 0.51s | loss=0.4591 (cls=0.032, par=0.410, rel=0.017) | elapsed=10.4min\n",
      "  step  965/1000 | step_time= 0.68s | loss=0.8137 (cls=0.070, par=0.729, rel=0.014) | elapsed=10.5min\n",
      "  step  970/1000 | step_time= 0.62s | loss=0.7999 (cls=0.047, par=0.726, rel=0.026) | elapsed=10.5min\n",
      "  step  975/1000 | step_time= 0.71s | loss=1.3688 (cls=0.050, par=1.283, rel=0.036) | elapsed=10.6min\n",
      "  step  980/1000 | step_time= 0.69s | loss=0.7530 (cls=0.032, par=0.706, rel=0.015) | elapsed=10.6min\n",
      "  step  985/1000 | step_time= 0.36s | loss=1.1448 (cls=0.069, par=1.019, rel=0.057) | elapsed=10.7min\n",
      "  step  990/1000 | step_time= 0.64s | loss=0.6843 (cls=0.043, par=0.628, rel=0.013) | elapsed=10.7min\n",
      "  step  995/1000 | step_time= 0.70s | loss=0.4982 (cls=0.020, par=0.468, rel=0.010) | elapsed=10.8min\n",
      "  step 1000/1000 | step_time= 0.70s | loss=0.8362 (cls=0.059, par=0.752, rel=0.025) | elapsed=10.9min\n",
      "[Train] epoch done in 10.86 min\n",
      "[ablation_softmask_off] ep=2 train={'loss': 0.8927980011999607, 'loss_cls': 0.059492411894723776, 'loss_par': 0.8052841940820217, 'loss_rel': 0.02802139636920765} test={'loss': 0.7281323254108429, 'loss_cls': 0.047371268313378095, 'loss_par': 0.6607974957823753, 'loss_rel': 0.019963560117874295}\n",
      "[ablation_softmask_off] saved -> ablation_runs\\ablation_softmask_off\\best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "export -> ablation_runs\\ablation_softmask_off\\exports_test: 100%|████████████████████| 500/500 [03:09<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "[ablation_softmask_off] export done, start eval...\n",
      "[ablation_softmask_off] eval done.\n",
      "============================================================\n",
      "num_docs                 : 500\n",
      "STEDS_strict_mean        : -0.3404573339955194\n",
      "STEDS_strict_std         : 0.24604283788231585\n",
      "STEDS_strict_median      : -0.3568057977332171\n",
      "STEDS_label_mean         : -0.0014663272754896397\n",
      "STEDS_label_std          : 0.21753994033137466\n",
      "STEDS_label_median       : 0.02823055028462995\n",
      "cls_acc_mean             : 0.8456671712425622\n",
      "parent_acc_mean          : 0.8478080657446997\n",
      "rel_acc_mean             : 0.8705111350022728\n",
      "exp_name                 : baseline_softmask_on\n",
      "use_softmask             : True\n",
      "seed                     : 42\n",
      "epochs                   : 2\n",
      "elapsed_sec              : 6517.02\n",
      "export_dir               : ablation_runs\\baseline_softmask_on\\exports_test\n",
      "============================================================\n",
      "num_docs                 : 500\n",
      "STEDS_strict_mean        : -0.32851114734823994\n",
      "STEDS_strict_std         : 0.25123337266583984\n",
      "STEDS_strict_median      : -0.34045034930139717\n",
      "STEDS_label_mean         : 0.01603230282748148\n",
      "STEDS_label_std          : 0.21788148695065962\n",
      "STEDS_label_median       : 0.044470078628937315\n",
      "cls_acc_mean             : 0.8464026688796641\n",
      "parent_acc_mean          : 0.8466357211699336\n",
      "rel_acc_mean             : 0.8703005190295653\n",
      "exp_name                 : ablation_softmask_off\n",
      "use_softmask             : False\n",
      "seed                     : 42\n",
      "epochs                   : 2\n",
      "elapsed_sec              : 6527.94\n",
      "export_dir               : ablation_runs\\ablation_softmask_off\\exports_test\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def run_experiment(exp_name: str, use_softmask: bool, train_loader, test_loader, M_cp, cfg, seed=42, save_root=\"ablation_runs\"):\n",
    "    # 固定随机种子\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    exp_dir = os.path.join(save_root, exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "    # 建模：只改变 use_softmask\n",
    "    model = DSPSModel(\n",
    "        num_classes=len(ID2LABEL_14),\n",
    "        num_rel=len(REL2ID),\n",
    "        M_cp=M_cp,\n",
    "        cfg=cfg,\n",
    "        use_text=True,\n",
    "        use_visual= False,\n",
    "        use_softmask=use_softmask,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    focal_cls = FocalLoss(gamma=cfg.focal_gamma, alpha=cfg.focal_alpha).to(device)\n",
    "    focal_rel = FocalLoss(gamma=cfg.focal_gamma, alpha=cfg.focal_alpha).to(device)\n",
    "\n",
    "    best = 1e18\n",
    "    best_path = os.path.join(exp_dir, \"best.pt\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    for ep in range(1, cfg.epochs + 1):\n",
    "        tr = train_one_epoch(model, train_loader, optimizer, cfg, focal_cls, focal_rel)\n",
    "        te = eval_one_epoch(model, test_loader, cfg, focal_cls, focal_rel)\n",
    "        print(f\"[{exp_name}] ep={ep} train={tr} test={te}\")\n",
    "\n",
    "        if te[\"loss\"] < best:\n",
    "            best = te[\"loss\"]\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"[{exp_name}] saved -> {best_path}\")\n",
    "\n",
    "    # 导出与评测\n",
    "    state = torch.load(best_path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(state)\n",
    "\n",
    "    export_dir = os.path.join(exp_dir, \"exports_test\")\n",
    "    export_split_predictions(model, test_loader, save_dir=export_dir, device=device)\n",
    "\n",
    "    print(f\"[{exp_name}] export done, start eval...\")\n",
    "    metrics = eval_export_dir_dual(export_dir, exclude_meta=True, show_progress=True)\n",
    "    metrics.update({\n",
    "        \"exp_name\": exp_name,\n",
    "        \"use_softmask\": use_softmask,\n",
    "        \"seed\": seed,\n",
    "        \"epochs\": cfg.epochs,\n",
    "        \"elapsed_sec\": round(time.time() - t0, 2),\n",
    "        \"export_dir\": export_dir,\n",
    "    })\n",
    "    print(f\"[{exp_name}] eval done.\")\n",
    "\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ======= 跑两组：soft-mask on/off =======\n",
    "os.makedirs(\"ablation_runs\", exist_ok=True)\n",
    "\n",
    "res_on = run_experiment(\n",
    "    exp_name=\"baseline_softmask_on\",\n",
    "    use_softmask=True,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    M_cp=M_cp,\n",
    "    cfg=cfg,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "res_off = run_experiment(\n",
    "    exp_name=\"ablation_softmask_off\",\n",
    "    use_softmask=False,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    M_cp=M_cp,\n",
    "    cfg=cfg,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "for r in [res_on, res_off]:\n",
    "    print(\"=\" * 60)\n",
    "    for k, v in r.items():\n",
    "        print(f\"{k:25s}: {v}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c432f7c6-59ae-48c8-9d6b-18912b959dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "cfg.batch_size = 1\n",
    "\n",
    "def _call_with_compatible_signature(fn, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Call fn with kwargs filtered to those supported by fn's signature.\n",
    "    This prevents errors like: got an unexpected keyword argument 'device'.\n",
    "    \"\"\"\n",
    "    sig = inspect.signature(fn)\n",
    "    accepted = set(sig.parameters.keys())\n",
    "    filtered = {k: v for k, v in kwargs.items() if k in accepted}\n",
    "    return fn(*args, **filtered)\n",
    "\n",
    "def train_one_epoch_compat(model, loader, optimizer, *, device=None, cfg=None, scaler=None):\n",
    "        if focal_cls is None:\n",
    "            focal_cls = FocalLoss(gamma=getattr(cfg, \"focal_gamma\", 2.0),\n",
    "                                  alpha=getattr(cfg, \"focal_alpha\", 0.25)).to(device)\n",
    "        if focal_rel is None:\n",
    "            focal_rel = FocalLoss(gamma=getattr(cfg, \"focal_gamma\", 2.0),\n",
    "                                  alpha=getattr(cfg, \"focal_alpha\", 0.25)).to(device)\n",
    "    # Try calling with keywords first; incompatible keywords are filtered out.\n",
    "        return _call_with_compatible_signature(\n",
    "        train_one_epoch,\n",
    "        model, loader, optimizer,\n",
    "        device=device, cfg=cfg, scaler=scaler\n",
    "    )\n",
    "\n",
    "def eval_one_epoch_compat(model, loader, *, device=None, cfg=None):\n",
    "    return _call_with_compatible_signature(\n",
    "        eval_one_epoch,\n",
    "        model, loader,\n",
    "        device=device, cfg=cfg\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "801c2d17-3b29-411d-8b67-d3697c48d410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] cuda\n",
      "[EXP_DIR] hrds_runs\\hrds_dsps_fullrun\n",
      "[HRDS] train docs = 900, test docs = 100\n",
      "[Dims] num_classes = 14 num_rel = 4\n",
      "[M_cp] loaded: hrds_runs\\hrds_dsps_fullrun\\M_cp_hrds.npy shape= (2, 15, 14)\n",
      "\n",
      "========== [HRDS] Epoch 1/2 ==========\n",
      "\n",
      "[Train] start epoch, num_docs = 900\n",
      "  step    1/900 | step_time= 1.01s | loss=16.9198 (cls=0.501, par=16.229, rel=0.190) | elapsed=0.0min\n",
      "    sanity: seq_len=512, avg_parent_candidates=2.0\n",
      "    sanity: seq_len=501, avg_parent_candidates=2.0\n",
      "  step    5/900 | step_time= 1.14s | loss=11.1667 (cls=1.411, par=9.604, rel=0.151) | elapsed=0.1min\n",
      "  step   10/900 | step_time= 0.92s | loss=9.8604 (cls=1.749, par=8.017, rel=0.094) | elapsed=0.2min\n",
      "  step   15/900 | step_time= 0.88s | loss=9.6233 (cls=1.643, par=7.908, rel=0.073) | elapsed=0.3min\n",
      "  step   20/900 | step_time= 1.08s | loss=9.3956 (cls=1.615, par=7.709, rel=0.071) | elapsed=0.3min\n",
      "  step   25/900 | step_time= 1.05s | loss=9.2130 (cls=1.660, par=7.486, rel=0.066) | elapsed=0.4min\n",
      "  step   30/900 | step_time= 1.06s | loss=8.9500 (cls=1.679, par=7.210, rel=0.061) | elapsed=0.5min\n",
      "  step   35/900 | step_time= 1.05s | loss=8.7475 (cls=1.623, par=7.073, rel=0.051) | elapsed=0.6min\n",
      "  step   40/900 | step_time= 1.08s | loss=8.5313 (cls=1.639, par=6.841, rel=0.052) | elapsed=0.7min\n",
      "  step   45/900 | step_time= 1.08s | loss=8.3723 (cls=1.646, par=6.679, rel=0.047) | elapsed=0.8min\n",
      "  step   50/900 | step_time= 1.04s | loss=8.3755 (cls=1.624, par=6.698, rel=0.054) | elapsed=0.9min\n",
      "  step   55/900 | step_time= 1.06s | loss=8.4316 (cls=1.639, par=6.739, rel=0.054) | elapsed=1.0min\n",
      "  step   60/900 | step_time= 1.62s | loss=8.2095 (cls=1.599, par=6.564, rel=0.047) | elapsed=1.1min\n",
      "  step   65/900 | step_time= 1.66s | loss=8.3312 (cls=1.686, par=6.598, rel=0.048) | elapsed=1.2min\n",
      "  step   70/900 | step_time= 1.42s | loss=8.2142 (cls=1.572, par=6.578, rel=0.065) | elapsed=1.3min\n",
      "  step   75/900 | step_time= 1.79s | loss=8.1818 (cls=1.726, par=6.398, rel=0.058) | elapsed=1.5min\n",
      "  step   80/900 | step_time= 0.95s | loss=8.0983 (cls=1.617, par=6.425, rel=0.056) | elapsed=1.6min\n",
      "  step   85/900 | step_time= 0.85s | loss=8.0132 (cls=1.669, par=6.285, rel=0.059) | elapsed=1.7min\n",
      "  step   90/900 | step_time= 0.79s | loss=8.0025 (cls=1.639, par=6.316, rel=0.047) | elapsed=1.7min\n",
      "  step   95/900 | step_time= 0.92s | loss=7.9896 (cls=1.692, par=6.244, rel=0.053) | elapsed=1.8min\n",
      "  step  100/900 | step_time= 0.95s | loss=7.8523 (cls=1.610, par=6.195, rel=0.047) | elapsed=1.9min\n",
      "  step  105/900 | step_time= 1.06s | loss=7.9965 (cls=1.689, par=6.257, rel=0.050) | elapsed=2.0min\n",
      "  step  110/900 | step_time= 0.89s | loss=7.8349 (cls=1.645, par=6.145, rel=0.045) | elapsed=2.1min\n",
      "  step  115/900 | step_time= 0.94s | loss=7.6250 (cls=1.635, par=5.948, rel=0.041) | elapsed=2.1min\n",
      "  step  120/900 | step_time= 0.95s | loss=7.5691 (cls=1.645, par=5.889, rel=0.035) | elapsed=2.2min\n",
      "  step  125/900 | step_time= 0.92s | loss=7.7805 (cls=1.636, par=6.100, rel=0.044) | elapsed=2.3min\n",
      "  step  130/900 | step_time= 0.91s | loss=7.6321 (cls=1.686, par=5.904, rel=0.042) | elapsed=2.4min\n",
      "  step  135/900 | step_time= 0.91s | loss=7.4878 (cls=1.590, par=5.858, rel=0.040) | elapsed=2.4min\n",
      "  step  140/900 | step_time= 0.89s | loss=7.5970 (cls=1.658, par=5.904, rel=0.035) | elapsed=2.5min\n",
      "  step  145/900 | step_time= 0.59s | loss=7.9583 (cls=1.618, par=6.282, rel=0.058) | elapsed=2.6min\n",
      "  step  150/900 | step_time= 0.90s | loss=7.5511 (cls=1.599, par=5.923, rel=0.029) | elapsed=2.7min\n",
      "  step  155/900 | step_time= 0.91s | loss=7.5590 (cls=1.659, par=5.853, rel=0.046) | elapsed=2.7min\n",
      "  step  160/900 | step_time= 0.90s | loss=7.4218 (cls=1.666, par=5.730, rel=0.026) | elapsed=2.8min\n",
      "  step  165/900 | step_time= 0.91s | loss=7.4711 (cls=1.609, par=5.827, rel=0.036) | elapsed=2.9min\n",
      "  step  170/900 | step_time= 0.91s | loss=7.3305 (cls=1.647, par=5.657, rel=0.026) | elapsed=3.0min\n",
      "  step  175/900 | step_time= 0.91s | loss=7.4103 (cls=1.640, par=5.739, rel=0.032) | elapsed=3.0min\n",
      "  step  180/900 | step_time= 0.90s | loss=7.3436 (cls=1.612, par=5.699, rel=0.033) | elapsed=3.1min\n",
      "  step  185/900 | step_time= 0.90s | loss=7.2871 (cls=1.633, par=5.633, rel=0.021) | elapsed=3.2min\n",
      "  step  190/900 | step_time= 0.89s | loss=7.4587 (cls=1.631, par=5.783, rel=0.045) | elapsed=3.3min\n",
      "  step  195/900 | step_time= 0.87s | loss=7.4694 (cls=1.629, par=5.799, rel=0.041) | elapsed=3.4min\n",
      "  step  200/900 | step_time= 0.96s | loss=7.1634 (cls=1.641, par=5.499, rel=0.023) | elapsed=3.4min\n",
      "  step  205/900 | step_time= 0.91s | loss=7.2878 (cls=1.594, par=5.653, rel=0.040) | elapsed=3.5min\n",
      "  step  210/900 | step_time= 0.58s | loss=7.3721 (cls=1.709, par=5.616, rel=0.047) | elapsed=3.6min\n",
      "  step  215/900 | step_time= 0.90s | loss=7.0932 (cls=1.616, par=5.455, rel=0.022) | elapsed=3.7min\n",
      "  step  220/900 | step_time= 0.90s | loss=7.2467 (cls=1.600, par=5.622, rel=0.024) | elapsed=3.7min\n",
      "  step  225/900 | step_time= 0.90s | loss=7.3386 (cls=1.639, par=5.671, rel=0.029) | elapsed=3.8min\n",
      "  step  230/900 | step_time= 0.91s | loss=7.4645 (cls=1.628, par=5.800, rel=0.037) | elapsed=3.9min\n",
      "  step  235/900 | step_time= 0.91s | loss=7.6842 (cls=1.631, par=6.013, rel=0.040) | elapsed=4.0min\n",
      "  step  240/900 | step_time= 0.90s | loss=7.3140 (cls=1.611, par=5.679, rel=0.024) | elapsed=4.0min\n",
      "  step  245/900 | step_time= 0.66s | loss=7.5444 (cls=1.637, par=5.864, rel=0.043) | elapsed=4.1min\n",
      "  step  250/900 | step_time= 0.89s | loss=7.2434 (cls=1.600, par=5.619, rel=0.024) | elapsed=4.2min\n",
      "  step  255/900 | step_time= 0.89s | loss=7.2016 (cls=1.626, par=5.547, rel=0.028) | elapsed=4.2min\n",
      "  step  260/900 | step_time= 0.89s | loss=7.1030 (cls=1.569, par=5.514, rel=0.020) | elapsed=4.3min\n",
      "  step  265/900 | step_time= 0.56s | loss=7.1663 (cls=1.632, par=5.496, rel=0.039) | elapsed=4.4min\n",
      "  step  270/900 | step_time= 0.88s | loss=7.2319 (cls=1.648, par=5.559, rel=0.025) | elapsed=4.5min\n",
      "  step  275/900 | step_time= 0.84s | loss=7.1106 (cls=1.578, par=5.505, rel=0.027) | elapsed=4.5min\n",
      "  step  280/900 | step_time= 0.90s | loss=7.3655 (cls=1.611, par=5.722, rel=0.033) | elapsed=4.6min\n",
      "  step  285/900 | step_time= 0.90s | loss=7.0893 (cls=1.628, par=5.442, rel=0.019) | elapsed=4.7min\n",
      "  step  290/900 | step_time= 0.90s | loss=7.0125 (cls=1.623, par=5.365, rel=0.025) | elapsed=4.8min\n",
      "  step  295/900 | step_time= 0.90s | loss=7.0396 (cls=1.683, par=5.340, rel=0.016) | elapsed=4.8min\n",
      "  step  300/900 | step_time= 0.89s | loss=6.9591 (cls=1.583, par=5.357, rel=0.019) | elapsed=4.9min\n",
      "  step  305/900 | step_time= 0.89s | loss=7.1459 (cls=1.677, par=5.436, rel=0.033) | elapsed=5.0min\n",
      "  step  310/900 | step_time= 0.88s | loss=6.9043 (cls=1.606, par=5.279, rel=0.019) | elapsed=5.1min\n",
      "  step  315/900 | step_time= 0.87s | loss=7.1552 (cls=1.670, par=5.458, rel=0.027) | elapsed=5.1min\n",
      "  step  320/900 | step_time= 0.88s | loss=6.8708 (cls=1.585, par=5.268, rel=0.018) | elapsed=5.2min\n",
      "  step  325/900 | step_time= 0.88s | loss=7.2203 (cls=1.621, par=5.560, rel=0.039) | elapsed=5.3min\n",
      "  step  330/900 | step_time= 0.87s | loss=6.9716 (cls=1.650, par=5.299, rel=0.023) | elapsed=5.4min\n",
      "  step  335/900 | step_time= 0.90s | loss=6.8277 (cls=1.591, par=5.222, rel=0.014) | elapsed=5.4min\n",
      "  step  340/900 | step_time= 0.93s | loss=7.0929 (cls=1.598, par=5.472, rel=0.022) | elapsed=5.5min\n",
      "  step  345/900 | step_time= 0.89s | loss=7.3456 (cls=1.646, par=5.664, rel=0.036) | elapsed=5.6min\n",
      "  step  350/900 | step_time= 0.73s | loss=7.0565 (cls=1.622, par=5.412, rel=0.023) | elapsed=5.7min\n",
      "  step  355/900 | step_time= 0.80s | loss=6.8307 (cls=1.602, par=5.206, rel=0.022) | elapsed=5.7min\n",
      "  step  360/900 | step_time= 0.88s | loss=6.9768 (cls=1.625, par=5.335, rel=0.017) | elapsed=5.8min\n",
      "  step  365/900 | step_time= 0.88s | loss=6.9814 (cls=1.631, par=5.331, rel=0.019) | elapsed=5.9min\n",
      "  step  370/900 | step_time= 0.89s | loss=6.7277 (cls=1.577, par=5.135, rel=0.016) | elapsed=6.0min\n",
      "  step  375/900 | step_time= 0.87s | loss=6.7784 (cls=1.622, par=5.139, rel=0.018) | elapsed=6.0min\n",
      "  step  380/900 | step_time= 0.53s | loss=6.8909 (cls=1.608, par=5.262, rel=0.021) | elapsed=6.1min\n",
      "  step  385/900 | step_time= 0.53s | loss=7.0565 (cls=1.667, par=5.367, rel=0.022) | elapsed=6.2min\n",
      "  step  390/900 | step_time= 0.88s | loss=6.7185 (cls=1.612, par=5.089, rel=0.017) | elapsed=6.2min\n",
      "  step  395/900 | step_time= 0.88s | loss=6.8093 (cls=1.602, par=5.188, rel=0.019) | elapsed=6.3min\n",
      "  step  400/900 | step_time= 0.89s | loss=6.9490 (cls=1.616, par=5.309, rel=0.024) | elapsed=6.4min\n",
      "  step  405/900 | step_time= 0.91s | loss=6.8477 (cls=1.612, par=5.218, rel=0.018) | elapsed=6.5min\n",
      "  step  410/900 | step_time= 0.97s | loss=6.6815 (cls=1.580, par=5.084, rel=0.017) | elapsed=6.6min\n",
      "  step  415/900 | step_time= 0.89s | loss=6.8631 (cls=1.630, par=5.220, rel=0.014) | elapsed=6.6min\n",
      "  step  420/900 | step_time= 0.83s | loss=6.8874 (cls=1.618, par=5.244, rel=0.026) | elapsed=6.7min\n",
      "  step  425/900 | step_time= 1.08s | loss=6.7448 (cls=1.614, par=5.118, rel=0.013) | elapsed=6.8min\n",
      "  step  430/900 | step_time= 0.89s | loss=6.6823 (cls=1.605, par=5.064, rel=0.014) | elapsed=6.9min\n",
      "  step  435/900 | step_time= 0.89s | loss=6.6707 (cls=1.560, par=5.099, rel=0.011) | elapsed=6.9min\n",
      "  step  440/900 | step_time= 0.92s | loss=6.6337 (cls=1.641, par=4.977, rel=0.015) | elapsed=7.0min\n",
      "  step  445/900 | step_time= 0.90s | loss=6.8658 (cls=1.571, par=5.272, rel=0.022) | elapsed=7.1min\n",
      "  step  450/900 | step_time= 0.87s | loss=6.7412 (cls=1.597, par=5.131, rel=0.013) | elapsed=7.2min\n",
      "  step  455/900 | step_time= 0.91s | loss=6.6749 (cls=1.588, par=5.070, rel=0.017) | elapsed=7.2min\n",
      "  step  460/900 | step_time= 0.91s | loss=6.5724 (cls=1.604, par=4.956, rel=0.012) | elapsed=7.3min\n",
      "  step  465/900 | step_time= 0.91s | loss=6.6186 (cls=1.602, par=5.000, rel=0.017) | elapsed=7.4min\n",
      "  step  470/900 | step_time= 0.88s | loss=6.7759 (cls=1.596, par=5.165, rel=0.015) | elapsed=7.5min\n",
      "  step  475/900 | step_time= 0.92s | loss=6.7624 (cls=1.582, par=5.157, rel=0.024) | elapsed=7.5min\n",
      "  step  480/900 | step_time= 0.91s | loss=6.7942 (cls=1.598, par=5.181, rel=0.015) | elapsed=7.6min\n",
      "  step  485/900 | step_time= 0.90s | loss=6.5529 (cls=1.580, par=4.965, rel=0.008) | elapsed=7.7min\n",
      "  step  490/900 | step_time= 0.90s | loss=6.9515 (cls=1.630, par=5.301, rel=0.020) | elapsed=7.8min\n",
      "  step  495/900 | step_time= 0.89s | loss=6.8149 (cls=1.577, par=5.217, rel=0.021) | elapsed=7.8min\n",
      "  step  500/900 | step_time= 0.93s | loss=6.4315 (cls=1.617, par=4.809, rel=0.006) | elapsed=7.9min\n",
      "  step  505/900 | step_time= 0.93s | loss=6.8733 (cls=1.634, par=5.230, rel=0.009) | elapsed=8.0min\n",
      "  step  510/900 | step_time= 0.93s | loss=6.6284 (cls=1.593, par=5.020, rel=0.015) | elapsed=8.1min\n",
      "  step  515/900 | step_time= 0.88s | loss=6.7270 (cls=1.587, par=5.122, rel=0.018) | elapsed=8.1min\n",
      "  step  520/900 | step_time= 0.92s | loss=6.7387 (cls=1.590, par=5.138, rel=0.011) | elapsed=8.2min\n",
      "  step  525/900 | step_time= 0.92s | loss=6.7053 (cls=1.609, par=5.079, rel=0.017) | elapsed=8.3min\n",
      "  step  530/900 | step_time= 0.88s | loss=6.7245 (cls=1.587, par=5.123, rel=0.014) | elapsed=8.4min\n",
      "  step  535/900 | step_time= 0.89s | loss=6.5845 (cls=1.569, par=5.004, rel=0.011) | elapsed=8.5min\n",
      "  step  540/900 | step_time= 0.91s | loss=6.6034 (cls=1.582, par=5.003, rel=0.018) | elapsed=8.5min\n",
      "  step  545/900 | step_time= 0.96s | loss=6.6489 (cls=1.568, par=5.064, rel=0.017) | elapsed=8.6min\n",
      "  step  550/900 | step_time= 0.90s | loss=6.6076 (cls=1.594, par=5.000, rel=0.013) | elapsed=8.7min\n",
      "  step  555/900 | step_time= 0.84s | loss=6.5331 (cls=1.620, par=4.892, rel=0.022) | elapsed=8.8min\n",
      "  step  560/900 | step_time= 0.89s | loss=6.6494 (cls=1.558, par=5.079, rel=0.013) | elapsed=8.8min\n",
      "  step  565/900 | step_time= 0.89s | loss=6.4626 (cls=1.596, par=4.856, rel=0.010) | elapsed=8.9min\n",
      "  step  570/900 | step_time= 0.88s | loss=6.6916 (cls=1.562, par=5.116, rel=0.014) | elapsed=9.0min\n",
      "  step  575/900 | step_time= 0.88s | loss=6.5756 (cls=1.592, par=4.971, rel=0.012) | elapsed=9.1min\n",
      "  step  580/900 | step_time= 0.90s | loss=6.6179 (cls=1.538, par=5.066, rel=0.014) | elapsed=9.1min\n",
      "  step  585/900 | step_time= 0.89s | loss=6.4526 (cls=1.597, par=4.848, rel=0.008) | elapsed=9.2min\n",
      "  step  590/900 | step_time= 0.88s | loss=6.4643 (cls=1.568, par=4.887, rel=0.010) | elapsed=9.3min\n",
      "  step  595/900 | step_time= 0.89s | loss=6.5191 (cls=1.570, par=4.940, rel=0.009) | elapsed=9.4min\n",
      "  step  600/900 | step_time= 0.90s | loss=6.5679 (cls=1.601, par=4.955, rel=0.012) | elapsed=9.4min\n",
      "  step  605/900 | step_time= 0.89s | loss=6.6288 (cls=1.558, par=5.064, rel=0.006) | elapsed=9.5min\n",
      "  step  610/900 | step_time= 0.90s | loss=6.7231 (cls=1.623, par=5.087, rel=0.013) | elapsed=9.6min\n",
      "  step  615/900 | step_time= 0.90s | loss=6.5567 (cls=1.594, par=4.954, rel=0.008) | elapsed=9.7min\n",
      "  step  620/900 | step_time= 0.89s | loss=6.5303 (cls=1.603, par=4.915, rel=0.013) | elapsed=9.7min\n",
      "  step  625/900 | step_time= 0.89s | loss=6.6609 (cls=1.585, par=5.065, rel=0.011) | elapsed=9.8min\n",
      "  step  630/900 | step_time= 0.89s | loss=6.4964 (cls=1.570, par=4.921, rel=0.006) | elapsed=9.9min\n",
      "  step  635/900 | step_time= 0.89s | loss=6.4888 (cls=1.608, par=4.871, rel=0.010) | elapsed=10.0min\n",
      "  step  640/900 | step_time= 0.90s | loss=6.5293 (cls=1.585, par=4.932, rel=0.012) | elapsed=10.0min\n",
      "  step  645/900 | step_time= 0.89s | loss=6.4313 (cls=1.583, par=4.843, rel=0.005) | elapsed=10.1min\n",
      "  step  650/900 | step_time= 0.89s | loss=6.3663 (cls=1.594, par=4.769, rel=0.004) | elapsed=10.2min\n",
      "  step  655/900 | step_time= 0.89s | loss=6.7168 (cls=1.580, par=5.116, rel=0.021) | elapsed=10.3min\n",
      "  step  660/900 | step_time= 0.89s | loss=6.3342 (cls=1.577, par=4.750, rel=0.007) | elapsed=10.3min\n",
      "  step  665/900 | step_time= 0.91s | loss=6.5388 (cls=1.541, par=4.982, rel=0.015) | elapsed=10.4min\n",
      "  step  670/900 | step_time= 0.91s | loss=6.5513 (cls=1.605, par=4.933, rel=0.014) | elapsed=10.5min\n",
      "  step  675/900 | step_time= 0.90s | loss=6.4225 (cls=1.580, par=4.834, rel=0.009) | elapsed=10.6min\n",
      "  step  680/900 | step_time= 0.86s | loss=6.4039 (cls=1.603, par=4.792, rel=0.008) | elapsed=10.6min\n",
      "  step  685/900 | step_time= 0.90s | loss=6.3670 (cls=1.568, par=4.794, rel=0.006) | elapsed=10.7min\n",
      "  step  690/900 | step_time= 0.88s | loss=6.4261 (cls=1.601, par=4.817, rel=0.008) | elapsed=10.8min\n",
      "  step  695/900 | step_time= 0.89s | loss=6.5126 (cls=1.585, par=4.921, rel=0.007) | elapsed=10.9min\n",
      "  step  700/900 | step_time= 0.89s | loss=6.5280 (cls=1.575, par=4.929, rel=0.024) | elapsed=10.9min\n",
      "  step  705/900 | step_time= 0.87s | loss=6.5631 (cls=1.560, par=4.991, rel=0.012) | elapsed=11.0min\n",
      "  step  710/900 | step_time= 0.88s | loss=6.4252 (cls=1.583, par=4.835, rel=0.007) | elapsed=11.1min\n",
      "  step  715/900 | step_time= 0.89s | loss=6.5167 (cls=1.583, par=4.925, rel=0.008) | elapsed=11.2min\n",
      "  step  720/900 | step_time= 0.87s | loss=6.3166 (cls=1.553, par=4.759, rel=0.004) | elapsed=11.2min\n",
      "  step  725/900 | step_time= 0.89s | loss=6.7743 (cls=1.558, par=5.198, rel=0.018) | elapsed=11.3min\n",
      "  step  730/900 | step_time= 0.88s | loss=6.4442 (cls=1.566, par=4.867, rel=0.011) | elapsed=11.4min\n",
      "  step  735/900 | step_time= 0.89s | loss=6.4974 (cls=1.565, par=4.919, rel=0.013) | elapsed=11.5min\n",
      "  step  740/900 | step_time= 0.89s | loss=6.5796 (cls=1.595, par=4.974, rel=0.010) | elapsed=11.5min\n",
      "  step  745/900 | step_time= 0.88s | loss=6.2902 (cls=1.550, par=4.731, rel=0.009) | elapsed=11.6min\n",
      "  step  750/900 | step_time= 0.91s | loss=6.4614 (cls=1.578, par=4.878, rel=0.005) | elapsed=11.7min\n",
      "  step  755/900 | step_time= 0.89s | loss=6.5148 (cls=1.596, par=4.908, rel=0.010) | elapsed=11.8min\n",
      "  step  760/900 | step_time= 0.88s | loss=6.4206 (cls=1.558, par=4.852, rel=0.010) | elapsed=11.8min\n",
      "  step  765/900 | step_time= 0.89s | loss=6.4510 (cls=1.573, par=4.870, rel=0.007) | elapsed=11.9min\n",
      "  step  770/900 | step_time= 0.89s | loss=6.8017 (cls=1.593, par=5.186, rel=0.022) | elapsed=12.0min\n",
      "  step  775/900 | step_time= 0.89s | loss=6.4186 (cls=1.584, par=4.828, rel=0.007) | elapsed=12.1min\n",
      "  step  780/900 | step_time= 0.89s | loss=6.3738 (cls=1.533, par=4.830, rel=0.010) | elapsed=12.1min\n",
      "  step  785/900 | step_time= 0.90s | loss=6.3743 (cls=1.591, par=4.778, rel=0.005) | elapsed=12.2min\n",
      "  step  790/900 | step_time= 0.89s | loss=6.5284 (cls=1.569, par=4.950, rel=0.010) | elapsed=12.3min\n",
      "  step  795/900 | step_time= 0.88s | loss=6.5027 (cls=1.547, par=4.945, rel=0.011) | elapsed=12.4min\n",
      "  step  800/900 | step_time= 0.89s | loss=6.3480 (cls=1.584, par=4.757, rel=0.007) | elapsed=12.4min\n",
      "  step  805/900 | step_time= 0.90s | loss=6.4690 (cls=1.595, par=4.869, rel=0.005) | elapsed=12.5min\n",
      "  step  810/900 | step_time= 0.89s | loss=6.3373 (cls=1.538, par=4.792, rel=0.007) | elapsed=12.6min\n",
      "  step  815/900 | step_time= 0.88s | loss=6.4720 (cls=1.569, par=4.896, rel=0.006) | elapsed=12.7min\n",
      "  step  820/900 | step_time= 0.90s | loss=6.9665 (cls=1.588, par=5.357, rel=0.021) | elapsed=12.7min\n",
      "  step  825/900 | step_time= 0.91s | loss=6.3660 (cls=1.587, par=4.772, rel=0.007) | elapsed=12.8min\n",
      "  step  830/900 | step_time= 0.92s | loss=6.4465 (cls=1.579, par=4.855, rel=0.012) | elapsed=12.9min\n",
      "  step  835/900 | step_time= 0.92s | loss=6.4108 (cls=1.547, par=4.859, rel=0.004) | elapsed=13.0min\n",
      "  step  840/900 | step_time= 0.91s | loss=6.3866 (cls=1.588, par=4.791, rel=0.007) | elapsed=13.0min\n",
      "  step  845/900 | step_time= 0.89s | loss=6.5987 (cls=1.575, par=5.013, rel=0.011) | elapsed=13.1min\n",
      "  step  850/900 | step_time= 0.91s | loss=6.3040 (cls=1.582, par=4.719, rel=0.003) | elapsed=13.2min\n",
      "  step  855/900 | step_time= 0.91s | loss=6.4062 (cls=1.541, par=4.860, rel=0.005) | elapsed=13.3min\n",
      "  step  860/900 | step_time= 0.91s | loss=6.4236 (cls=1.608, par=4.805, rel=0.010) | elapsed=13.3min\n",
      "  step  865/900 | step_time= 0.91s | loss=6.3194 (cls=1.558, par=4.756, rel=0.005) | elapsed=13.4min\n",
      "  step  870/900 | step_time= 0.90s | loss=6.4943 (cls=1.571, par=4.918, rel=0.006) | elapsed=13.5min\n",
      "  step  875/900 | step_time= 0.90s | loss=6.5478 (cls=1.607, par=4.927, rel=0.014) | elapsed=13.6min\n",
      "  step  880/900 | step_time= 0.89s | loss=6.2835 (cls=1.581, par=4.698, rel=0.004) | elapsed=13.6min\n",
      "  step  885/900 | step_time= 0.91s | loss=6.3558 (cls=1.579, par=4.768, rel=0.008) | elapsed=13.7min\n",
      "  step  890/900 | step_time= 0.89s | loss=6.4284 (cls=1.560, par=4.855, rel=0.013) | elapsed=13.8min\n",
      "  step  895/900 | step_time= 0.90s | loss=6.3662 (cls=1.558, par=4.800, rel=0.008) | elapsed=13.9min\n",
      "  step  900/900 | step_time= 0.91s | loss=6.2512 (cls=1.574, par=4.673, rel=0.004) | elapsed=13.9min\n",
      "[Train] epoch done in 13.94 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "export -> hrds_runs\\hrds_dsps_fullrun\\export_ep01:   0%|                                       | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRecursionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 202\u001b[39m\n\u001b[32m    199\u001b[39m os.makedirs(export_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    201\u001b[39m t0 = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m \u001b[43mexport_split_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m metrics = eval_export_dir_dual(export_dir, exclude_meta=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    204\u001b[39m eval_time = time.time() - t0\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\hrdoc\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mexport_split_predictions\u001b[39m\u001b[34m(model, loader, save_dir, device)\u001b[39m\n\u001b[32m      6\u001b[39m os.makedirs(save_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tqdm(loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexport -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     pred = \u001b[43mpredict_doc_with_rel_recompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     gt_json = export_tree_json(doc, pred=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     12\u001b[39m     pr_json = export_tree_json(doc, pred=pred)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\hrdoc\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36m_safe_predict_doc_with_rel_recompute\u001b[39m\u001b[34m(model, doc, device)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_safe_predict_doc_with_rel_recompute\u001b[39m(model, doc, device):\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     pred = \u001b[43m_orig_predict_doc_with_rel_recompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# pred 结构未知（取决于你实现），这里采用“尽力而为”的安全修复：\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# 只要 pred 里有 parents / parent_ids 等字段，就 clamp 到合法范围。\u001b[39;00m\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# 如果你 pred 是 tuple/list，也尝试定位 parents。\u001b[39;00m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_clamp_parents\u001b[39m(parents, L):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\hrdoc\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36m_safe_predict_doc_with_rel_recompute\u001b[39m\u001b[34m(model, doc, device)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_safe_predict_doc_with_rel_recompute\u001b[39m(model, doc, device):\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     pred = \u001b[43m_orig_predict_doc_with_rel_recompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# pred 结构未知（取决于你实现），这里采用“尽力而为”的安全修复：\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# 只要 pred 里有 parents / parent_ids 等字段，就 clamp 到合法范围。\u001b[39;00m\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# 如果你 pred 是 tuple/list，也尝试定位 parents。\u001b[39;00m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_clamp_parents\u001b[39m(parents, L):\n",
      "    \u001b[31m[... skipping similar frames: _safe_predict_doc_with_rel_recompute at line 67 (1481 times), context_decorator.<locals>.decorate_context at line 116 (1481 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\hrdoc\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36m_safe_predict_doc_with_rel_recompute\u001b[39m\u001b[34m(model, doc, device)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_safe_predict_doc_with_rel_recompute\u001b[39m(model, doc, device):\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     pred = \u001b[43m_orig_predict_doc_with_rel_recompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# pred 结构未知（取决于你实现），这里采用“尽力而为”的安全修复：\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# 只要 pred 里有 parents / parent_ids 等字段，就 clamp 到合法范围。\u001b[39;00m\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# 如果你 pred 是 tuple/list，也尝试定位 parents。\u001b[39;00m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_clamp_parents\u001b[39m(parents, L):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\hrdoc\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mctx_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\hrdoc\\Lib\\site-packages\\torch\\utils\\_contextlib.py:148\u001b[39m, in \u001b[36m_DecoratorContextManager.clone\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclone\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# override this method if your children class takes __init__ parameters\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\hrdoc\\Lib\\site-packages\\torch\\utils\\_contextlib.py:156\u001b[39m, in \u001b[36m_NoParamDecoratorContextManager.__new__\u001b[39m\u001b[34m(cls, orig_func)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, orig_func=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m orig_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m()(orig_func)\n",
      "\u001b[31mRecursionError\u001b[39m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# HRDS 全流程：train -> best ckpt -> export -> eval\n",
    "# 依赖你 notebook 已有定义（Run All 到此处后再运行本 cell）：\n",
    "#   HRDHDataset, collate_doc\n",
    "#   compute_M_cp_from_dataset\n",
    "#   DSPSModel\n",
    "#   FocalLoss\n",
    "#   train_one_epoch\n",
    "#   export_split_predictions\n",
    "#   eval_export_dir_dual\n",
    "# =========================\n",
    "\n",
    "import os, json, time, shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --------- A) 你必须改的路径：HRDS_ROOT ----------\n",
    "HRDS_ROOT = r\"C:\\Users\\tomra\\Desktop\\PAPER\\final OBJ\\HRDS\"   # 改成你 HRDoc-Simple 根目录\n",
    "\n",
    "# --------- B) 实验输出目录 ----------\n",
    "EXP_NAME = \"hrds_dsps_fullrun\"\n",
    "RUN_ROOT = \"hrds_runs\"\n",
    "EXP_DIR = os.path.join(RUN_ROOT, EXP_NAME)\n",
    "os.makedirs(EXP_DIR, exist_ok=True)\n",
    "\n",
    "# --------- C) 基础对象检查：确保你前面 cell 已经定义完 ----------\n",
    "required = [\n",
    "    \"HRDHDataset\", \"collate_doc\",\n",
    "    \"compute_M_cp_from_dataset\",\n",
    "    \"DSPSModel\",\n",
    "    \"FocalLoss\",\n",
    "    \"train_one_epoch\",\n",
    "    \"export_split_predictions\",\n",
    "    \"eval_export_dir_dual\",\n",
    "    \"cfg\",\n",
    "]\n",
    "missing = [n for n in required if n not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        \"你还没有运行到包含以下定义的 cell（或变量名不同）：\\n\"\n",
    "        + \"\\n\".join(missing)\n",
    "        + \"\\n\\n请先 Run All 直到模型/数据集/评估函数都定义完，再运行本 cell。\"\n",
    "    )\n",
    "\n",
    "if not os.path.isdir(HRDS_ROOT):\n",
    "    raise FileNotFoundError(f\"HRDS_ROOT 不存在：{HRDS_ROOT}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[Device]\", device)\n",
    "print(\"[EXP_DIR]\", EXP_DIR)\n",
    "\n",
    "# --------- D) 固定随机种子（保持复现稳定） ----------\n",
    "seed = 42\n",
    "import random\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# --------- E) 构建 HRDS 数据集与 dataloader ----------\n",
    "train_ds = HRDHDataset(HRDS_ROOT, split=\"train\", max_len=cfg.max_len)\n",
    "test_ds  = HRDHDataset(HRDS_ROOT, split=\"test\",  max_len=cfg.max_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=1,         \n",
    "    shuffle=True,\n",
    "    num_workers=getattr(cfg, \"num_workers\", 0),\n",
    "    collate_fn=collate_doc,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=getattr(cfg, \"num_workers\", 0),\n",
    "    collate_fn=collate_doc,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "print(f\"[HRDS] train docs = {len(train_loader)}, test docs = {len(test_loader)}\")\n",
    "\n",
    "# --------- F) 类别/关系维度（来自你 dataset 定义） ----------\n",
    "# 你的 dataset cell 里固定 14 类，关系含 meta（4 类）\n",
    "num_classes = len(ID2LABEL_14)\n",
    "num_rel = len(REL2ID)\n",
    "print(\"[Dims] num_classes =\", num_classes, \"num_rel =\", num_rel)\n",
    "\n",
    "# --------- G) 计算/加载 M_cp（soft-mask 先验） ----------\n",
    "MCP_PATH = os.path.join(EXP_DIR, \"M_cp_hrds.npy\")\n",
    "if os.path.exists(MCP_PATH):\n",
    "    M_cp = np.load(MCP_PATH)\n",
    "    print(\"[M_cp] loaded:\", MCP_PATH, \"shape=\", M_cp.shape)\n",
    "else:\n",
    "    print(\"[M_cp] computing from HRDS train ...\")\n",
    "    ret = compute_M_cp_from_dataset(train_ds, num_classes=num_classes, pseudo_count=5.0)\n",
    "\n",
    "    # 兼容：函数可能返回 (M_cp, stats) 或 {\"M_cp\":..., ...} 或直接 ndarray\n",
    "    if isinstance(ret, tuple):\n",
    "        M_cp = ret[0]\n",
    "        mcp_extra = ret[1:]\n",
    "    elif isinstance(ret, dict):\n",
    "        M_cp = ret.get(\"M_cp\", None)\n",
    "        mcp_extra = {k:v for k,v in ret.items() if k != \"M_cp\"}\n",
    "        if M_cp is None:\n",
    "            raise RuntimeError(\"compute_M_cp_from_dataset 返回 dict，但不包含键 'M_cp'\")\n",
    "    else:\n",
    "        M_cp = ret\n",
    "        mcp_extra = None\n",
    "\n",
    "    M_cp = np.asarray(M_cp)\n",
    "    np.save(MCP_PATH, M_cp)\n",
    "    print(\"[M_cp] saved:\", MCP_PATH, \"shape=\", M_cp.shape)\n",
    "\n",
    "    # 可选：把额外统计信息也存下来（不影响训练）\n",
    "    if mcp_extra is not None:\n",
    "        extra_path = os.path.join(EXP_DIR, \"M_cp_hrds_extra.json\")\n",
    "        try:\n",
    "            with open(extra_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(mcp_extra, f, ensure_ascii=False, indent=2, default=str)\n",
    "            print(\"[M_cp] extra saved:\", extra_path)\n",
    "        except Exception as e:\n",
    "            print(\"[M_cp] extra save skipped:\", repr(e))\n",
    "\n",
    "# --------- H) 建模（按你现有 DSPSModel 接口） ----------\n",
    "# 这里默认用你当前复现的设置：use_text=False；视觉是否启用取决于你 notebook 中 DSPSModel 的实现\n",
    "# 若你后续要开视觉或文本，改下面两个开关即可\n",
    "USE_TEXT = False\n",
    "USE_VISUAL = False\n",
    "USE_SOFTMASK = True\n",
    "\n",
    "model = DSPSModel(\n",
    "    num_classes=num_classes,\n",
    "    num_rel=num_rel,\n",
    "    M_cp=M_cp,\n",
    "    cfg=cfg,\n",
    "    use_text=USE_TEXT,\n",
    "    use_visual=USE_VISUAL,\n",
    "    use_softmask=USE_SOFTMASK,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=getattr(cfg, \"lr\", 2e-4),\n",
    "    weight_decay=getattr(cfg, \"weight_decay\", 1e-2),\n",
    ")\n",
    "\n",
    "focal_cls = FocalLoss(gamma=getattr(cfg, \"focal_gamma\", 2.0), alpha=getattr(cfg, \"focal_alpha\", 0.25)).to(device)\n",
    "focal_rel = FocalLoss(gamma=getattr(cfg, \"focal_gamma\", 2.0), alpha=getattr(cfg, \"focal_alpha\", 0.25)).to(device)\n",
    "\n",
    "# --------- I) 训练 + 每 epoch 验证 + 保存 best ----------\n",
    "CKPT_BEST = os.path.join(EXP_DIR, \"best_hrds.pt\")\n",
    "REPORT_PATH = os.path.join(EXP_DIR, \"train_eval_log.json\")\n",
    "\n",
    "history = []\n",
    "best_score = -1.0\n",
    "best_epoch = -1\n",
    "\n",
    "def _extract_macro_strict(metrics_dict):\n",
    "    \"\"\"\n",
    "    兼容 eval_export_dir_dual 的返回格式：\n",
    "    - 若存在宏严格 STEDS 的 key，就取它\n",
    "    - 否则退化为 strict_mean 或第一项可用 strict 指标\n",
    "    \"\"\"\n",
    "    if not isinstance(metrics_dict, dict):\n",
    "        return None\n",
    "\n",
    "    # 常见候选 key（根据你实现的命名习惯做容错）\n",
    "    candidates = [\n",
    "        \"steds_strict_macro\", \"macro_steds_strict\", \"steds_macro_strict\",\n",
    "        \"steds_strict_mean\",  \"strict_mean\",        \"steds_strict\",\n",
    "    ]\n",
    "    for k in candidates:\n",
    "        if k in metrics_dict and isinstance(metrics_dict[k], (int, float)):\n",
    "            return float(metrics_dict[k])\n",
    "\n",
    "    # 兜底：如果 metrics_dict 里有任何包含 \"strict\" 的数值字段，取第一个\n",
    "    for k, v in metrics_dict.items():\n",
    "        if \"strict\" in k.lower() and isinstance(v, (int, float)):\n",
    "            return float(v)\n",
    "\n",
    "    return None\n",
    "\n",
    "for ep in range(1, cfg.epochs + 1):\n",
    "    print(f\"\\n========== [HRDS] Epoch {ep}/{cfg.epochs} ==========\")\n",
    "\n",
    "    # 1) train\n",
    "    tr_logs = train_one_epoch(model, train_loader, optimizer, cfg, focal_cls, focal_rel)\n",
    "\n",
    "    # 2) export + eval on test\n",
    "    export_dir = os.path.join(EXP_DIR, f\"export_ep{ep:02d}\")\n",
    "    # 为避免旧结果干扰，每次先清目录\n",
    "    if os.path.isdir(export_dir):\n",
    "        shutil.rmtree(export_dir, ignore_errors=True)\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "    t0 = time.time()\n",
    "    export_split_predictions(model, test_loader, save_dir=export_dir, device=device)\n",
    "    metrics = eval_export_dir_dual(export_dir, exclude_meta=True)\n",
    "    eval_time = time.time() - t0\n",
    "\n",
    "    # 3) best selection（以 Macro Strict STEDS 为主）\n",
    "    score = _extract_macro_strict(metrics)\n",
    "    if score is None:\n",
    "        print(\"[WARN] 无法从 metrics 中提取 strict 指标字段，将不保存 best。metrics=\", metrics)\n",
    "        score = -1.0\n",
    "\n",
    "    is_best = score > best_score\n",
    "    if is_best:\n",
    "        best_score = score\n",
    "        best_epoch = ep\n",
    "        torch.save({\"model\": model.state_dict(), \"epoch\": ep, \"metrics\": metrics}, CKPT_BEST)\n",
    "        print(f\"[BEST] updated: epoch={ep}, score={best_score:.6f} -> {CKPT_BEST}\")\n",
    "\n",
    "    row = {\n",
    "        \"epoch\": ep,\n",
    "        \"train\": tr_logs,\n",
    "        \"metrics\": metrics,\n",
    "        \"macro_strict_for_select\": score,\n",
    "        \"eval_time_sec\": eval_time,\n",
    "        \"is_best\": is_best,\n",
    "    }\n",
    "    history.append(row)\n",
    "\n",
    "    with open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"exp_name\": EXP_NAME,\n",
    "                \"hrds_root\": HRDS_ROOT,\n",
    "                \"cfg\": cfg.__dict__ if hasattr(cfg, \"__dict__\") else str(cfg),\n",
    "                \"use_text\": USE_TEXT,\n",
    "                \"use_visual\": USE_VISUAL,\n",
    "                \"use_softmask\": USE_SOFTMASK,\n",
    "                \"best_epoch\": best_epoch,\n",
    "                \"best_score_macro_strict\": best_score,\n",
    "                \"best_ckpt\": CKPT_BEST,\n",
    "                \"history\": history,\n",
    "            },\n",
    "            f,\n",
    "            ensure_ascii=False,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "    print(\"[Epoch Summary]\")\n",
    "    print(\"  train loss(last):\", tr_logs[\"loss\"][-1] if isinstance(tr_logs, dict) and \"loss\" in tr_logs else tr_logs)\n",
    "    print(\"  metrics:\", metrics)\n",
    "    print(\"  eval_time_sec:\", eval_time)\n",
    "\n",
    "print(\"\\n========== HRDS Full Flow DONE ==========\")\n",
    "print(\"[BEST] epoch =\", best_epoch, \"macro_strict =\", best_score)\n",
    "print(\"[BEST CKPT]\", CKPT_BEST)\n",
    "print(\"[LOG JSON ]\", REPORT_PATH)\n",
    "\n",
    "print(\"\\nPaper reference (Table 2 HRDS best, Document+Semantic+Vision+Soft-mask):\")\n",
    "print(\"Micro-STEDS ≈ 0.8143\")\n",
    "print(\"Macro-STEDS ≈ 0.8174\")\n",
    "\n",
    "# --------- J) 用 best ckpt 再导出一次（最终留档） ----------\n",
    "FINAL_EXPORT = os.path.join(EXP_DIR, \"export_best_final\")\n",
    "if os.path.isdir(FINAL_EXPORT):\n",
    "    shutil.rmtree(FINAL_EXPORT, ignore_errors=True)\n",
    "os.makedirs(FINAL_EXPORT, exist_ok=True)\n",
    "\n",
    "if os.path.exists(CKPT_BEST):\n",
    "    ckpt = torch.load(CKPT_BEST, map_location=\"cpu\")\n",
    "    model.load_state_dict(ckpt[\"model\"], strict=False)\n",
    "    model.eval()\n",
    "\n",
    "export_split_predictions(model, test_loader, save_dir=FINAL_EXPORT, device=device)\n",
    "final_metrics = eval_export_dir_dual(FINAL_EXPORT, exclude_meta=True)\n",
    "\n",
    "FINAL_REPORT = os.path.join(\"final_reports\", f\"{EXP_NAME}_HRDS_report.json\")\n",
    "os.makedirs(\"final_reports\", exist_ok=True)\n",
    "with open(FINAL_REPORT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"dataset\": \"HRDoc-Simple (HRDS)\",\n",
    "            \"hrds_root\": HRDS_ROOT,\n",
    "            \"best_ckpt\": CKPT_BEST,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"best_score_macro_strict\": best_score,\n",
    "            \"final_export_dir\": FINAL_EXPORT,\n",
    "            \"final_metrics\": final_metrics,\n",
    "            \"log_path\": REPORT_PATH,\n",
    "        },\n",
    "        f,\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "print(\"\\n===== HRDS Final Validation =====\")\n",
    "print(\"Final export :\", FINAL_EXPORT)\n",
    "print(\"Final report :\", FINAL_REPORT)\n",
    "print(\"Final metrics:\", final_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5b0013b4-ff70-411f-b615-d85d0b9c53c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] cuda\n",
      "[EXP_DIR] hrds_runs\\hrds_dsps_fullrun\n",
      "[HRDS] train docs = 900, test docs = 100\n",
      "[Dims] num_classes = 14 num_rel = 4\n",
      "[M_cp] loaded: hrds_runs\\hrds_dsps_fullrun\\M_cp_hrds.npy shape= (2, 15, 14)\n",
      "\n",
      "========== [HRDS] Epoch 1/2 ==========\n",
      "\n",
      "[Train] start epoch, num_docs = 900\n",
      "  step    1/900 | step_time= 0.94s | loss=16.9198 (cls=0.501, par=16.229, rel=0.190) | elapsed=0.0min\n",
      "    sanity: seq_len=512, avg_parent_candidates=2.0\n",
      "    sanity: seq_len=501, avg_parent_candidates=2.0\n",
      "  step    5/900 | step_time= 1.03s | loss=11.1667 (cls=1.411, par=9.604, rel=0.151) | elapsed=0.1min\n",
      "  step   10/900 | step_time= 0.92s | loss=9.8604 (cls=1.749, par=8.017, rel=0.094) | elapsed=0.2min\n",
      "  step   15/900 | step_time= 0.91s | loss=9.6233 (cls=1.643, par=7.908, rel=0.073) | elapsed=0.2min\n",
      "  step   20/900 | step_time= 0.89s | loss=9.3956 (cls=1.615, par=7.709, rel=0.071) | elapsed=0.3min\n",
      "  step   25/900 | step_time= 0.89s | loss=9.2130 (cls=1.660, par=7.486, rel=0.066) | elapsed=0.4min\n",
      "  step   30/900 | step_time= 0.88s | loss=8.9500 (cls=1.679, par=7.210, rel=0.061) | elapsed=0.5min\n",
      "  step   35/900 | step_time= 0.93s | loss=8.7475 (cls=1.623, par=7.073, rel=0.051) | elapsed=0.5min\n",
      "  step   40/900 | step_time= 0.97s | loss=8.5313 (cls=1.639, par=6.841, rel=0.052) | elapsed=0.6min\n",
      "  step   45/900 | step_time= 0.88s | loss=8.3723 (cls=1.646, par=6.679, rel=0.047) | elapsed=0.7min\n",
      "  step   50/900 | step_time= 0.90s | loss=8.3755 (cls=1.624, par=6.698, rel=0.054) | elapsed=0.8min\n",
      "  step   55/900 | step_time= 1.09s | loss=8.4316 (cls=1.639, par=6.739, rel=0.054) | elapsed=0.9min\n",
      "  step   60/900 | step_time= 0.92s | loss=8.2095 (cls=1.599, par=6.564, rel=0.047) | elapsed=1.0min\n",
      "  step   65/900 | step_time= 0.92s | loss=8.3312 (cls=1.686, par=6.598, rel=0.048) | elapsed=1.0min\n",
      "  step   70/900 | step_time= 0.77s | loss=8.2143 (cls=1.572, par=6.578, rel=0.065) | elapsed=1.1min\n",
      "  step   75/900 | step_time= 0.88s | loss=8.1818 (cls=1.726, par=6.398, rel=0.058) | elapsed=1.2min\n",
      "  step   80/900 | step_time= 0.84s | loss=8.0983 (cls=1.617, par=6.425, rel=0.056) | elapsed=1.3min\n",
      "  step   85/900 | step_time= 0.79s | loss=8.0133 (cls=1.669, par=6.285, rel=0.059) | elapsed=1.3min\n",
      "  step   90/900 | step_time= 0.74s | loss=8.0025 (cls=1.639, par=6.316, rel=0.047) | elapsed=1.4min\n",
      "  step   95/900 | step_time= 0.89s | loss=7.9895 (cls=1.692, par=6.244, rel=0.053) | elapsed=1.5min\n",
      "  step  100/900 | step_time= 0.93s | loss=7.8522 (cls=1.610, par=6.195, rel=0.047) | elapsed=1.6min\n",
      "  step  105/900 | step_time= 0.92s | loss=7.9964 (cls=1.689, par=6.257, rel=0.050) | elapsed=1.6min\n",
      "  step  110/900 | step_time= 0.89s | loss=7.8349 (cls=1.645, par=6.145, rel=0.045) | elapsed=1.7min\n",
      "  step  115/900 | step_time= 0.89s | loss=7.6250 (cls=1.635, par=5.948, rel=0.041) | elapsed=1.8min\n",
      "  step  120/900 | step_time= 0.95s | loss=7.5691 (cls=1.645, par=5.889, rel=0.035) | elapsed=1.9min\n",
      "  step  125/900 | step_time= 1.12s | loss=7.7805 (cls=1.636, par=6.100, rel=0.044) | elapsed=2.0min\n",
      "  step  130/900 | step_time= 1.01s | loss=7.6321 (cls=1.686, par=5.904, rel=0.042) | elapsed=2.0min\n",
      "  step  135/900 | step_time= 0.89s | loss=7.4878 (cls=1.591, par=5.858, rel=0.040) | elapsed=2.1min\n",
      "  step  140/900 | step_time= 0.93s | loss=7.5969 (cls=1.658, par=5.904, rel=0.035) | elapsed=2.2min\n",
      "  step  145/900 | step_time= 0.61s | loss=7.9582 (cls=1.618, par=6.282, rel=0.058) | elapsed=2.3min\n",
      "  step  150/900 | step_time= 0.92s | loss=7.5511 (cls=1.599, par=5.923, rel=0.029) | elapsed=2.4min\n",
      "  step  155/900 | step_time= 0.95s | loss=7.5589 (cls=1.660, par=5.853, rel=0.046) | elapsed=2.4min\n",
      "  step  160/900 | step_time= 0.88s | loss=7.4218 (cls=1.666, par=5.730, rel=0.026) | elapsed=2.5min\n",
      "  step  165/900 | step_time= 1.17s | loss=7.4710 (cls=1.609, par=5.827, rel=0.036) | elapsed=2.6min\n",
      "  step  170/900 | step_time= 0.92s | loss=7.3305 (cls=1.647, par=5.657, rel=0.026) | elapsed=2.7min\n",
      "  step  175/900 | step_time= 1.00s | loss=7.4101 (cls=1.640, par=5.739, rel=0.032) | elapsed=2.8min\n",
      "  step  180/900 | step_time= 0.91s | loss=7.3436 (cls=1.611, par=5.699, rel=0.033) | elapsed=2.8min\n",
      "  step  185/900 | step_time= 1.12s | loss=7.2870 (cls=1.633, par=5.633, rel=0.021) | elapsed=2.9min\n",
      "  step  190/900 | step_time= 0.93s | loss=7.4587 (cls=1.631, par=5.783, rel=0.045) | elapsed=3.0min\n",
      "  step  195/900 | step_time= 8.62s | loss=7.4693 (cls=1.629, par=5.799, rel=0.041) | elapsed=3.2min\n",
      "  step  200/900 | step_time= 1.52s | loss=7.1634 (cls=1.641, par=5.499, rel=0.023) | elapsed=3.8min\n",
      "  step  205/900 | step_time= 1.55s | loss=7.2878 (cls=1.594, par=5.653, rel=0.040) | elapsed=3.9min\n",
      "  step  210/900 | step_time= 0.93s | loss=7.3722 (cls=1.709, par=5.616, rel=0.047) | elapsed=4.0min\n",
      "  step  215/900 | step_time= 1.29s | loss=7.0932 (cls=1.616, par=5.455, rel=0.022) | elapsed=4.1min\n",
      "  step  220/900 | step_time= 1.70s | loss=7.2467 (cls=1.600, par=5.622, rel=0.024) | elapsed=4.2min\n",
      "  step  225/900 | step_time= 1.12s | loss=7.3387 (cls=1.639, par=5.671, rel=0.029) | elapsed=4.3min\n",
      "  step  230/900 | step_time= 1.03s | loss=7.4646 (cls=1.628, par=5.800, rel=0.037) | elapsed=4.4min\n",
      "  step  235/900 | step_time= 1.49s | loss=7.6841 (cls=1.631, par=6.013, rel=0.040) | elapsed=4.6min\n",
      "  step  240/900 | step_time= 1.14s | loss=7.3140 (cls=1.611, par=5.679, rel=0.024) | elapsed=4.7min\n",
      "  step  245/900 | step_time= 1.03s | loss=7.5444 (cls=1.637, par=5.864, rel=0.043) | elapsed=4.8min\n",
      "  step  250/900 | step_time= 1.04s | loss=7.2433 (cls=1.600, par=5.619, rel=0.024) | elapsed=4.9min\n",
      "  step  255/900 | step_time= 1.14s | loss=7.2016 (cls=1.626, par=5.547, rel=0.028) | elapsed=5.0min\n",
      "  step  260/900 | step_time= 1.13s | loss=7.1031 (cls=1.569, par=5.514, rel=0.020) | elapsed=5.1min\n",
      "  step  265/900 | step_time= 0.96s | loss=7.1663 (cls=1.632, par=5.496, rel=0.039) | elapsed=5.2min\n",
      "  step  270/900 | step_time= 1.01s | loss=7.2319 (cls=1.648, par=5.559, rel=0.025) | elapsed=5.3min\n",
      "  step  275/900 | step_time= 1.29s | loss=7.1106 (cls=1.578, par=5.505, rel=0.027) | elapsed=5.4min\n",
      "  step  280/900 | step_time= 1.76s | loss=7.3655 (cls=1.611, par=5.722, rel=0.033) | elapsed=5.5min\n",
      "  step  285/900 | step_time= 1.57s | loss=7.0893 (cls=1.628, par=5.442, rel=0.019) | elapsed=5.7min\n",
      "  step  290/900 | step_time= 5.94s | loss=7.0125 (cls=1.623, par=5.365, rel=0.025) | elapsed=6.0min\n",
      "  step  295/900 | step_time= 3.09s | loss=7.0396 (cls=1.683, par=5.340, rel=0.016) | elapsed=6.4min\n",
      "  step  300/900 | step_time= 7.49s | loss=6.9590 (cls=1.583, par=5.357, rel=0.019) | elapsed=7.0min\n",
      "  step  305/900 | step_time= 2.42s | loss=7.1459 (cls=1.677, par=5.436, rel=0.033) | elapsed=7.3min\n",
      "  step  310/900 | step_time= 8.19s | loss=6.9043 (cls=1.606, par=5.279, rel=0.019) | elapsed=7.6min\n",
      "  step  315/900 | step_time= 9.86s | loss=7.1551 (cls=1.670, par=5.458, rel=0.027) | elapsed=8.4min\n",
      "  step  320/900 | step_time=10.81s | loss=6.8709 (cls=1.585, par=5.268, rel=0.018) | elapsed=9.3min\n",
      "  step  325/900 | step_time= 9.64s | loss=7.2203 (cls=1.621, par=5.560, rel=0.039) | elapsed=10.1min\n",
      "  step  330/900 | step_time=10.38s | loss=6.9717 (cls=1.650, par=5.299, rel=0.023) | elapsed=10.9min\n",
      "  step  335/900 | step_time=10.59s | loss=6.8277 (cls=1.591, par=5.222, rel=0.014) | elapsed=11.6min\n",
      "  step  340/900 | step_time=10.35s | loss=7.0929 (cls=1.598, par=5.472, rel=0.022) | elapsed=12.4min\n",
      "  step  345/900 | step_time=10.66s | loss=7.3457 (cls=1.646, par=5.664, rel=0.036) | elapsed=13.3min\n",
      "  step  350/900 | step_time= 8.43s | loss=7.0565 (cls=1.622, par=5.412, rel=0.023) | elapsed=14.1min\n",
      "  step  355/900 | step_time= 9.45s | loss=6.8308 (cls=1.603, par=5.206, rel=0.022) | elapsed=14.8min\n",
      "  step  360/900 | step_time= 4.65s | loss=6.9768 (cls=1.625, par=5.335, rel=0.017) | elapsed=15.6min\n",
      "  step  365/900 | step_time=10.66s | loss=6.9814 (cls=1.631, par=5.331, rel=0.019) | elapsed=16.5min\n",
      "  step  370/900 | step_time=10.56s | loss=6.7277 (cls=1.577, par=5.135, rel=0.016) | elapsed=17.4min\n",
      "  step  375/900 | step_time= 1.34s | loss=6.7784 (cls=1.622, par=5.139, rel=0.018) | elapsed=18.4min\n",
      "  step  380/900 | step_time= 1.27s | loss=6.8908 (cls=1.608, par=5.262, rel=0.021) | elapsed=18.5min\n",
      "  step  385/900 | step_time= 6.78s | loss=7.0564 (cls=1.667, par=5.367, rel=0.022) | elapsed=19.5min\n",
      "  step  390/900 | step_time= 1.62s | loss=6.7185 (cls=1.612, par=5.089, rel=0.017) | elapsed=20.1min\n",
      "  step  395/900 | step_time= 1.17s | loss=6.8093 (cls=1.602, par=5.188, rel=0.019) | elapsed=20.2min\n",
      "  step  400/900 | step_time= 1.20s | loss=6.9490 (cls=1.616, par=5.309, rel=0.024) | elapsed=20.3min\n",
      "  step  405/900 | step_time=11.29s | loss=6.8477 (cls=1.612, par=5.218, rel=0.018) | elapsed=20.6min\n",
      "  step  410/900 | step_time= 1.58s | loss=6.6815 (cls=1.580, par=5.084, rel=0.017) | elapsed=21.0min\n",
      "  step  415/900 | step_time= 1.03s | loss=6.8631 (cls=1.630, par=5.220, rel=0.014) | elapsed=21.1min\n",
      "  step  420/900 | step_time= 1.56s | loss=6.8874 (cls=1.618, par=5.244, rel=0.026) | elapsed=21.2min\n",
      "  step  425/900 | step_time= 1.94s | loss=6.7449 (cls=1.614, par=5.118, rel=0.013) | elapsed=21.3min\n",
      "  step  430/900 | step_time= 1.21s | loss=6.6823 (cls=1.605, par=5.064, rel=0.014) | elapsed=21.4min\n",
      "  step  435/900 | step_time= 1.16s | loss=6.6707 (cls=1.560, par=5.099, rel=0.011) | elapsed=21.5min\n",
      "  step  440/900 | step_time= 1.23s | loss=6.6337 (cls=1.642, par=4.977, rel=0.015) | elapsed=21.6min\n",
      "  step  445/900 | step_time= 1.22s | loss=6.8658 (cls=1.571, par=5.272, rel=0.022) | elapsed=21.7min\n",
      "  step  450/900 | step_time= 1.87s | loss=6.7412 (cls=1.597, par=5.131, rel=0.013) | elapsed=21.8min\n",
      "  step  455/900 | step_time=10.20s | loss=6.6750 (cls=1.588, par=5.070, rel=0.017) | elapsed=22.3min\n",
      "  step  460/900 | step_time= 1.61s | loss=6.5725 (cls=1.604, par=4.956, rel=0.012) | elapsed=22.4min\n",
      "  step  465/900 | step_time=11.01s | loss=6.6186 (cls=1.602, par=5.000, rel=0.017) | elapsed=23.0min\n",
      "  step  470/900 | step_time= 4.59s | loss=6.7759 (cls=1.596, par=5.165, rel=0.015) | elapsed=23.4min\n",
      "  step  475/900 | step_time= 1.58s | loss=6.7624 (cls=1.582, par=5.157, rel=0.024) | elapsed=24.0min\n",
      "  step  480/900 | step_time=22.70s | loss=6.7942 (cls=1.598, par=5.181, rel=0.015) | elapsed=24.9min\n",
      "  step  485/900 | step_time=13.28s | loss=6.5529 (cls=1.580, par=4.965, rel=0.008) | elapsed=25.8min\n",
      "  step  490/900 | step_time= 1.12s | loss=6.9515 (cls=1.630, par=5.301, rel=0.020) | elapsed=26.2min\n",
      "  step  495/900 | step_time= 1.40s | loss=6.8149 (cls=1.577, par=5.217, rel=0.021) | elapsed=26.3min\n",
      "  step  500/900 | step_time=14.99s | loss=6.4315 (cls=1.617, par=4.809, rel=0.006) | elapsed=26.7min\n",
      "  step  505/900 | step_time= 5.36s | loss=6.8736 (cls=1.634, par=5.230, rel=0.009) | elapsed=28.1min\n",
      "  step  510/900 | step_time= 4.60s | loss=6.6284 (cls=1.593, par=5.020, rel=0.015) | elapsed=28.5min\n",
      "  step  515/900 | step_time= 2.60s | loss=6.7270 (cls=1.588, par=5.122, rel=0.018) | elapsed=28.7min\n",
      "  step  520/900 | step_time= 2.65s | loss=6.7387 (cls=1.590, par=5.138, rel=0.011) | elapsed=29.0min\n",
      "  step  525/900 | step_time= 2.62s | loss=6.7053 (cls=1.610, par=5.079, rel=0.017) | elapsed=29.2min\n",
      "  step  530/900 | step_time= 2.75s | loss=6.7244 (cls=1.587, par=5.123, rel=0.014) | elapsed=29.4min\n",
      "  step  535/900 | step_time= 3.34s | loss=6.5845 (cls=1.569, par=5.004, rel=0.011) | elapsed=29.7min\n",
      "  step  540/900 | step_time= 3.74s | loss=6.6033 (cls=1.582, par=5.003, rel=0.018) | elapsed=29.9min\n",
      "  step  545/900 | step_time= 4.24s | loss=6.6489 (cls=1.568, par=5.064, rel=0.017) | elapsed=30.3min\n",
      "  step  550/900 | step_time= 3.89s | loss=6.6076 (cls=1.594, par=5.000, rel=0.013) | elapsed=30.6min\n",
      "  step  555/900 | step_time= 4.31s | loss=6.5331 (cls=1.620, par=4.892, rel=0.022) | elapsed=31.0min\n",
      "  step  560/900 | step_time= 3.26s | loss=6.6494 (cls=1.557, par=5.079, rel=0.013) | elapsed=31.2min\n",
      "  step  565/900 | step_time= 2.90s | loss=6.4627 (cls=1.596, par=4.856, rel=0.010) | elapsed=31.5min\n",
      "  step  570/900 | step_time= 3.17s | loss=6.6917 (cls=1.562, par=5.116, rel=0.014) | elapsed=31.9min\n",
      "  step  575/900 | step_time= 4.30s | loss=6.5755 (cls=1.592, par=4.971, rel=0.012) | elapsed=32.2min\n",
      "  step  580/900 | step_time= 3.07s | loss=6.6179 (cls=1.538, par=5.066, rel=0.014) | elapsed=32.5min\n",
      "  step  585/900 | step_time= 2.52s | loss=6.4526 (cls=1.597, par=4.848, rel=0.008) | elapsed=32.7min\n",
      "  step  590/900 | step_time= 1.38s | loss=6.4643 (cls=1.568, par=4.887, rel=0.010) | elapsed=32.8min\n",
      "  step  595/900 | step_time= 1.01s | loss=6.5191 (cls=1.570, par=4.940, rel=0.009) | elapsed=32.9min\n",
      "  step  600/900 | step_time= 1.55s | loss=6.5679 (cls=1.601, par=4.955, rel=0.012) | elapsed=33.0min\n",
      "  step  605/900 | step_time= 1.01s | loss=6.6289 (cls=1.558, par=5.064, rel=0.006) | elapsed=33.1min\n",
      "  step  610/900 | step_time= 1.17s | loss=6.7231 (cls=1.623, par=5.087, rel=0.013) | elapsed=33.2min\n",
      "  step  615/900 | step_time= 1.55s | loss=6.5567 (cls=1.594, par=4.954, rel=0.008) | elapsed=33.3min\n",
      "  step  620/900 | step_time= 1.05s | loss=6.5303 (cls=1.603, par=4.915, rel=0.013) | elapsed=33.4min\n",
      "  step  625/900 | step_time= 1.15s | loss=6.6609 (cls=1.585, par=5.065, rel=0.011) | elapsed=33.5min\n",
      "  step  630/900 | step_time= 1.11s | loss=6.4964 (cls=1.570, par=4.921, rel=0.006) | elapsed=33.6min\n",
      "  step  635/900 | step_time= 1.46s | loss=6.4888 (cls=1.608, par=4.871, rel=0.010) | elapsed=33.7min\n",
      "  step  640/900 | step_time= 0.98s | loss=6.5293 (cls=1.585, par=4.932, rel=0.012) | elapsed=33.8min\n",
      "  step  645/900 | step_time= 1.20s | loss=6.4313 (cls=1.583, par=4.843, rel=0.005) | elapsed=33.9min\n",
      "  step  650/900 | step_time= 1.17s | loss=6.3663 (cls=1.594, par=4.769, rel=0.004) | elapsed=34.0min\n",
      "  step  655/900 | step_time= 2.92s | loss=6.7168 (cls=1.580, par=5.116, rel=0.021) | elapsed=34.2min\n",
      "  step  660/900 | step_time= 2.54s | loss=6.3342 (cls=1.577, par=4.750, rel=0.007) | elapsed=34.4min\n",
      "  step  665/900 | step_time= 2.50s | loss=6.5388 (cls=1.541, par=4.982, rel=0.015) | elapsed=34.6min\n",
      "  step  670/900 | step_time= 3.17s | loss=6.5513 (cls=1.605, par=4.933, rel=0.014) | elapsed=34.9min\n",
      "  step  675/900 | step_time= 2.56s | loss=6.4225 (cls=1.580, par=4.834, rel=0.009) | elapsed=35.1min\n",
      "  step  680/900 | step_time= 2.50s | loss=6.4038 (cls=1.603, par=4.792, rel=0.008) | elapsed=35.3min\n",
      "  step  685/900 | step_time= 2.34s | loss=6.3671 (cls=1.568, par=4.794, rel=0.006) | elapsed=35.5min\n",
      "  step  690/900 | step_time= 2.02s | loss=6.4261 (cls=1.601, par=4.817, rel=0.008) | elapsed=35.7min\n",
      "  step  695/900 | step_time= 1.95s | loss=6.5125 (cls=1.585, par=4.921, rel=0.007) | elapsed=35.9min\n",
      "  step  700/900 | step_time= 2.46s | loss=6.5280 (cls=1.575, par=4.929, rel=0.024) | elapsed=36.1min\n",
      "  step  705/900 | step_time= 2.55s | loss=6.5631 (cls=1.560, par=4.991, rel=0.012) | elapsed=36.3min\n",
      "  step  710/900 | step_time= 2.60s | loss=6.4251 (cls=1.583, par=4.835, rel=0.007) | elapsed=36.5min\n",
      "  step  715/900 | step_time=13.75s | loss=6.5167 (cls=1.583, par=4.925, rel=0.008) | elapsed=36.9min\n",
      "  step  720/900 | step_time= 5.73s | loss=6.3166 (cls=1.554, par=4.759, rel=0.004) | elapsed=37.3min\n",
      "  step  725/900 | step_time= 1.65s | loss=6.7743 (cls=1.558, par=5.198, rel=0.018) | elapsed=37.4min\n",
      "  step  730/900 | step_time= 2.08s | loss=6.4441 (cls=1.566, par=4.867, rel=0.011) | elapsed=37.6min\n",
      "  step  735/900 | step_time= 1.87s | loss=6.4974 (cls=1.565, par=4.919, rel=0.013) | elapsed=37.7min\n",
      "  step  740/900 | step_time= 1.84s | loss=6.5796 (cls=1.595, par=4.974, rel=0.010) | elapsed=37.9min\n",
      "  step  745/900 | step_time= 1.83s | loss=6.2901 (cls=1.550, par=4.731, rel=0.009) | elapsed=38.0min\n",
      "  step  750/900 | step_time= 2.10s | loss=6.4615 (cls=1.578, par=4.878, rel=0.005) | elapsed=38.2min\n",
      "  step  755/900 | step_time= 1.78s | loss=6.5149 (cls=1.596, par=4.908, rel=0.010) | elapsed=38.3min\n",
      "  step  760/900 | step_time= 1.64s | loss=6.4206 (cls=1.558, par=4.852, rel=0.010) | elapsed=38.5min\n",
      "  step  765/900 | step_time= 1.87s | loss=6.4510 (cls=1.573, par=4.870, rel=0.007) | elapsed=38.6min\n",
      "  step  770/900 | step_time= 1.83s | loss=6.8018 (cls=1.593, par=5.186, rel=0.022) | elapsed=38.8min\n",
      "  step  775/900 | step_time= 1.52s | loss=6.4185 (cls=1.584, par=4.828, rel=0.007) | elapsed=38.9min\n",
      "  step  780/900 | step_time= 1.98s | loss=6.3738 (cls=1.533, par=4.830, rel=0.010) | elapsed=39.1min\n",
      "  step  785/900 | step_time= 1.86s | loss=6.3744 (cls=1.591, par=4.778, rel=0.005) | elapsed=39.2min\n",
      "  step  790/900 | step_time= 1.79s | loss=6.5285 (cls=1.569, par=4.950, rel=0.010) | elapsed=39.4min\n",
      "  step  795/900 | step_time= 1.60s | loss=6.5027 (cls=1.547, par=4.945, rel=0.011) | elapsed=39.5min\n",
      "  step  800/900 | step_time= 1.94s | loss=6.3480 (cls=1.584, par=4.757, rel=0.007) | elapsed=39.7min\n",
      "  step  805/900 | step_time= 1.92s | loss=6.4690 (cls=1.595, par=4.869, rel=0.005) | elapsed=39.8min\n",
      "  step  810/900 | step_time= 1.74s | loss=6.3373 (cls=1.538, par=4.792, rel=0.007) | elapsed=40.0min\n",
      "  step  815/900 | step_time= 1.70s | loss=6.4719 (cls=1.569, par=4.896, rel=0.006) | elapsed=40.1min\n",
      "  step  820/900 | step_time= 1.85s | loss=6.9665 (cls=1.588, par=5.357, rel=0.021) | elapsed=40.3min\n",
      "  step  825/900 | step_time= 1.93s | loss=6.3660 (cls=1.587, par=4.772, rel=0.007) | elapsed=40.4min\n",
      "  step  830/900 | step_time= 2.04s | loss=6.4464 (cls=1.579, par=4.855, rel=0.012) | elapsed=40.6min\n",
      "  step  835/900 | step_time= 2.37s | loss=6.4108 (cls=1.547, par=4.859, rel=0.004) | elapsed=40.8min\n",
      "  step  840/900 | step_time= 2.40s | loss=6.3866 (cls=1.588, par=4.791, rel=0.007) | elapsed=41.0min\n",
      "  step  845/900 | step_time= 3.81s | loss=6.5987 (cls=1.575, par=5.013, rel=0.011) | elapsed=41.2min\n",
      "  step  850/900 | step_time= 4.17s | loss=6.3040 (cls=1.582, par=4.719, rel=0.003) | elapsed=42.0min\n",
      "  step  855/900 | step_time= 3.67s | loss=6.4062 (cls=1.541, par=4.860, rel=0.005) | elapsed=42.3min\n",
      "  step  860/900 | step_time= 4.08s | loss=6.4237 (cls=1.608, par=4.805, rel=0.010) | elapsed=42.9min\n",
      "  step  865/900 | step_time= 3.40s | loss=6.3194 (cls=1.558, par=4.756, rel=0.005) | elapsed=43.2min\n",
      "  step  870/900 | step_time= 1.10s | loss=6.4943 (cls=1.571, par=4.918, rel=0.006) | elapsed=43.4min\n",
      "  step  875/900 | step_time=51.84s | loss=6.5479 (cls=1.607, par=4.927, rel=0.014) | elapsed=45.7min\n",
      "  step  880/900 | step_time= 4.68s | loss=6.2835 (cls=1.581, par=4.698, rel=0.004) | elapsed=48.1min\n",
      "  step  885/900 | step_time= 6.69s | loss=6.3557 (cls=1.579, par=4.768, rel=0.008) | elapsed=49.3min\n",
      "  step  890/900 | step_time= 5.98s | loss=6.4284 (cls=1.560, par=4.855, rel=0.013) | elapsed=49.8min\n",
      "  step  895/900 | step_time= 4.89s | loss=6.3662 (cls=1.558, par=4.800, rel=0.008) | elapsed=50.3min\n",
      "  step  900/900 | step_time=15.35s | loss=6.2512 (cls=1.574, par=4.673, rel=0.004) | elapsed=51.4min\n",
      "[Train] epoch done in 51.38 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "export -> hrds_runs\\hrds_dsps_fullrun\\export_ep01: 100%|█████████████████████████████| 100/100 [14:55<00:00,  8.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "[BEST] updated: epoch=1, score=-0.897637 -> hrds_runs\\hrds_dsps_fullrun\\best_hrds.pt\n",
      "[Epoch Summary]\n",
      "  train loss(last): 7.06806297885047\n",
      "  metrics: {'num_docs': 100, 'STEDS_strict_mean': -0.8976369367269377, 'STEDS_strict_std': 0.01802730545087131, 'STEDS_strict_median': -0.8993822865937217, 'STEDS_label_mean': -0.8976163181702367, 'STEDS_label_std': 0.018029307272830534, 'STEDS_label_median': -0.8993822865937217, 'cls_acc_mean': 2.0618556701030927e-05, 'parent_acc_mean': 0.012676110928042338, 'rel_acc_mean': 0.96478113879417}\n",
      "  eval_time_sec: 1217.1289012432098\n",
      "\n",
      "========== [HRDS] Epoch 2/2 ==========\n",
      "\n",
      "[Train] start epoch, num_docs = 900\n",
      "  step    1/900 | step_time= 3.35s | loss=6.3079 (cls=1.569, par=4.733, rel=0.005) | elapsed=0.1min\n",
      "    sanity: seq_len=512, avg_parent_candidates=2.0\n",
      "    sanity: seq_len=512, avg_parent_candidates=2.0\n",
      "  step    5/900 | step_time= 2.89s | loss=6.2962 (cls=1.563, par=4.726, rel=0.007) | elapsed=0.2min\n",
      "  step   10/900 | step_time= 2.26s | loss=6.3730 (cls=1.555, par=4.812, rel=0.005) | elapsed=0.5min\n",
      "  step   15/900 | step_time= 2.49s | loss=6.4463 (cls=1.554, par=4.883, rel=0.010) | elapsed=0.7min\n",
      "  step   20/900 | step_time= 2.42s | loss=6.2342 (cls=1.557, par=4.671, rel=0.006) | elapsed=0.9min\n",
      "  step   25/900 | step_time= 2.89s | loss=6.4108 (cls=1.597, par=4.803, rel=0.011) | elapsed=1.1min\n",
      "  step   30/900 | step_time= 2.75s | loss=6.2569 (cls=1.564, par=4.688, rel=0.005) | elapsed=1.3min\n",
      "  step   35/900 | step_time= 2.64s | loss=6.2133 (cls=1.559, par=4.649, rel=0.006) | elapsed=1.6min\n",
      "  step   40/900 | step_time= 2.81s | loss=6.4572 (cls=1.586, par=4.858, rel=0.013) | elapsed=1.8min\n",
      "  step   45/900 | step_time= 2.82s | loss=6.2901 (cls=1.542, par=4.744, rel=0.004) | elapsed=2.0min\n",
      "  step   50/900 | step_time= 2.15s | loss=6.4610 (cls=1.574, par=4.871, rel=0.016) | elapsed=2.2min\n",
      "  step   55/900 | step_time= 2.54s | loss=6.3814 (cls=1.555, par=4.822, rel=0.005) | elapsed=2.4min\n",
      "  step   60/900 | step_time= 2.83s | loss=6.3513 (cls=1.561, par=4.784, rel=0.006) | elapsed=2.7min\n",
      "  step   65/900 | step_time= 2.91s | loss=6.2928 (cls=1.573, par=4.715, rel=0.005) | elapsed=2.9min\n",
      "  step   70/900 | step_time= 2.82s | loss=6.4862 (cls=1.579, par=4.899, rel=0.008) | elapsed=3.1min\n",
      "  step   75/900 | step_time= 3.13s | loss=6.3527 (cls=1.576, par=4.770, rel=0.006) | elapsed=3.4min\n",
      "  step   80/900 | step_time= 3.10s | loss=6.6302 (cls=1.593, par=5.028, rel=0.009) | elapsed=3.6min\n",
      "  step   85/900 | step_time= 2.98s | loss=6.2744 (cls=1.558, par=4.713, rel=0.003) | elapsed=3.9min\n",
      "  step   90/900 | step_time= 3.16s | loss=6.3597 (cls=1.577, par=4.776, rel=0.007) | elapsed=4.1min\n",
      "  step   95/900 | step_time= 2.28s | loss=6.2877 (cls=1.562, par=4.723, rel=0.003) | elapsed=4.4min\n",
      "  step  100/900 | step_time= 2.95s | loss=6.6126 (cls=1.553, par=5.046, rel=0.014) | elapsed=4.6min\n",
      "  step  105/900 | step_time= 2.79s | loss=6.3845 (cls=1.563, par=4.818, rel=0.003) | elapsed=4.8min\n",
      "  step  110/900 | step_time= 3.04s | loss=6.7200 (cls=1.544, par=5.159, rel=0.017) | elapsed=5.1min\n",
      "  step  115/900 | step_time= 2.63s | loss=6.2127 (cls=1.580, par=4.627, rel=0.006) | elapsed=5.3min\n",
      "  step  120/900 | step_time= 3.08s | loss=6.4916 (cls=1.569, par=4.914, rel=0.009) | elapsed=5.6min\n",
      "  step  125/900 | step_time= 2.51s | loss=6.3663 (cls=1.516, par=4.841, rel=0.010) | elapsed=5.8min\n",
      "  step  130/900 | step_time= 2.71s | loss=6.3774 (cls=1.573, par=4.795, rel=0.010) | elapsed=6.0min\n",
      "  step  135/900 | step_time= 3.10s | loss=6.4075 (cls=1.516, par=4.887, rel=0.004) | elapsed=6.3min\n",
      "  step  140/900 | step_time= 2.75s | loss=6.5816 (cls=1.602, par=4.962, rel=0.017) | elapsed=6.5min\n",
      "  step  145/900 | step_time= 3.32s | loss=6.4898 (cls=1.560, par=4.919, rel=0.010) | elapsed=6.8min\n",
      "  step  150/900 | step_time= 2.62s | loss=6.2686 (cls=1.545, par=4.716, rel=0.007) | elapsed=7.0min\n",
      "  step  155/900 | step_time= 2.73s | loss=6.2251 (cls=1.568, par=4.651, rel=0.006) | elapsed=7.2min\n",
      "  step  160/900 | step_time= 2.97s | loss=6.4616 (cls=1.588, par=4.864, rel=0.010) | elapsed=7.5min\n",
      "  step  165/900 | step_time= 3.45s | loss=6.4014 (cls=1.600, par=4.795, rel=0.006) | elapsed=7.7min\n",
      "  step  170/900 | step_time= 2.98s | loss=7.1877 (cls=1.605, par=5.558, rel=0.025) | elapsed=8.0min\n",
      "  step  175/900 | step_time= 2.53s | loss=6.4646 (cls=1.566, par=4.887, rel=0.012) | elapsed=8.2min\n",
      "  step  180/900 | step_time= 3.17s | loss=6.2141 (cls=1.586, par=4.624, rel=0.004) | elapsed=8.4min\n",
      "  step  185/900 | step_time= 3.37s | loss=6.4217 (cls=1.568, par=4.847, rel=0.007) | elapsed=8.7min\n",
      "  step  190/900 | step_time= 3.14s | loss=6.1934 (cls=1.577, par=4.614, rel=0.003) | elapsed=9.0min\n",
      "  step  195/900 | step_time= 3.85s | loss=6.3887 (cls=1.548, par=4.837, rel=0.004) | elapsed=9.3min\n",
      "  step  200/900 | step_time= 2.88s | loss=6.3826 (cls=1.555, par=4.821, rel=0.007) | elapsed=9.5min\n",
      "  step  205/900 | step_time= 2.75s | loss=6.3863 (cls=1.595, par=4.778, rel=0.013) | elapsed=9.8min\n",
      "  step  210/900 | step_time= 3.34s | loss=6.3098 (cls=1.583, par=4.721, rel=0.006) | elapsed=10.0min\n",
      "  step  215/900 | step_time= 2.55s | loss=6.3759 (cls=1.590, par=4.779, rel=0.007) | elapsed=10.2min\n",
      "  step  220/900 | step_time= 2.91s | loss=6.3512 (cls=1.549, par=4.796, rel=0.007) | elapsed=10.5min\n",
      "  step  225/900 | step_time= 2.70s | loss=6.3294 (cls=1.553, par=4.765, rel=0.011) | elapsed=10.8min\n",
      "  step  230/900 | step_time= 2.66s | loss=6.7362 (cls=1.606, par=5.111, rel=0.020) | elapsed=11.0min\n",
      "  step  235/900 | step_time= 3.07s | loss=6.2815 (cls=1.558, par=4.719, rel=0.004) | elapsed=11.2min\n",
      "  step  240/900 | step_time= 3.89s | loss=6.2516 (cls=1.559, par=4.690, rel=0.003) | elapsed=11.5min\n",
      "  step  245/900 | step_time= 2.31s | loss=6.2495 (cls=1.553, par=4.693, rel=0.004) | elapsed=11.7min\n",
      "  step  250/900 | step_time= 2.84s | loss=6.3393 (cls=1.566, par=4.766, rel=0.008) | elapsed=12.0min\n",
      "  step  255/900 | step_time= 3.63s | loss=6.2850 (cls=1.588, par=4.694, rel=0.003) | elapsed=12.2min\n",
      "  step  260/900 | step_time= 3.03s | loss=6.3094 (cls=1.536, par=4.770, rel=0.004) | elapsed=12.5min\n",
      "  step  265/900 | step_time= 2.82s | loss=6.4599 (cls=1.596, par=4.858, rel=0.006) | elapsed=12.7min\n",
      "  step  270/900 | step_time= 2.78s | loss=6.4090 (cls=1.539, par=4.859, rel=0.011) | elapsed=12.9min\n",
      "  step  275/900 | step_time= 3.31s | loss=6.4284 (cls=1.566, par=4.852, rel=0.009) | elapsed=13.2min\n",
      "  step  280/900 | step_time= 2.63s | loss=6.2576 (cls=1.553, par=4.702, rel=0.003) | elapsed=13.4min\n",
      "  step  285/900 | step_time= 2.59s | loss=6.4759 (cls=1.572, par=4.897, rel=0.007) | elapsed=13.7min\n",
      "  step  290/900 | step_time= 3.64s | loss=6.3146 (cls=1.546, par=4.762, rel=0.007) | elapsed=13.9min\n",
      "  step  295/900 | step_time= 3.14s | loss=6.1909 (cls=1.569, par=4.618, rel=0.004) | elapsed=14.2min\n",
      "  step  300/900 | step_time= 2.75s | loss=6.3664 (cls=1.562, par=4.802, rel=0.002) | elapsed=14.4min\n",
      "  step  305/900 | step_time= 2.05s | loss=6.2915 (cls=1.593, par=4.693, rel=0.005) | elapsed=14.6min\n",
      "  step  310/900 | step_time= 2.79s | loss=6.3081 (cls=1.526, par=4.778, rel=0.004) | elapsed=14.8min\n",
      "  step  315/900 | step_time= 2.48s | loss=6.6496 (cls=1.617, par=5.018, rel=0.015) | elapsed=15.0min\n",
      "  step  320/900 | step_time= 3.17s | loss=6.2607 (cls=1.564, par=4.693, rel=0.004) | elapsed=15.3min\n",
      "  step  325/900 | step_time= 2.66s | loss=6.3621 (cls=1.555, par=4.803, rel=0.004) | elapsed=15.5min\n",
      "  step  330/900 | step_time= 2.67s | loss=6.4152 (cls=1.577, par=4.832, rel=0.006) | elapsed=15.7min\n",
      "  step  335/900 | step_time= 2.50s | loss=6.2881 (cls=1.542, par=4.740, rel=0.006) | elapsed=15.9min\n",
      "  step  340/900 | step_time= 2.49s | loss=6.4271 (cls=1.594, par=4.825, rel=0.007) | elapsed=16.1min\n",
      "  step  345/900 | step_time= 9.31s | loss=6.1685 (cls=1.542, par=4.624, rel=0.002) | elapsed=16.9min\n",
      "  step  350/900 | step_time= 1.87s | loss=6.1996 (cls=1.557, par=4.639, rel=0.003) | elapsed=17.1min\n",
      "  step  355/900 | step_time= 2.99s | loss=6.1925 (cls=1.567, par=4.622, rel=0.004) | elapsed=17.3min\n",
      "  step  360/900 | step_time= 2.90s | loss=6.2183 (cls=1.533, par=4.681, rel=0.004) | elapsed=17.6min\n",
      "  step  365/900 | step_time= 2.66s | loss=6.1297 (cls=1.529, par=4.599, rel=0.001) | elapsed=17.8min\n",
      "  step  370/900 | step_time= 2.29s | loss=6.3952 (cls=1.538, par=4.847, rel=0.010) | elapsed=18.0min\n",
      "  step  375/900 | step_time= 2.40s | loss=6.2861 (cls=1.563, par=4.720, rel=0.004) | elapsed=18.2min\n",
      "  step  380/900 | step_time= 2.49s | loss=6.3604 (cls=1.537, par=4.817, rel=0.007) | elapsed=18.4min\n",
      "  step  385/900 | step_time= 2.52s | loss=6.2899 (cls=1.582, par=4.701, rel=0.008) | elapsed=18.6min\n",
      "  step  390/900 | step_time= 2.55s | loss=6.1844 (cls=1.511, par=4.670, rel=0.003) | elapsed=18.8min\n",
      "  step  395/900 | step_time= 2.44s | loss=6.4555 (cls=1.597, par=4.850, rel=0.008) | elapsed=19.0min\n",
      "  step  400/900 | step_time= 2.48s | loss=6.2295 (cls=1.569, par=4.658, rel=0.003) | elapsed=19.2min\n",
      "  step  405/900 | step_time= 2.60s | loss=6.3113 (cls=1.554, par=4.753, rel=0.004) | elapsed=19.4min\n",
      "  step  410/900 | step_time= 2.57s | loss=6.2614 (cls=1.578, par=4.680, rel=0.004) | elapsed=19.6min\n",
      "  step  415/900 | step_time= 1.75s | loss=6.6011 (cls=1.550, par=5.040, rel=0.011) | elapsed=19.8min\n",
      "  step  420/900 | step_time= 1.75s | loss=6.4623 (cls=1.506, par=4.953, rel=0.003) | elapsed=19.9min\n",
      "  step  425/900 | step_time= 2.30s | loss=6.4012 (cls=1.567, par=4.826, rel=0.008) | elapsed=20.1min\n",
      "  step  430/900 | step_time= 3.26s | loss=6.2193 (cls=1.523, par=4.692, rel=0.004) | elapsed=20.4min\n",
      "  step  435/900 | step_time= 2.76s | loss=6.2531 (cls=1.555, par=4.695, rel=0.002) | elapsed=20.6min\n",
      "  step  440/900 | step_time= 2.47s | loss=6.3697 (cls=1.572, par=4.793, rel=0.005) | elapsed=20.8min\n",
      "  step  445/900 | step_time= 1.34s | loss=6.2462 (cls=1.568, par=4.674, rel=0.004) | elapsed=21.0min\n",
      "  step  450/900 | step_time= 1.59s | loss=6.2533 (cls=1.565, par=4.686, rel=0.003) | elapsed=21.1min\n",
      "  step  455/900 | step_time= 1.23s | loss=6.1956 (cls=1.538, par=4.656, rel=0.002) | elapsed=21.2min\n",
      "  step  460/900 | step_time= 1.20s | loss=6.1990 (cls=1.559, par=4.638, rel=0.003) | elapsed=21.3min\n",
      "  step  465/900 | step_time= 1.27s | loss=6.2316 (cls=1.573, par=4.657, rel=0.002) | elapsed=21.4min\n",
      "  step  470/900 | step_time= 1.40s | loss=6.3732 (cls=1.558, par=4.806, rel=0.009) | elapsed=21.5min\n",
      "  step  475/900 | step_time= 2.77s | loss=6.2834 (cls=1.561, par=4.720, rel=0.002) | elapsed=21.7min\n",
      "  step  480/900 | step_time= 2.84s | loss=6.1938 (cls=1.532, par=4.659, rel=0.003) | elapsed=22.0min\n",
      "  step  485/900 | step_time= 3.27s | loss=6.1824 (cls=1.565, par=4.615, rel=0.003) | elapsed=22.3min\n",
      "  step  490/900 | step_time= 2.76s | loss=6.1930 (cls=1.546, par=4.642, rel=0.005) | elapsed=22.5min\n",
      "  step  495/900 | step_time= 3.08s | loss=6.4308 (cls=1.532, par=4.892, rel=0.006) | elapsed=22.8min\n",
      "  step  500/900 | step_time= 2.35s | loss=6.1771 (cls=1.541, par=4.632, rel=0.005) | elapsed=23.0min\n",
      "  step  505/900 | step_time= 2.22s | loss=6.2213 (cls=1.558, par=4.658, rel=0.005) | elapsed=23.3min\n",
      "  step  510/900 | step_time= 1.82s | loss=6.4369 (cls=1.543, par=4.883, rel=0.011) | elapsed=23.5min\n",
      "  step  515/900 | step_time= 1.92s | loss=6.3830 (cls=1.552, par=4.825, rel=0.006) | elapsed=23.6min\n",
      "  step  520/900 | step_time= 1.85s | loss=6.3117 (cls=1.580, par=4.728, rel=0.004) | elapsed=23.9min\n",
      "  step  525/900 | step_time= 1.95s | loss=6.2653 (cls=1.574, par=4.686, rel=0.005) | elapsed=24.3min\n",
      "  step  530/900 | step_time= 1.78s | loss=6.2089 (cls=1.572, par=4.634, rel=0.003) | elapsed=24.4min\n",
      "  step  535/900 | step_time= 1.89s | loss=6.1662 (cls=1.553, par=4.612, rel=0.002) | elapsed=24.6min\n",
      "  step  540/900 | step_time= 1.79s | loss=6.2942 (cls=1.544, par=4.747, rel=0.004) | elapsed=24.7min\n",
      "  step  545/900 | step_time= 1.78s | loss=6.2989 (cls=1.542, par=4.748, rel=0.009) | elapsed=24.9min\n",
      "  step  550/900 | step_time= 1.75s | loss=6.3380 (cls=1.563, par=4.770, rel=0.005) | elapsed=25.0min\n",
      "  step  555/900 | step_time= 1.81s | loss=6.2212 (cls=1.551, par=4.668, rel=0.002) | elapsed=25.2min\n",
      "  step  560/900 | step_time= 1.75s | loss=6.1763 (cls=1.567, par=4.605, rel=0.004) | elapsed=25.4min\n",
      "  step  565/900 | step_time= 1.82s | loss=6.3316 (cls=1.557, par=4.771, rel=0.003) | elapsed=25.5min\n",
      "  step  570/900 | step_time= 1.48s | loss=6.3269 (cls=1.550, par=4.769, rel=0.008) | elapsed=25.6min\n",
      "  step  575/900 | step_time= 1.35s | loss=6.3803 (cls=1.524, par=4.850, rel=0.006) | elapsed=25.8min\n",
      "  step  580/900 | step_time= 1.40s | loss=6.2618 (cls=1.591, par=4.666, rel=0.005) | elapsed=25.9min\n",
      "  step  585/900 | step_time= 1.42s | loss=6.1527 (cls=1.523, par=4.627, rel=0.003) | elapsed=26.0min\n",
      "  step  590/900 | step_time= 1.48s | loss=6.3544 (cls=1.553, par=4.798, rel=0.004) | elapsed=26.1min\n",
      "  step  595/900 | step_time= 1.61s | loss=6.2964 (cls=1.569, par=4.719, rel=0.009) | elapsed=26.2min\n",
      "  step  600/900 | step_time=56.78s | loss=6.1969 (cls=1.524, par=4.671, rel=0.002) | elapsed=27.5min\n",
      "  step  605/900 | step_time= 1.83s | loss=6.6667 (cls=1.603, par=5.047, rel=0.017) | elapsed=27.7min\n",
      "  step  610/900 | step_time= 2.38s | loss=6.1832 (cls=1.535, par=4.644, rel=0.004) | elapsed=27.9min\n",
      "  step  615/900 | step_time= 2.72s | loss=6.3682 (cls=1.561, par=4.801, rel=0.006) | elapsed=28.1min\n",
      "  step  620/900 | step_time= 2.72s | loss=6.2481 (cls=1.539, par=4.702, rel=0.006) | elapsed=28.4min\n",
      "  step  625/900 | step_time= 3.14s | loss=6.2021 (cls=1.558, par=4.640, rel=0.003) | elapsed=28.6min\n",
      "  step  630/900 | step_time= 3.66s | loss=6.2341 (cls=1.548, par=4.682, rel=0.004) | elapsed=28.9min\n",
      "  step  635/900 | step_time= 3.47s | loss=6.4137 (cls=1.570, par=4.833, rel=0.011) | elapsed=29.2min\n",
      "  step  640/900 | step_time= 3.40s | loss=6.4145 (cls=1.591, par=4.816, rel=0.007) | elapsed=29.5min\n",
      "  step  645/900 | step_time= 3.48s | loss=6.2080 (cls=1.560, par=4.646, rel=0.002) | elapsed=29.7min\n",
      "  step  650/900 | step_time= 3.49s | loss=6.2636 (cls=1.537, par=4.722, rel=0.005) | elapsed=30.1min\n",
      "  step  655/900 | step_time= 2.35s | loss=6.2284 (cls=1.527, par=4.698, rel=0.003) | elapsed=30.9min\n",
      "  step  660/900 | step_time= 3.19s | loss=6.3760 (cls=1.566, par=4.798, rel=0.011) | elapsed=31.1min\n",
      "  step  665/900 | step_time= 2.76s | loss=6.1745 (cls=1.534, par=4.637, rel=0.004) | elapsed=31.4min\n",
      "  step  670/900 | step_time= 1.97s | loss=6.2069 (cls=1.582, par=4.624, rel=0.001) | elapsed=31.7min\n",
      "  step  675/900 | step_time= 5.44s | loss=7.1413 (cls=1.611, par=5.501, rel=0.029) | elapsed=31.9min\n",
      "  step  680/900 | step_time= 2.82s | loss=6.2273 (cls=1.570, par=4.653, rel=0.004) | elapsed=32.1min\n",
      "  step  685/900 | step_time= 2.53s | loss=6.2105 (cls=1.529, par=4.678, rel=0.003) | elapsed=32.4min\n",
      "  step  690/900 | step_time=21.79s | loss=6.2288 (cls=1.544, par=4.683, rel=0.002) | elapsed=33.0min\n",
      "  step  695/900 | step_time= 3.57s | loss=6.3759 (cls=1.532, par=4.835, rel=0.010) | elapsed=33.4min\n",
      "  step  700/900 | step_time= 2.77s | loss=6.1782 (cls=1.542, par=4.634, rel=0.002) | elapsed=33.9min\n",
      "  step  705/900 | step_time= 3.02s | loss=6.7053 (cls=1.579, par=5.109, rel=0.017) | elapsed=34.2min\n",
      "  step  710/900 | step_time=14.63s | loss=6.1846 (cls=1.530, par=4.649, rel=0.006) | elapsed=34.7min\n",
      "  step  715/900 | step_time=24.55s | loss=6.2587 (cls=1.560, par=4.694, rel=0.005) | elapsed=35.7min\n",
      "  step  720/900 | step_time= 4.78s | loss=6.1576 (cls=1.547, par=4.609, rel=0.002) | elapsed=36.2min\n",
      "  step  725/900 | step_time= 3.30s | loss=6.3028 (cls=1.552, par=4.748, rel=0.002) | elapsed=36.7min\n",
      "  step  730/900 | step_time= 4.92s | loss=6.2366 (cls=1.550, par=4.682, rel=0.005) | elapsed=37.2min\n",
      "  step  735/900 | step_time= 4.70s | loss=6.3217 (cls=1.543, par=4.772, rel=0.007) | elapsed=38.4min\n",
      "  step  740/900 | step_time= 8.24s | loss=6.1708 (cls=1.530, par=4.638, rel=0.003) | elapsed=38.9min\n",
      "  step  745/900 | step_time= 3.32s | loss=6.2994 (cls=1.545, par=4.750, rel=0.005) | elapsed=39.8min\n",
      "  step  750/900 | step_time= 1.75s | loss=7.5438 (cls=1.599, par=5.903, rel=0.042) | elapsed=40.2min\n",
      "  step  755/900 | step_time=16.87s | loss=6.2332 (cls=1.543, par=4.686, rel=0.004) | elapsed=40.7min\n",
      "  step  760/900 | step_time= 2.53s | loss=6.2987 (cls=1.568, par=4.725, rel=0.006) | elapsed=41.4min\n",
      "  step  765/900 | step_time=16.76s | loss=6.1986 (cls=1.536, par=4.657, rel=0.005) | elapsed=41.8min\n",
      "  step  770/900 | step_time= 2.29s | loss=6.4394 (cls=1.530, par=4.902, rel=0.008) | elapsed=43.2min\n",
      "  step  775/900 | step_time= 2.50s | loss=6.2766 (cls=1.549, par=4.719, rel=0.009) | elapsed=43.4min\n",
      "  step  780/900 | step_time=28.64s | loss=6.4716 (cls=1.530, par=4.931, rel=0.010) | elapsed=44.8min\n",
      "  step  785/900 | step_time= 9.04s | loss=6.2071 (cls=1.578, par=4.626, rel=0.003) | elapsed=46.4min\n",
      "  step  790/900 | step_time= 2.49s | loss=6.2535 (cls=1.543, par=4.706, rel=0.004) | elapsed=47.6min\n",
      "  step  795/900 | step_time= 1.38s | loss=6.2172 (cls=1.554, par=4.661, rel=0.002) | elapsed=48.6min\n",
      "  step  800/900 | step_time= 2.95s | loss=6.2808 (cls=1.564, par=4.707, rel=0.010) | elapsed=52.4min\n",
      "  step  805/900 | step_time= 2.55s | loss=6.1754 (cls=1.565, par=4.608, rel=0.002) | elapsed=52.6min\n",
      "  step  810/900 | step_time= 2.51s | loss=6.2259 (cls=1.549, par=4.674, rel=0.003) | elapsed=52.8min\n",
      "  step  815/900 | step_time= 3.12s | loss=6.2373 (cls=1.543, par=4.689, rel=0.005) | elapsed=53.1min\n",
      "  step  820/900 | step_time= 2.77s | loss=6.3035 (cls=1.566, par=4.734, rel=0.004) | elapsed=53.3min\n",
      "  step  825/900 | step_time= 2.99s | loss=6.1807 (cls=1.565, par=4.611, rel=0.005) | elapsed=53.6min\n",
      "  step  830/900 | step_time= 2.82s | loss=6.2712 (cls=1.547, par=4.718, rel=0.006) | elapsed=53.8min\n",
      "  step  835/900 | step_time= 2.92s | loss=6.3647 (cls=1.539, par=4.817, rel=0.008) | elapsed=54.0min\n",
      "  step  840/900 | step_time= 2.45s | loss=6.2788 (cls=1.545, par=4.723, rel=0.011) | elapsed=54.3min\n",
      "  step  845/900 | step_time= 2.64s | loss=6.2268 (cls=1.537, par=4.688, rel=0.002) | elapsed=54.5min\n",
      "  step  850/900 | step_time= 2.25s | loss=6.3435 (cls=1.554, par=4.783, rel=0.007) | elapsed=54.7min\n",
      "  step  855/900 | step_time= 2.58s | loss=6.4301 (cls=1.580, par=4.842, rel=0.008) | elapsed=54.9min\n",
      "  step  860/900 | step_time= 2.82s | loss=6.1643 (cls=1.529, par=4.632, rel=0.003) | elapsed=55.1min\n",
      "  step  865/900 | step_time= 2.57s | loss=6.2373 (cls=1.563, par=4.671, rel=0.003) | elapsed=55.3min\n",
      "  step  870/900 | step_time= 2.57s | loss=6.2757 (cls=1.591, par=4.677, rel=0.007) | elapsed=55.5min\n",
      "  step  875/900 | step_time= 2.92s | loss=6.2522 (cls=1.540, par=4.710, rel=0.002) | elapsed=55.8min\n",
      "  step  880/900 | step_time= 2.46s | loss=6.2207 (cls=1.571, par=4.645, rel=0.004) | elapsed=56.0min\n",
      "  step  885/900 | step_time= 2.52s | loss=6.2234 (cls=1.572, par=4.649, rel=0.003) | elapsed=56.2min\n",
      "  step  890/900 | step_time= 2.05s | loss=6.1270 (cls=1.580, par=4.546, rel=0.001) | elapsed=56.4min\n",
      "  step  895/900 | step_time= 2.63s | loss=6.1940 (cls=1.565, par=4.625, rel=0.004) | elapsed=56.6min\n",
      "  step  900/900 | step_time= 2.30s | loss=6.3558 (cls=1.547, par=4.799, rel=0.010) | elapsed=56.9min\n",
      "[Train] epoch done in 56.86 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "export -> hrds_runs\\hrds_dsps_fullrun\\export_ep02: 100%|█████████████████████████████| 100/100 [01:41<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomra\\AppData\\Local\\Temp\\ipykernel_8292\\3028038573.py:355: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(CKPT_BEST, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BEST] updated: epoch=2, score=-0.897487 -> hrds_runs\\hrds_dsps_fullrun\\best_hrds.pt\n",
      "[Epoch Summary]\n",
      "  train loss(last): 6.333203865687053\n",
      "  metrics: {'num_docs': 100, 'STEDS_strict_mean': -0.8974870466097591, 'STEDS_strict_std': 0.018143019926097962, 'STEDS_strict_median': -0.8993822865937217, 'STEDS_label_mean': -0.8974870466097591, 'STEDS_label_std': 0.018143019926097962, 'STEDS_label_median': -0.8993822865937217, 'cls_acc_mean': 0.00014989011717859245, 'parent_acc_mean': 0.012676110928042338, 'rel_acc_mean': 0.9747978463365711}\n",
      "  eval_time_sec: 436.40010809898376\n",
      "\n",
      "========== HRDS Full Flow DONE ==========\n",
      "[BEST] epoch = 2 macro_strict = -0.8974870466097591\n",
      "[BEST CKPT] hrds_runs\\hrds_dsps_fullrun\\best_hrds.pt\n",
      "[LOG JSON ] hrds_runs\\hrds_dsps_fullrun\\train_eval_log.json\n",
      "\n",
      "Paper reference (Table 2 HRDS best, Document+Semantic+Vision+Soft-mask):\n",
      "Micro-STEDS ≈ 0.8143\n",
      "Macro-STEDS ≈ 0.8174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "export -> hrds_runs\\hrds_dsps_fullrun\\export_best_final: 100%|███████████████████████| 100/100 [05:08<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "\n",
      "===== HRDS Final Validation =====\n",
      "Final export : hrds_runs\\hrds_dsps_fullrun\\export_best_final\n",
      "Final report : final_reports\\hrds_dsps_fullrun_HRDS_report.json\n",
      "Final metrics: {'num_docs': 100, 'STEDS_strict_mean': -0.8975058299566281, 'STEDS_strict_std': 0.018212007788965538, 'STEDS_strict_median': -0.8993822865937217, 'STEDS_label_mean': -0.897485749635343, 'STEDS_label_std': 0.01821301050286327, 'STEDS_label_median': -0.8993822865937217, 'cls_acc_mean': 0.0001311067703095378, 'parent_acc_mean': 0.012676110928042338, 'rel_acc_mean': 0.9749736378061659}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# HRDS 全流程：train -> best ckpt -> export -> eval\n",
    "# 依赖你 notebook 已有定义（Run All 到此处后再运行本 cell）：\n",
    "#   HRDHDataset, collate_doc\n",
    "#   compute_M_cp_from_dataset\n",
    "#   DSPSModel\n",
    "#   FocalLoss\n",
    "#   train_one_epoch\n",
    "#   export_split_predictions\n",
    "#   eval_export_dir_dual\n",
    "# =========================\n",
    "\n",
    "import os, json, time, shutil, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --------- A) 你必须改的路径：HRDS_ROOT ----------\n",
    "HRDS_ROOT = r\"C:\\Users\\tomra\\Desktop\\PAPER\\final OBJ\\HRDS\"   # 改成你 HRDoc-Simple 根目录\n",
    "\n",
    "# --------- B) 实验输出目录 ----------\n",
    "EXP_NAME = \"hrds_dsps_fullrun\"\n",
    "RUN_ROOT = \"hrds_runs\"\n",
    "EXP_DIR = os.path.join(RUN_ROOT, EXP_NAME)\n",
    "os.makedirs(EXP_DIR, exist_ok=True)\n",
    "\n",
    "# --------- C) 基础对象检查：确保你前面 cell 已经定义完 ----------\n",
    "required = [\n",
    "    \"HRDHDataset\", \"collate_doc\",\n",
    "    \"compute_M_cp_from_dataset\",\n",
    "    \"DSPSModel\",\n",
    "    \"FocalLoss\",\n",
    "    \"train_one_epoch\",\n",
    "    \"export_split_predictions\",\n",
    "    \"eval_export_dir_dual\",\n",
    "    \"cfg\",\n",
    "]\n",
    "missing = [n for n in required if n not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        \"你还没有运行到包含以下定义的 cell（或变量名不同）：\\n\"\n",
    "        + \"\\n\".join(missing)\n",
    "        + \"\\n\\n请先 Run All 直到模型/数据集/评估函数都定义完，再运行本 cell。\"\n",
    "    )\n",
    "\n",
    "if not os.path.isdir(HRDS_ROOT):\n",
    "    raise FileNotFoundError(f\"HRDS_ROOT 不存在：{HRDS_ROOT}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[Device]\", device)\n",
    "print(\"[EXP_DIR]\", EXP_DIR)\n",
    "\n",
    "# --------- D) 固定随机种子（保持复现稳定） ----------\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# --------- E) 构建 HRDS 数据集与 dataloader ----------\n",
    "train_ds = HRDHDataset(HRDS_ROOT, split=\"train\", max_len=cfg.max_len)\n",
    "test_ds  = HRDHDataset(HRDS_ROOT, split=\"test\",  max_len=cfg.max_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=1,         \n",
    "    shuffle=True,\n",
    "    num_workers=getattr(cfg, \"num_workers\", 0),\n",
    "    collate_fn=collate_doc,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=getattr(cfg, \"num_workers\", 0),\n",
    "    collate_fn=collate_doc,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "print(f\"[HRDS] train docs = {len(train_loader)}, test docs = {len(test_loader)}\")\n",
    "\n",
    "# --------- F) 类别/关系维度（来自你 dataset 定义） ----------\n",
    "num_classes = len(ID2LABEL_14)\n",
    "num_rel = len(REL2ID)\n",
    "print(\"[Dims] num_classes =\", num_classes, \"num_rel =\", num_rel)\n",
    "\n",
    "# --------- G) 计算/加载 M_cp（soft-mask 先验） ----------\n",
    "MCP_PATH = os.path.join(EXP_DIR, \"M_cp_hrds.npy\")\n",
    "if os.path.exists(MCP_PATH):\n",
    "    M_cp = np.asarray(np.load(MCP_PATH))\n",
    "    print(\"[M_cp] loaded:\", MCP_PATH, \"shape=\", M_cp.shape)\n",
    "else:\n",
    "    print(\"[M_cp] computing from HRDS train ...\")\n",
    "    ret = compute_M_cp_from_dataset(train_ds, num_classes=num_classes, pseudo_count=5.0)\n",
    "\n",
    "    # 兼容：函数可能返回 (M_cp, stats) 或 {\"M_cp\":..., ...} 或直接 ndarray\n",
    "    if isinstance(ret, tuple):\n",
    "        M_cp = ret[0]\n",
    "        mcp_extra = ret[1:]\n",
    "    elif isinstance(ret, dict):\n",
    "        M_cp = ret.get(\"M_cp\", None)\n",
    "        mcp_extra = {k:v for k,v in ret.items() if k != \"M_cp\"}\n",
    "        if M_cp is None:\n",
    "            raise RuntimeError(\"compute_M_cp_from_dataset 返回 dict，但不包含键 'M_cp'\")\n",
    "    else:\n",
    "        M_cp = ret\n",
    "        mcp_extra = None\n",
    "\n",
    "    M_cp = np.asarray(M_cp)\n",
    "    np.save(MCP_PATH, M_cp)\n",
    "    print(\"[M_cp] saved:\", MCP_PATH, \"shape=\", M_cp.shape)\n",
    "\n",
    "    # 可选：把额外统计信息也存下来（不影响训练）\n",
    "    if mcp_extra is not None:\n",
    "        extra_path = os.path.join(EXP_DIR, \"M_cp_hrds_extra.json\")\n",
    "        try:\n",
    "            with open(extra_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(mcp_extra, f, ensure_ascii=False, indent=2, default=str)\n",
    "            print(\"[M_cp] extra saved:\", extra_path)\n",
    "        except Exception as e:\n",
    "            print(\"[M_cp] extra save skipped:\", repr(e))\n",
    "\n",
    "# --------- H) 建模 ----------\n",
    "USE_TEXT = False\n",
    "USE_VISUAL = False\n",
    "USE_SOFTMASK = True\n",
    "\n",
    "model = DSPSModel(\n",
    "    num_classes=num_classes,\n",
    "    num_rel=num_rel,\n",
    "    M_cp=M_cp,\n",
    "    cfg=cfg,\n",
    "    use_text=USE_TEXT,\n",
    "    use_visual=USE_VISUAL,\n",
    "    use_softmask=USE_SOFTMASK,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=getattr(cfg, \"lr\", 2e-4),\n",
    "    weight_decay=getattr(cfg, \"weight_decay\", 1e-2),\n",
    ")\n",
    "\n",
    "focal_cls = FocalLoss(gamma=getattr(cfg, \"focal_gamma\", 2.0), alpha=getattr(cfg, \"focal_alpha\", 0.25)).to(device)\n",
    "focal_rel = FocalLoss(gamma=getattr(cfg, \"focal_gamma\", 2.0), alpha=getattr(cfg, \"focal_alpha\", 0.25)).to(device)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# [关键整合修正] 安全推理函数：保证 export_tree_json 不再炸\n",
    "# ---------------------------------------------------------\n",
    "def _install_safe_predict_doc_with_rel_recompute():\n",
    "    import torch\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _safe(model, doc, device):\n",
    "        out = model(doc)\n",
    "\n",
    "        # 必需：cls_logits / par_logits\n",
    "        cls_logits = out[\"cls_logits\"]   # (L,C)\n",
    "        par_logits = out[\"par_logits\"]   # list-like\n",
    "        L = int(cls_logits.size(0))\n",
    "\n",
    "        pred_cls = torch.argmax(cls_logits, dim=-1).detach().cpu().tolist()\n",
    "\n",
    "        # parents: argmax + clamp\n",
    "        pred_parent = [-1] * L\n",
    "        for i in range(L):\n",
    "            if i == 0:\n",
    "                pred_parent[i] = -1\n",
    "                continue\n",
    "            logits_i = par_logits[i].view(-1)\n",
    "            p = int(torch.argmax(logits_i).item())\n",
    "            pred_parent[i] = p\n",
    "\n",
    "        # clamp parents 防越界/自指/指向未来\n",
    "        for i in range(L):\n",
    "            p = pred_parent[i]\n",
    "            if (p is None) or (not isinstance(p, int)) or (p < 0) or (p >= L) or (p == i) or (p >= i):\n",
    "                pred_parent[i] = -1\n",
    "\n",
    "        # rel logits：优先重算；若找不到 hidden seq 则回退 out[\"rel_logits\"]；否则全零\n",
    "        rel_logits = out.get(\"rel_logits\", None)\n",
    "\n",
    "        # 尝试找 hidden sequence (L,D) —— 不硬编码键名，按形状挑一个\n",
    "        h_seq = None\n",
    "        for k, v in out.items():\n",
    "            if torch.is_tensor(v) and v.dim() == 2 and v.size(0) == L:\n",
    "                if k in (\"cls_logits\", \"rel_logits\"):\n",
    "                    continue\n",
    "                h_seq = v\n",
    "                break\n",
    "\n",
    "        root = out.get(\"root\", None)\n",
    "        if root is None and h_seq is not None:\n",
    "            root = h_seq[0:1]  # (1,D)\n",
    "\n",
    "        if (h_seq is not None) and hasattr(model, \"rel_head\"):\n",
    "            root_vec = root.squeeze(0) if (torch.is_tensor(root) and root.dim() == 2) else root\n",
    "            rel_list = []\n",
    "            for i in range(L):\n",
    "                p = pred_parent[i]\n",
    "                parent_vec = root_vec if (p < 0 or p >= L) else h_seq[p]\n",
    "                feat = torch.cat([h_seq[i], parent_vec], dim=-1)\n",
    "                rel_list.append(model.rel_head(feat))\n",
    "            rel_logits = torch.stack(rel_list, dim=0)\n",
    "\n",
    "        if rel_logits is None:\n",
    "            R = getattr(model, \"num_rel\", num_rel)\n",
    "            rel_logits = torch.zeros((L, R), device=cls_logits.device)\n",
    "\n",
    "        pred_rel = torch.argmax(rel_logits, dim=-1).detach().cpu().tolist()\n",
    "\n",
    "        # 关键：键名必须匹配 export_tree_json\n",
    "        return {\"pred_cls\": pred_cls, \"pred_parent\": pred_parent, \"pred_rel\": pred_rel}\n",
    "\n",
    "    # 覆写全局函数名，供 export_split_predictions 调用\n",
    "    globals()[\"predict_doc_with_rel_recompute\"] = _safe\n",
    "\n",
    "\n",
    "# --------- I) 训练 + 每 epoch 验证 + 保存 best ----------\n",
    "CKPT_BEST = os.path.join(EXP_DIR, \"best_hrds.pt\")\n",
    "REPORT_PATH = os.path.join(EXP_DIR, \"train_eval_log.json\")\n",
    "\n",
    "history = []\n",
    "best_score = -1.0\n",
    "best_epoch = -1\n",
    "\n",
    "def _extract_macro_strict(metrics_dict):\n",
    "    if not isinstance(metrics_dict, dict):\n",
    "        return None\n",
    "    candidates = [\n",
    "        \"steds_strict_macro\", \"macro_steds_strict\", \"steds_macro_strict\",\n",
    "        \"steds_strict_mean\",  \"strict_mean\",        \"steds_strict\",\n",
    "    ]\n",
    "    for k in candidates:\n",
    "        if k in metrics_dict and isinstance(metrics_dict[k], (int, float)):\n",
    "            return float(metrics_dict[k])\n",
    "    for k, v in metrics_dict.items():\n",
    "        if \"strict\" in k.lower() and isinstance(v, (int, float)):\n",
    "            return float(v)\n",
    "    return None\n",
    "\n",
    "for ep in range(1, cfg.epochs + 1):\n",
    "    print(f\"\\n========== [HRDS] Epoch {ep}/{cfg.epochs} ==========\")\n",
    "\n",
    "    # 1) train\n",
    "    tr_logs = train_one_epoch(model, train_loader, optimizer, cfg, focal_cls, focal_rel)\n",
    "\n",
    "    # 2) export + eval on test\n",
    "    export_dir = os.path.join(EXP_DIR, f\"export_ep{ep:02d}\")\n",
    "    if os.path.isdir(export_dir):\n",
    "        shutil.rmtree(export_dir, ignore_errors=True)\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "    # [关键] 每次导出前安装安全推理函数，避免旧定义干扰\n",
    "    _install_safe_predict_doc_with_rel_recompute()\n",
    "\n",
    "    t0 = time.time()\n",
    "    metrics = None\n",
    "    try:\n",
    "        export_split_predictions(model, test_loader, save_dir=export_dir, device=device)\n",
    "        metrics = eval_export_dir_dual(export_dir, exclude_meta=True)\n",
    "    except Exception as e:\n",
    "        # 不中断训练：记录错误，继续下一个 epoch\n",
    "        print(\"[WARN] export/eval failed at epoch\", ep, \"error:\", repr(e))\n",
    "        metrics = {\"_error\": repr(e)}\n",
    "    eval_time = time.time() - t0\n",
    "\n",
    "    # 3) best selection（以 Macro Strict STEDS 为主）\n",
    "    score = _extract_macro_strict(metrics) if isinstance(metrics, dict) else None\n",
    "    if score is None:\n",
    "        print(\"[WARN] 无法从 metrics 中提取 strict 指标字段，本 epoch 不更新 best。metrics=\", metrics)\n",
    "        score = -1.0\n",
    "\n",
    "    is_best = score > best_score\n",
    "    if is_best:\n",
    "        best_score = score\n",
    "        best_epoch = ep\n",
    "        torch.save({\"model\": model.state_dict(), \"epoch\": ep, \"metrics\": metrics}, CKPT_BEST)\n",
    "        print(f\"[BEST] updated: epoch={ep}, score={best_score:.6f} -> {CKPT_BEST}\")\n",
    "\n",
    "    row = {\n",
    "        \"epoch\": ep,\n",
    "        \"train\": tr_logs,\n",
    "        \"metrics\": metrics,\n",
    "        \"macro_strict_for_select\": score,\n",
    "        \"eval_time_sec\": eval_time,\n",
    "        \"is_best\": is_best,\n",
    "    }\n",
    "    history.append(row)\n",
    "\n",
    "    with open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"exp_name\": EXP_NAME,\n",
    "                \"hrds_root\": HRDS_ROOT,\n",
    "                \"cfg\": cfg.__dict__ if hasattr(cfg, \"__dict__\") else str(cfg),\n",
    "                \"use_text\": USE_TEXT,\n",
    "                \"use_visual\": USE_VISUAL,\n",
    "                \"use_softmask\": USE_SOFTMASK,\n",
    "                \"best_epoch\": best_epoch,\n",
    "                \"best_score_macro_strict\": best_score,\n",
    "                \"best_ckpt\": CKPT_BEST,\n",
    "                \"history\": history,\n",
    "            },\n",
    "            f,\n",
    "            ensure_ascii=False,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "    print(\"[Epoch Summary]\")\n",
    "    # 兼容：loss 可能是 float / list / numpy / tensor\n",
    "    loss_last = None\n",
    "    if isinstance(tr_logs, dict) and \"loss\" in tr_logs:\n",
    "        v = tr_logs[\"loss\"]\n",
    "        if isinstance(v, (list, tuple)):\n",
    "            loss_last = v[-1] if len(v) else None\n",
    "        elif torch.is_tensor(v):\n",
    "            loss_last = float(v.detach().cpu().item()) if v.numel() == 1 else float(v.detach().cpu().mean().item())\n",
    "        else:\n",
    "            # float / int / numpy scalar\n",
    "            try:\n",
    "                loss_last = float(v)\n",
    "            except Exception:\n",
    "                loss_last = v\n",
    "    else:\n",
    "            loss_last = tr_logs\n",
    "\n",
    "    print(\"  train loss(last):\", loss_last)\n",
    "\n",
    "    print(\"  metrics:\", metrics)\n",
    "    print(\"  eval_time_sec:\", eval_time)\n",
    "\n",
    "print(\"\\n========== HRDS Full Flow DONE ==========\")\n",
    "print(\"[BEST] epoch =\", best_epoch, \"macro_strict =\", best_score)\n",
    "print(\"[BEST CKPT]\", CKPT_BEST)\n",
    "print(\"[LOG JSON ]\", REPORT_PATH)\n",
    "\n",
    "print(\"\\nPaper reference (Table 2 HRDS best, Document+Semantic+Vision+Soft-mask):\")\n",
    "print(\"Micro-STEDS ≈ 0.8143\")\n",
    "print(\"Macro-STEDS ≈ 0.8174\")\n",
    "\n",
    "# --------- J) 用 best ckpt 再导出一次（最终留档） ----------\n",
    "FINAL_EXPORT = os.path.join(EXP_DIR, \"export_best_final\")\n",
    "if os.path.isdir(FINAL_EXPORT):\n",
    "    shutil.rmtree(FINAL_EXPORT, ignore_errors=True)\n",
    "os.makedirs(FINAL_EXPORT, exist_ok=True)\n",
    "\n",
    "_install_safe_predict_doc_with_rel_recompute()\n",
    "\n",
    "if os.path.exists(CKPT_BEST):\n",
    "    ckpt = torch.load(CKPT_BEST, map_location=\"cpu\")\n",
    "    state = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else ckpt\n",
    "    model.load_state_dict(state, strict=False)\n",
    "    model.eval()\n",
    "\n",
    "export_split_predictions(model, test_loader, save_dir=FINAL_EXPORT, device=device)\n",
    "final_metrics = eval_export_dir_dual(FINAL_EXPORT, exclude_meta=True)\n",
    "\n",
    "FINAL_REPORT = os.path.join(\"final_reports\", f\"{EXP_NAME}_HRDS_report.json\")\n",
    "os.makedirs(\"final_reports\", exist_ok=True)\n",
    "with open(FINAL_REPORT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"dataset\": \"HRDoc-Simple (HRDS)\",\n",
    "            \"hrds_root\": HRDS_ROOT,\n",
    "            \"best_ckpt\": CKPT_BEST,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"best_score_macro_strict\": best_score,\n",
    "            \"final_export_dir\": FINAL_EXPORT,\n",
    "            \"final_metrics\": final_metrics,\n",
    "            \"log_path\": REPORT_PATH,\n",
    "        },\n",
    "        f,\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "print(\"\\n===== HRDS Final Validation =====\")\n",
    "print(\"Final export :\", FINAL_EXPORT)\n",
    "print(\"Final report :\", FINAL_REPORT)\n",
    "print(\"Final metrics:\", final_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb11f2-790e-4be1-9a56-037ebc0d3c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hrdoc)",
   "language": "python",
   "name": "hrdoc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
