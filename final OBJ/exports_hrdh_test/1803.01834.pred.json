{
  "doc_id": "1803.01834",
  "nodes": [
    {
      "id": 0,
      "text": "Conducting Credit Assignment by Aligning Lo cal Distributed",
      "label_id": 0,
      "label_name": "Title",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 1,
      "text": "Representations",
      "label_id": 0,
      "label_name": "Title",
      "is_meta": true,
      "parent": 0,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 2,
      "text": "Alexander G. Ororbia",
      "label_id": 1,
      "label_name": "Author",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 3,
      "text": "ago109@psu.edu",
      "label_id": 3,
      "label_name": "Affiliation",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 4,
      "text": "Col lege of Information & Sciences Technology",
      "label_id": 3,
      "label_name": "Affiliation",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 5,
      "text": "Penn State University",
      "label_id": 3,
      "label_name": "Affiliation",
      "is_meta": true,
      "parent": 4,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 6,
      "text": "University Park, PA 16802, USA",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 7,
      "text": "Ankur Mali",
      "label_id": 3,
      "label_name": "Affiliation",
      "is_meta": true,
      "parent": 6,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 8,
      "text": "aam35@psu.edu",
      "label_id": 3,
      "label_name": "Affiliation",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 9,
      "text": "Col lege of Information & Sciences Technology",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 10,
      "text": "Penn State University",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 9,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 11,
      "text": "University Park, PA 16802, USA",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 12,
      "text": "Daniel Kifer",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 13,
      "text": "dkifer@cse.psu.edu",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 14,
      "text": "School of Electrical Engineering and Computer Science",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 13,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 15,
      "text": "Penn State University",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 14,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 16,
      "text": "University Park, PA 16802, USA",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 13,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 17,
      "text": "C. Lee Giles",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 16,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 18,
      "text": "giles@ist.psu.edu",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 17,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 19,
      "text": "Col lege of Information & Sciences Technology",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 18,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 20,
      "text": "Penn State University",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 19,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 21,
      "text": "University Park, PA 16802, USA",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 19,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 22,
      "text": "Editor:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 21,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 23,
      "text": "Abstract",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 21,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 24,
      "text": "Using back-propagation and its variants to train deep networks is often problematic for new",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 23,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 25,
      "text": "users. Issues such as exploding gradients, vanishing gradients, and high sensitivity to weight",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 24,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 26,
      "text": "initialization strategies often make networks difficult to train, esp ecially when users are",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 25,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 27,
      "text": "experimenting with new architectures. Here, we present Local Representation Alignment",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 26,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 28,
      "text": "(LRA), a training procedure that is much less sensitive to bad initializations, does not require",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 27,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 29,
      "text": "modifications to the network architecture, and can be adapted to networks with highly",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 24,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 30,
      "text": "nonlinear and discrete-valued activation functions. Furthermore, we show that one variation",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 29,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 31,
      "text": "of LRA can start with a null initialization of network weights and still successfully train",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 30,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 32,
      "text": "networks with a wide variety of nonlinearities, including tanh, ReLU-6, softplus, signum",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 31,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 33,
      "text": "and others that may draw their inspiration from biology.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 32,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 34,
      "text": "A comprehensive set of experiments on MNIST and the much harder Fashion MNIST",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 21,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 35,
      "text": "data sets show that LRA can be used to train networks robustly and effectively, succeeding",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 34,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 36,
      "text": "even when back-propagation fails and outperforming other alternative learning algorithms,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 35,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 37,
      "text": "such as target propagation and feedback alignment.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 36,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 38,
      "text": "Keywords: learning algorithm, artificial neural networks, credit assignment, representation",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 24,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 39,
      "text": "learning, local learning",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 38,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 40,
      "text": "1. Intro duction",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 21,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 41,
      "text": "In artificial neural networks, credit assignment is the task of computing the contribution to",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 40,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 42,
      "text": "the overall error caused by individual units in the network, and then using this information",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 41,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 43,
      "text": "©XXXX Alexander G. Ororbia, Ankur Mali, Daniel Kifer, and C. Lee Giles.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 42,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 44,
      "text": "License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 41,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 45,
      "text": "http://jmlr.org/papers/vX/ago109.html.",
      "label_id": 13,
      "label_name": "Footnote",
      "is_meta": true,
      "parent": 44,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 46,
      "text": "Ororbia, Mali, Kifer, and Giles",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 47,
      "text": "to up date the parameters of the entire network. Credit assignment and updates are most",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 41,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 48,
      "text": "often done with the help of the gradient calculated by the well-known back-propagation",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 47,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 49,
      "text": "of errors (Rumelhart et al., 1988),1 which provides a theoretical basis for training deep",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 48,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 50,
      "text": "networks (i.e. gradient descent) but also presents challenges in practice due to the known",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 49,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 51,
      "text": "vanishing/explo ding gradient and shrinking variance problems.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 50,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 52,
      "text": "The most common strategies for dealing with such problems include (1) a careful",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 41,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 53,
      "text": "initialization of the network parameters, often following from a network-specific analysis",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 52,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 54,
      "text": "of the learning dynamics caused by back-propagation (Glorot and Bengio, 2010; He et al.,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 53,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 55,
      "text": "2015; Sussillo, 2014; Mishkin and Matas, 2015), and (2) modifying the network structure,",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 54,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 56,
      "text": "for example by using ReLU instead of sigmoid activations or adding batch normalization",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 55,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 57,
      "text": "layers Ioffe and Szegedy (2015). Challenges such as these are often a barrier for new users",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 56,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 58,
      "text": "to training deep networks to accuracies that approach the state of the art.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 57,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 59,
      "text": "In this paper, we present a novel training algorithm, called Local Representation Align-",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 52,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 60,
      "text": "ment (LRA), that is robust to p oor choices of initialization and can train deep networks",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 59,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 61,
      "text": "from initializations that would cause backprop to fail. This allows network designers to",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 60,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 62,
      "text": "choose units, including non-differentiable ones, based on the type of representation they",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 61,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 63,
      "text": "provide rather than on the ideosyncrasies of backprop-based algorithms.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 62,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 64,
      "text": "The idea b ehind LRA is that every layer, not just the output layer, has a target and each",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 59,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 65,
      "text": "layer’s weights are adjusted so that its output moves closer to its target. While this idea is",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 64,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 66,
      "text": "common to prior work such as TargetProp (Carreira-Perpi˜n´an and Wang, 2012; Bengio, 2014;",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 65,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 67,
      "text": "Lee et al., 2015), one key novelty of LRA is that it cho oses targets that are in the possible",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 66,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 68,
      "text": "representation of the associated layers and hence the layer’s parameters can be updated more",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 67,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 69,
      "text": "effectively (i.e. layers are not forced to try to match a target that is imp ossible to achieve).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 68,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 70,
      "text": "Thus, unlike innovations such as Difference Target Propagation (Lee et al., 2015), Batch",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 69,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 71,
      "text": "Normalization (Ioffe and Szegedy, 2015), etc, LRA do es not need to introduce new layers in",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 70,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 72,
      "text": "the architecture. As a result it can be viewed either as an alternative to such approaches, or",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 71,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 73,
      "text": "as a complementary technique because it is compatible with these other metho ds (i.e. it can",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 72,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 74,
      "text": "be used with batch normalization layers and residual blocks and any other layers that are",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 73,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 75,
      "text": "helpful for the problem-specific representations a deep network needs to acquire).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 74,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 76,
      "text": "Our metho d for setting the targets treats a deep network as a collection of smaller,",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 64,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 77,
      "text": "related subgraphs. This view allows us to flexibly incorporate ideas like feedback alignment",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 76,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 78,
      "text": "(Lillicrap et al., 2016) to train deep networks with non-differentiable activations – specifically,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 77,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 79,
      "text": "we use the feedback matrix to up date the targets rather than the weights. This variant of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 78,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 80,
      "text": "LRA can also train differentiable networks as fast as back-propagation but more robustly –",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 79,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 81,
      "text": "it is less sensitive to weight initializations than backprop and even other variants of feedback",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 80,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 82,
      "text": "alignment (Lillicrap et al., 2016; Nøkland, 2016).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 81,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 83,
      "text": "Another interesting feature of LRA is that it dynamically chooses which layers need to",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 76,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 84,
      "text": "b e trained - in the beginning, all layers, even the b ottom-most layers in the network, receive",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 83,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 85,
      "text": "significant up dates; towards the end of training, only the top few layers need to b e updated.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 84,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 86,
      "text": "In the experiments of this pap er, we compare two variations of the LRA – one where",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 83,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 87,
      "text": "updates are based on calculus and another where error feedback weights are used (which",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 86,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 88,
      "text": "turns out to be superior in robustness and speed). In our results, we show that LRA is:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 87,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 89,
      "text": "1. We will refer to back-propagation of errors also as “backprop” and “back-propagation” throughout.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 86,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 90,
      "text": "2",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 89,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 91,
      "text": "Local Representation Alignment",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 92,
      "text": "• robust to initialization when training highly nonlinear, deep networks. This result even",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 86,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 93,
      "text": "holds in the extreme case of zero initialization, which back-propagation and target",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 92,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 94,
      "text": "propagation cannot even handle.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 93,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 95,
      "text": "• able to avoid the vanishing gradient problem and train deep networks rather indepen-",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 92,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 96,
      "text": "dently of the nonlinearity used internally. This means we can train p erformant mo dels",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 95,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 97,
      "text": "comp osed of many units such as the classical logistic sigmoid.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 96,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 98,
      "text": "• able to adapt the amount of computation expended during training. The depth of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 95,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 99,
      "text": "credit assignment is tied to how well a representation aligns with a target as governed",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 98,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 100,
      "text": "by the lo cal loss function.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 99,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 101,
      "text": "• able to learn networks that contain discrete-valued activation functions.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 95,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 102,
      "text": "• able to learn networks that contain sto chastic units.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 101,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 103,
      "text": "• easily able to work with biologically inspired mechanisms, such as hard and soft lateral",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 102,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 104,
      "text": "competition among hidden units. Since derivatives are no longer required for these",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 103,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 105,
      "text": "mechanisms, a pathway to integrating other, even non-differentiable neurocognitively-",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 104,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 106,
      "text": "motivated mechanisms is opened up.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 105,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 107,
      "text": "We first explain how back-propagation can be re-cast in the target propagation framework",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 103,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 108,
      "text": "of Lee et al. (2015) in Section 2. This makes it easier to identify weaknesses in the “backprop",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 107,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 109,
      "text": "as targetprop” viewpoint and explain the modifications that give rise to LRA. Then we",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 108,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 110,
      "text": "explain how a further modification allows LRA to work with discrete, non-differentiable units,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 109,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 111,
      "text": "as well as stochastic units. We discuss related work in Section 4 and present exp erimental",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 110,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 112,
      "text": "results in Section 3 using MNIST and Fashion MNIST, a new and much more challenging",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 111,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 113,
      "text": "benchmark.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 112,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 114,
      "text": "2. Local Representation Alignment",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 101,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 115,
      "text": "To simplify notation, we describ e LRA in the context of a multilayer p erceptron architecture.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 114,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 116,
      "text": "However, LRA naturally applies to any stacked neural architecture, including those that are",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 115,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 117,
      "text": "recurrent. In the supplementary material we provide the general formulation of LRA such",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 116,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 118,
      "text": "that it may be applied recurrent mo dels as well.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 117,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 119,
      "text": "2.1 Notation",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 114,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 120,
      "text": "Let zℓ−1 b e the inputs, as computed in the feed-forward phase, to the no des in layer ℓ. This",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 119,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 121,
      "text": "means that zℓ is also the output of layer ℓ (hence we call it the post-activation ). Let Wℓ b e",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 120,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 122,
      "text": "the weight matrix at layer ℓ that multiplies the inputs zℓ−1. Let hℓ be the pre-activation of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 121,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 123,
      "text": "layer ℓ (i.e. hℓ = Wℓ zℓ−1). Let f ℓ b e the vector of activation functions for layer ℓ, so that",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 122,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 124,
      "text": "zℓ = f ℓ (hℓ ).",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 123,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 125,
      "text": "Following TargetProp (Lee et al., 2015), during training, for each layer ℓ (starting from",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 119,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 126,
      "text": "the output layer and working down the network) we set a target yℓz for the feedforward",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 125,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 127,
      "text": "post-activations zℓ. Then, for each layer ℓ, we take a gradient descent step on the network",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 126,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 128,
      "text": "parameters Wℓ of layer ℓ to move zℓ closer to yℓz by minimizing a layer-specific loss function",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 127,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 129,
      "text": "Lℓ (zℓ , y ℓz ). Thus we iteratively cho ose new targets, then refine weights, then choose new",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 128,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 130,
      "text": "3",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 129,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 131,
      "text": "Ororbia, Mali, Kifer, and Giles",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 129,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 132,
      "text": "(cid:654)-1 (cid:654)y(cid:766)W(cid:654)L(cid:654)h(cid:654)z(cid:654)h(cid:654)-1z(cid:654)-1y(cid:766)(cid:654) ∂h∂L ( , )(cid:654) (cid:654)-1_h β ∂h∂L(cid:654) (cid:654)-1 z (cid:654)y(cid:766)(cid:654)-1 == ( )f(cid:654)yh(cid:654)-1 = (cid:654)-1h_(cid:654)-1h_(cid:654)-1z_ = (cid:654)-1h_(a) LR-diff target search. (cid:654)y(cid:766)W(cid:654)L(cid:654)h(cid:654)z(cid:654)h(cid:654)-1z(cid:654)-1y(cid:766)(cid:654) ∂h∂L ( , )(cid:654) (cid:654)z (cid:654)y(cid:766)(cid:654)-1 = ( )f(cid:654)yh(cid:654)-1 = (cid:654)-1h_(cid:654)-1z_ = (cid:654)-1h_ E(cid:654)(cid:654)-1 _h β ∂h∂L (cid:654) (cid:654)=(cid:654)-1h_ ( )E(cid:654)(b) LRA-fdbk target search.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 133,
      "text": "Figure 1: The target calculation process for two variants of LRA applied to a differentiable",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 132,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 134,
      "text": "multilayer perceptron (MLP). A computational subgraph is formed from two layers.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 133,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 135,
      "text": "Given the target for layer ℓ, we compute a target yℓ−1h for the pre-activation of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 134,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 136,
      "text": "layer ℓ − 1 and use it to obtain a target y ℓ−1z for the output of layer ℓ − 1.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 135,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 137,
      "text": "targets, etc. 2 However, unlike TargetProp (Lee et al., 2015), LRA will not modify the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 136,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 138,
      "text": "network architecture. The setting of targets in differentiable networks is illustrated in Figure",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 137,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 139,
      "text": "1 (which shows two variants of LRA). Handling non-differentiable and stochastic activations",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 138,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 140,
      "text": "requires a small modification that is discussed in Section 2.3.2.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 139,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 141,
      "text": "2.2 Setting the targets.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 114,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 142,
      "text": "We first explain how backpropagation fits into this framework. One way, noted by Lee et al.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 141,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 143,
      "text": "(2015), is to set the target for the output of layer ℓ to be its current feed-forward value plus",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 142,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 144,
      "text": "the gradient of the overall loss with resp ect to the output of layer ℓ (and the loss function is",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 143,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 145,
      "text": "squared loss). This is a global view - the target of each layer is set to optimize the global",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 144,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 146,
      "text": "loss. Instead, we present a more lo cal view in that the target for layer i is specifically set to",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 145,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 147,
      "text": "help layer ℓ + 1 reach its own target. This view is the starting point that will allow us to",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 146,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 148,
      "text": "consider the training of a deep network as training on computation subgraphs (as in Figure",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 147,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 149,
      "text": "1) that can be optimized locally.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 148,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 150,
      "text": "In our view of backprop as target prop, we use L2 loss for each layer and set the target",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 142,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 151,
      "text": "yℓz of layer ℓ so that the difference between the layer ℓ output zℓ and its target yℓz is the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 150,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 152,
      "text": "gradient of the next3 layer’s loss with respect to zℓ (i.e. the direction that zℓ should move",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 151,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 153,
      "text": "to achieve the steepest local change in the next layer’s output).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 152,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 154,
      "text": "2. During the weight update, for the purposes of computing derivatives, the current target yℓz is not treated",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 150,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 155,
      "text": "as a function of the input weights (so that the feed-forward output moves closer to the desired targets",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 154,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 156,
      "text": "rather than vice versa).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 155,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 157,
      "text": "3. i.e., ℓ + 1",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": 156,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 158,
      "text": "4",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": 157,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 159,
      "text": "Local Representation Alignment",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 160,
      "text": "Lemma 1 Consider a network with n layers, input x, feedforward activations zℓ (ℓ =",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 154,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 161,
      "text": "1, . . . , n), output layer target t, and squared loss Lℓ for layers 1, . . . , ℓ − 1 (that is, except the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 160,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 162,
      "text": "top layer). Recursively set ynz = t and, for ℓ = n− 1, . . . , 1, set yℓz = zℓ − ∇zℓ Lℓ+1 (zℓ+1, yℓ+1z ),",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 161,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 163,
      "text": "where zℓ+1 is considered a function of zℓ. A simultaneous one-step gradient descent update",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 162,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 164,
      "text": "to all the weight matrices W1 , . . . Wn with respect to the layer-wise losses is equivalent to",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 163,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 165,
      "text": "one step of back-propagation.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 164,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 166,
      "text": "Proof It suffices to show that if Wℓ,j is the weight vector for node j in layer ℓ, then the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 163,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 167,
      "text": "derivative of the loss for layer ℓ with resp ect to these weights is the same as the derivative",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 166,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 168,
      "text": "of the overall loss with respect to these weights: ddWℓ,j Lℓ(zℓ, yℓz ) ≡ ddWℓ,j 12 ||zℓ − yℓz||22 =",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 167,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 169,
      "text": "∇Wℓ,j Ln(zn , t) for each j and for ℓ = 1, . . . , n − 1 (since the update to the weights Wn at",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 168,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 170,
      "text": "the top layer is always a gradient descent update). We will prove the stronger state ment,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 169,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 171,
      "text": "that for any k ≥ ℓ, then ddWℓ,j Lk (zk , ykz ) = ∇Wℓ,j Ln(zn , t) for each j and for ℓ = 1, . . . , n − 1",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 170,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 172,
      "text": "In the base case, when k = n − 1,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 171,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 173,
      "text": "d dWℓ, j Ln−1(zn−1 , yn−1z ) = d dWℓ , j 1 2 ||zn−1 − yn−1z ||22 = dzn−1 dWℓ,j (zn−1 − yn−1z ) (cid:12) (cid:12) (cid:12)yn−1z =zn−1 −∇zn−1 Ln(zn ,t) = dzn−1 dWℓ,j ∇zn−1 Ln(zn , t) = d dWℓ,j Ln (zn, t)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 172,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 174,
      "text": "by the chain rule. For the inductive step, we show that if the result is true for k (i.e.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 173,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 175,
      "text": "ddWℓ,j Lk (zk , ykz ) = ∇Wℓ,j Ln (zn , t)) then it is also true for k − 1 (i.e. ddWℓ,j Lk −1 (zk−1 , yk −1z ) =",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 174,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 176,
      "text": "∇Wℓ,j Ln(zn , t)). So",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 175,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 177,
      "text": "d dWℓ, j Lk −1 (zk−1 , yk−1z ) = ddWℓ, j 12 ||zk−1 − yk −1z ||22 = dzk−1 dWℓ,j (zk−1 − yk−1z ) (cid:12) (cid:12) (cid:12)yk −1z =zk−1−∇zk−1 Lk(zk ,ykz ) = dzk−1 dWℓ,j ∇zk−1 Lk (zk , ykz ) = d dWℓ,j Lk (zk , y kz ) = d dWℓ Ln (zn, t)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 176,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 178,
      "text": "by the chain rule and inductive hypothesis.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 177,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 179,
      "text": "Hence mini-batch gradient descent can be viewed as, for every iteration, selecting a",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 178,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 180,
      "text": "mini-batch, choosing targets for each layer to obtain a p er-layer optimization problem, and",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 179,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 181,
      "text": "partially optimizing the layer-wise loss (with one gradient step).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 180,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 182,
      "text": "This view naturally leads to three ways to improve training:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 181,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 183,
      "text": "• The target yℓz , intuitively, is a desired value for the output of layer ℓ that will help",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 182,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 184,
      "text": "layer ℓ + 1 lower its loss. Thus it is imp ortant to ensure that the target is actually",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 183,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 185,
      "text": "representable by layer ℓ. Therefore we should look at the pre-activation hℓ (i.e. inputs",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 184,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 186,
      "text": "5",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 185,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 187,
      "text": "Ororbia, Mali, Kifer, and Giles",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 185,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 188,
      "text": "to the nodes at layer ℓ) and determine what values of hℓ , when fed through the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 187,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 189,
      "text": "activation function of layer ℓ, will help layer ℓ + 1 reduce its loss. We set yℓh to b e",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 188,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 190,
      "text": "the target for the pre-activation hℓ and feed these pre-activation targets through",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 189,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 191,
      "text": "the activation function to obtain the targets y ℓz for layer ℓ, as shown in Figures 1.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 190,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 192,
      "text": "One way to set the pre-activation target is through one step of gradient descent on",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 182,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 193,
      "text": "the lo cal loss function: yℓh = hℓ − η ∇hℓ Lℓ+1 (zℓ+1 , yℓ+1z ) and then yℓz = f ℓ (yℓh) =",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 192,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 194,
      "text": "f ℓ (cid:0)hℓ − η ∇hℓ L(zℓ+1 − yℓ+1z )(cid:1).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 193,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 195,
      "text": "• To cho ose better pre-activation targets yℓh, we can perform multiple gradient descent",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 194,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 196,
      "text": "steps for y ℓh on the loss of the next layer Lℓ+1 (zℓ+1, yℓ+1z ). Such a pro cedure can b e",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 195,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 197,
      "text": "viewed as walking along the manifold of zℓ parametrized by hℓ, to find a pre-activation",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 196,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 198,
      "text": "target yℓh (and hence yℓz ) that would help the next layer reduce its loss. An alternative",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 195,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 199,
      "text": "to gradient steps is to use feedback alignment to update the targets (instead of the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 198,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 200,
      "text": "standard approach of updating the weights Nøkland (2016); Lillicrap et al. (2016)).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 199,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 201,
      "text": "• The p er-layer loss can be customized for each layer. For example, the least squares",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 200,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 202,
      "text": "loss can be replaced by the L1 norm or the log-p enalty (Cauchy).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 201,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 203,
      "text": "After the targets are set, the weights are up dated with one step of gradient descent:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 198,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 204,
      "text": "Wℓ ← Wℓ − ∇Wℓ Lℓ (zℓ, yℓz ) – possibly using an adaptive learning rate rule, e.g., Adam or",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 203,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 205,
      "text": "RMSprop. To handle discrete-valued and/or sto chastic activations, we employ the notion of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 204,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 206,
      "text": "a “short-circuit” connection (only used during training) to form the error pathway around",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 205,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 207,
      "text": "the activation. This detail is discussed in Section 2.3.2.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 206,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 208,
      "text": "2.3 Divide and Conquer: The Computation Subgraph",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 203,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 209,
      "text": "LRA aims to decompose the larger credit assignment problem in neural architectures into",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 208,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 210,
      "text": "smaller, easier-to-solve sub-problems. With this in mind, we can view any stacked neural",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 209,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 211,
      "text": "architecture, or rather, its full, underlying op eration graph, as a composition of “computation",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 210,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 212,
      "text": "subgraphs”. A directed, acyclic computation graph may be decomp osed into a set of smaller",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 211,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 213,
      "text": "direct subgraphs, where a subgraph’s b oundaries are defined as the set of input node variables",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 212,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 214,
      "text": "and the set of output no de variables.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 213,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 215,
      "text": "To be more specific, the input to a subgraph of a multilayer neural architecture is the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 209,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 216,
      "text": "vector of pre-activation values, hℓ−1 computed from the subgraph below (unless this is the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 215,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 217,
      "text": "bottommost subgraph which means that the input is simply the raw feature vector). The",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 216,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 218,
      "text": "output of the subgraph is simply the post-activation it computes as a function of its input, or",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 217,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 219,
      "text": "simply, zℓ . Note that while we present our notation to imply that a computation subgraph",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 218,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 220,
      "text": "only encapsulates two layers of actual processing elements, layers ℓ − 1 and ℓ, the subgraph",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 219,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 221,
      "text": "itself could b e deep and include pro cessing elements one decides are internal to the subgraph",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 220,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 222,
      "text": "itself. This would allow us to house self-connected no des inside the graph as well, assuming",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 221,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 223,
      "text": "that the subgraph is temporal and is intended to compute given sequences of inputs/outputs.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 222,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 224,
      "text": "In choosing boundaries, one could distinguish which node layers are to be representations",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 223,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 225,
      "text": "(or latent variables) and which node layers are simply inner computation elements, meant to",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 224,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 226,
      "text": "support the representation nodes. Figure 1 show one such subgraph Note that this subgraph",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 225,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 227,
      "text": "also includes a loss function Lℓ and the targets yℓz .",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 226,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 228,
      "text": "6",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 227,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 229,
      "text": "Local Representation Alignment",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 230,
      "text": "Algorithm 1 The LRA Algorithm applied to differentiable, feedforward neural architectures.Input: data (x, t), number steps K, halting criterion ϵ, step-size β, mo del parametersΘ = {W1 , b1, · · · , Wℓ, bℓ, · · · , Wn, bn }, and norm constraints {c1 , c2}Specify: {f1(h), · · · , fℓ(h), · · · , fn(h)}, {L1(z, y), · · · , Lℓ (z, y ), · · · , Ln(z, y)}, and, op-tionally, error weights {E1, · · · , Eℓ, · · · , En }// yℓh : what we would like input to the activation function at layer ℓ to be// yℓz: what we would like output of activation function fℓ at layer ℓ to be// hℓ : input to activation function at layer ℓ resulting from feed forward phase// zℓ: output of activation function at layer ℓ resulting from feed forward phase// ¯hℓ indicates temp orary variable for hℓ and ¯zℓ indicates temporary variable for zℓFunction{ComputeUpdateDirection}{(x, t), Θ, K, c1, c2 , β, ϵ}z0 = x // Run feedforward pass to get initial layer-wise statisticsfor ℓ = 1 to n doWℓ , bℓ, ← Θhℓ ← Wℓ zℓ−1 + bℓ, y ℓh ← hℓ , ¯hℓ ← hℓzℓ ← fℓ(hℓ), yℓz ← zℓ, ¯zℓ ← zℓend forynz ← t // Override top-level target with correct target from training dataℓ = nwhile ℓ ≥ 1 and Lℓ (zℓ , yℓz ) ≥ ϵ doWℓ , bℓ, ← Θ// Calculate parameter update direction for layer ℓ, comparing initial guess to target// Normalize(·, ·) is defined in Eq 1.∇Wℓ ← N ormalize( ∂ Lℓ(zℓ,yℓz )∂ Wℓ , c1 ), ∇bℓ ← N or mal iz e( ∂Lℓ (zℓ,yℓz )∂ bℓ , c1 )// Find target for layer ℓ − 1 (Note: could add early stopping criterion)for k = 1 to K do// Calculate pre-activation displacementif LRA-diff then∆hℓ−1 ← ∂ Lℓ(zℓ,yℓz)∂hℓ−1 = (Wℓ )T (cid:16) ∂Lℓ (zℓ ,y ℓz )∂zℓ ⊗ ∂ fℓ(hℓ )∂ hℓ (cid:17) ⊗ ∂fℓ−1(hℓ−1 )∂ hℓ−1else if LRA-fdbk then∆hℓ−1 ← Eℓ ∂ Lℓ(zℓ,yℓz)∂hℓ = Eℓ(cid:16) ∂ Lℓ(zℓ ,yℓz)∂ zℓ ⊗ ∂fℓ(hℓ)∂hℓ (cid:17)end if∆hℓ−1 ← N ormalize(∆hℓ−1 , c2)// Recalculate neural activities of subgraph¯hℓ−1 ← ¯hℓ−1 − β ∆hℓ−1 , ¯zℓ−1 ← fℓ−1 (¯hℓ−1 )¯hℓ ← Wℓ¯zℓ−1 + bℓ, ¯zℓ ← fℓ(¯hℓ )end foryℓ−1z ← ¯zℓ−1 // Update variable holding target for subgraph b elowℓ = ℓ − 1end whileReturn ∇Θ = {∇W1 , ∇b1 , · · · , ∇Wℓ , ∇bℓ , · · · , ∇Wn , ∇bn }EndFunction",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 231,
      "text": "7",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": 230,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 232,
      "text": "Ororbia, Mali, Kifer, and Giles",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 230,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 233,
      "text": "2.3.1 Instantiation: The Differentiable Multilayer Perceptron",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 232,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 234,
      "text": "Although LRA can b e extended to recurrent networks (an extension is presented in the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 233,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 235,
      "text": "supplementary materials), in this paper, for the sake of explanation, we sp ecialized it to",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 234,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 236,
      "text": "feedforward neural architectures.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 235,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 237,
      "text": "Algorithm 1 presents the pseudo code for LRA for differentiable networks (for non-",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 233,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 238,
      "text": "differentiable networks, additional notation is needed, so this mo dification is deferred to",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 237,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 239,
      "text": "Section 2.3.2). The pseudocode presents two versions of LRA: LRA-diff (the immediate",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 238,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 240,
      "text": "generalization of backprop we have b een discussing) and LRA-fdbk which incorp orates ideas",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 239,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 241,
      "text": "from feedback alignment Lillicrap et al. (2016). The difference between them is that LRA-diff",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 240,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 242,
      "text": "sets the target for layer ℓ using the full derivative δLℓ+1 (zℓ+1 ,y ℓ+1z )δhℓ . Using the chain rule, this",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 241,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 243,
      "text": "derivative is expressed as δhℓ+1δhℓ δLℓ+1 (zℓ+1 ,y ℓ+1z )δhℓ+1 . LRA-fdbk, during training, “short-circuits”",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 242,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 244,
      "text": "the connection between hℓ+1 and hℓ (as in Figure 1) by replacing δhℓ+1δ hℓ in the chain rule",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 243,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 245,
      "text": "with a fixed matrix Eℓ+1 that is randomly chosen before the start of training (e.g., sample",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 244,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 246,
      "text": "its weights from a standard Gaussian).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 245,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 247,
      "text": "Such short-circuit op erations have b een shown to be useful empirically (Lillicrap et al.,",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 237,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 248,
      "text": "2016; Nøkland, 2016) although they are not very well understo od theoretically. Their use in",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 247,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 249,
      "text": "prior work even allowed networks consisting of tanh nonlinearities (but not ReLU) to b e",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 248,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 250,
      "text": "trained from initial weights equal to 0 Nøkland (2016). In contrast, our experiments show",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 249,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 251,
      "text": "that LRA-fdbk is even more robust and can train a much broader set of networks from 0.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 250,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 252,
      "text": "In Algorithm 1, ⊗ is used to denote the Hadamard pro duct (or elementwise multiplica-",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 247,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 253,
      "text": "tion).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 252,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 254,
      "text": "The normalization function N ormalize(·) depicted in Algorithm 1 is defined formally as:",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 252,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 255,
      "text": "N or maliz e(∆, c) = (cid:40) c ||∆|| ∆, if ||∆|| ≥ c, and ∆, if ||∆|| < c (cid:41) (1)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 254,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 256,
      "text": "where ∆ is any vector",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 254,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 257,
      "text": "There are many possible choices for the local losses that measure the discrepancy between",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 254,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 258,
      "text": "a layer’s output and its target. One possibility is the L2-norm, defined as",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 257,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 259,
      "text": "Lℓ(z, y ) = 1 2 |z| (cid:88) i=1 (yi − zi)2. (2)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 258,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 260,
      "text": "Another choice is the L1 norm, which is defined as:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 259,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 261,
      "text": "Lℓ(z, y) = |z| (cid:88) i=1 |(yi − zi )|. (3)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 262,
      "text": "After preliminary exp erimentation, we actually found a different loss, the log-penalty, to",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 260,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 263,
      "text": "work much better with LRA for a wide variety of networks. The log-penalty function is",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 262,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 264,
      "text": "derived from the log likelihoo d of the Cauchy distribution. In this paper, we implemented",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 263,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 265,
      "text": "the log-penalty loss, for a single vector, as:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 264,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 266,
      "text": "Lℓ (z, y) = |z| (cid:88) i=1 log(1 + (yi − zi)2) (4)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 265,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 267,
      "text": "8",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 266,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 268,
      "text": "Local Representation Alignment",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 266,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 269,
      "text": "where the loss is computed over all dimensions |z| of the vector z (where a dimension is",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 254,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 270,
      "text": "indexed/accessed by integer i).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 269,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 271,
      "text": "LRA also p erforms variable depth credit assignment because of the condition in the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 269,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 272,
      "text": "while loop that stops the backward pass early when the feedforward activation of a layer is",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 271,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 273,
      "text": "close to its target (i.e. the local loss is at most ϵ). This feature is not strictly necessary, but",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 272,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 274,
      "text": "it is a nice addition that allows it to save computation by eventually only modifying the top",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 273,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 275,
      "text": "layers of a network.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 274,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 276,
      "text": "Finally, for the inner loop which successively refines the target, we found that with",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 271,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 277,
      "text": "LRA-fdbk, the best p erformance is achieved when K = 1. This setting also makes its",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 276,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 278,
      "text": "computational requirements comparable to backprop.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 277,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 279,
      "text": "2.3.2 Handling Non-Differentiable/Stochastic Activations",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 208,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 280,
      "text": "The only thing that prevents the LRA-fdbk version of Algorithm 1 from handling non-",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 279,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 281,
      "text": "differentiable and sto chastic units is the computation of ∂ Lℓ (zℓ,yℓz )∂ hℓ as it involves, via chain",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 280,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 282,
      "text": "rule, the derivative of the activation function fℓ of layer ℓ.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 281,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 283,
      "text": "This difficulty is easily circumvented with a trick inspired by Lee et al. (2015). We",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 280,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 284,
      "text": "illustrate it with the following two activations (1) sign(h) (also known as the Heavyside step",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 283,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 285,
      "text": "function) which returns −1, 0, or 1, dep ending on whether h is negative, 0, positive, respec-",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 284,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 286,
      "text": "tively, and (2) ber noull i(h) which outputs 1 with probability σ(h) and 0 with probability",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 285,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 287,
      "text": "1 − σ(h) (where σ is the sigmoid function).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 286,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 288,
      "text": "This means that we can rewrite these activations as a composition of two functions",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 283,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 289,
      "text": "g(f (h)), where f is a differentiable approximation of the activation function. For instance:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 288,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 290,
      "text": "sign(h) = sign(tanh(h)) ber noulli(h) = bernoulli∗(σ(h))",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 289,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 291,
      "text": "where bernoulli∗ (x) returns 1 with probability x and 0 otherwise.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 290,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 292,
      "text": "Splitting activation functions this way allows us to extend our notation so that:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 291,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 293,
      "text": "zℓ = f (hℓ ) zℓ∗ = g (hℓ )",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 292,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 294,
      "text": "Now, zℓ is an intermediate output and zℓ∗ is the output of layer ℓ. The mo dification to LRA",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 293,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 295,
      "text": "is almost trivial: we use zℓ∗ in the feed-forward phase4 but set the targets for zℓ instead of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 294,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 296,
      "text": "zℓ∗ . For reference, the complete algorithm is shown in the supplementary materials.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 295,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 297,
      "text": "2.4 Overcoming Poor Initializations",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 296,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 298,
      "text": "Poor initializations affect networks with various activations differently, as we shall observe",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 297,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 299,
      "text": "in the experiments later in this chapter. LRA, in both forms, can be seen as correcting for",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 298,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 300,
      "text": "p oor settings–something back-propagation of errors cannot do. LRA-diff can, if given a large",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 299,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 301,
      "text": "enough local computation budget K, “walk away” from poor settings quickly. That is, even",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 300,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 302,
      "text": "if the penultimate layer n − 1 provides bad features for the final classification, the target for",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 301,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 303,
      "text": "that layer will be a set of features that help the final layer make a b etter prediction. Then,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 302,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 304,
      "text": "4. i.e. in Algorithm 1 we set hℓ = Wℓzℓ−1∗ + bℓ then zℓ = fℓ(hℓ), and then zℓ∗ = g (zℓ)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 303,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 305,
      "text": "9",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 304,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 306,
      "text": "Ororbia, Mali, Kifer, and Giles",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 307,
      "text": "recursively, if layer n − 2 provides bad features, the target for that layer will be a set of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 298,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 308,
      "text": "features that will help layer n − 1 achieve its target which, in turn, will help the final layer.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 307,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 309,
      "text": "In the experiments, we investigate how robust LRA is to various settings of the ini-",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 308,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 310,
      "text": "tialization scheme and how other algorithms, especially target propagation and feedback",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 309,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 311,
      "text": "alignment, compare.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 310,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 312,
      "text": "2.5 Overcoming Explo ding and Vanishing Gradients",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 304,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 313,
      "text": "When neural networks are made deeper, backprop error gradients must pass backward through",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 312,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 314,
      "text": "many layers using the global feedback pathway that involves a series of multiplications.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 313,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 315,
      "text": "As a result of these extra multiplications, these gradients tend to either explode or vanish",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 314,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 316,
      "text": "(Bengio et al., 1994; Pascanu et al., 2013). In order to keep the values of the gradients within",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 315,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 317,
      "text": "reasonable magnitudes and prevent zero gradients, it is common to imp ose constraints to",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 316,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 318,
      "text": "ensure that layers are sufficiently linear in order to prevent p ost-activations from reaching",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 317,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 319,
      "text": "their saturation regimes. However, this required linearity can create less than desirable",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 318,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 320,
      "text": "side-effects, e.g., adversarial samples (Szegedy et al., 2013; Ororbia I I et al., 2017b).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 319,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 321,
      "text": "LRA handles the vanishing gradient problem by tackling the global credit assignment",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 316,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 322,
      "text": "in a lo cal fashion, using the persp ective of computation subgraphs as describ ed earlier",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 321,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 323,
      "text": "in Section 2.3. In other words, LRA treats the underlying graph of the neural graphical",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 322,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 324,
      "text": "mo del as a series of subgraphs and then pro ceeds to optimize each subproblem. This is",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 323,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 325,
      "text": "the essence of lo cal learning in LRA. To overcome exploding gradients, LRA makes use of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 324,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 326,
      "text": "gradient re-pro jection, which is often used to introduce stability in recurrent neural networks",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 325,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 327,
      "text": "(Pascanu et al., 2013). Re-projection, embodied in the N ormalize(·) function call, is used in",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 321,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 328,
      "text": "two places within the LRA procedure–1) rescale parameter updates ∇Wℓ to have Frobenius",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 327,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 329,
      "text": "norm c1 and ∇bℓ to have L2 norm equal to c1, and 2) rescale the calculated representation",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 328,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 330,
      "text": "displacement to have L2 norm equal to c2.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 329,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 331,
      "text": "3. Exp erimental Results",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 330,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 332,
      "text": "The goal of these experiments is to test how easy it is to train deep networks with different",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 331,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 333,
      "text": "algorithms (compared to LRA) and how robust these algorithms are to various settings,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 332,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 334,
      "text": "such as choice of initialization weights. Our claim is that LRA makes training of algorithms",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 333,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 335,
      "text": "very easy and do es not require much tuning to achieve high levels of accuracy – our goal is",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 334,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 336,
      "text": "not necessarily to reach the state-of-the-art on any particular task (which typically requires",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 335,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 337,
      "text": "expensive hyperparameter tuning). One of the use-cases of LRA is for researchers outside of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 336,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 338,
      "text": "deep learning who want to experiment with many different (and possibly novel) architectures",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 337,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 339,
      "text": "designed for their data.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 338,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 340,
      "text": "For all experiments in this paper, we keep the parameter optimization setting the same",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 332,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 341,
      "text": "for all scenarios so that we may tease out the effects of individual learning algorithms instead.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 340,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 342,
      "text": "Specifically, updates calculated by each algorithm are used in a simple first-order gradient",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 341,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 343,
      "text": "descent with a fixed learning rate of 0.01 and mini-batches of 50 samples.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 342,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 344,
      "text": "We briefly describe the datasets used to investigate the ability of each learning algorithm",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 343,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 345,
      "text": "in training deep, nonlinear networks.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 344,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 346,
      "text": "10",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 344,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 347,
      "text": "Local Representation Alignment",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 346,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 348,
      "text": "EpochClassificationError(%) 0 1 2 3 4 5 6 7 8 9 1005102030405060708090100(a) 10 epochs of LRA-diff (MNIST). EpochClassificationError(%) 0 1 2 3 4 5 6 7 8 9 1005102030405060708090100(b) 10 epochs of LRA-diff (Fashion MNIST)EpochClassificationError(%) 0 1 2 3 4 5 6 7 8 9 1005102030405060708090100(d) 10 epochs of LRA-fdbk (MNIST). EpochClassificationError(%) 0 1 2 3 4 5 6 7 8 9 10 11 1205102030405060708090100(e) 10 ep ochs of LRA-fdbk (Fashion MNIST).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 347,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 349,
      "text": "Figure 2: Drop in test-set error during first few epo chs of training tanh networks on the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 348,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 350,
      "text": "MNIST and Fashion MNIST image datasets.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 349,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 351,
      "text": "11",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 350,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 352,
      "text": "Ororbia, Mali, Kifer, and Giles",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": 351,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 353,
      "text": "(a) Backprop acquired filters. (b) LRA-diff acquired filters. (c) LRA-fdbk acquired filters.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 354,
      "text": "Figure 3: Third-level filters acquired after 1 epoch under each learning algorithm: (a)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 353,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 355,
      "text": "backprop, (b) LRA-diff, (c) LRA-fdbk.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 354,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 356,
      "text": "MNIST: The popular MNIST dataset (LeCun et al., 1998a) contains 28x28 grey-scale",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 354,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 357,
      "text": "pixel images, each belonging to one of 10 digit categories. There are 60,000 training images,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 356,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 358,
      "text": "from which we create a validation subset of 10,000 images, and 10,000 test images.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 357,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 359,
      "text": "Fashion MNIST: Fashion MNIST (Xiao et al., 2017) is a dataset composed of 28x28 grey-",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 358,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 360,
      "text": "scale images of clothing items, meant to serve as a much more difficult drop-in replacement",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 359,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 361,
      "text": "for MNIST itself. The size and structure of the training and testing splits are the same as",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 360,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 362,
      "text": "in MNIST and each image is associated with one of 10 classes. We create a validation set of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 361,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 363,
      "text": "10,000 samples from the training split via random sampling without replacement.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 362,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 364,
      "text": "3.1 Effect of Manifold-Walking",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 359,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 365,
      "text": "We first investigate how LRA-diff ’s computation budget K, specifically the numb er of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 364,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 366,
      "text": "sub-optimization steps allo cated p er subgraph, affects its ability to train a highly nonlinear",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 365,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 367,
      "text": "network. Furthermore, we contrast this procedure against the vastly simpler error-feedback",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 366,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 368,
      "text": "variant, LRA-fdbk (which uses K = 1 so is also much faster). To do so, we construct",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 367,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 369,
      "text": "networks of three hidden layers of 64 hyperbolic tangent units with biases initialized from",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 368,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 370,
      "text": "zero and weights according to the following classical heuristic:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 369,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 371,
      "text": "Wij ∼ U (cid:20) − 1 √ nin , 1 √ nin (cid:21) , (5)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 370,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 372,
      "text": "noting that nin is the size of previous/incoming layer of post-activities or the numb er of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 371,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 373,
      "text": "columns of the weight matrix W (if working in column-major form). U [−a, a] is the uniform",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 372,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 374,
      "text": "distribution in the interval (−a, a).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 373,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 375,
      "text": "For LRA-diff, we varied the computation budget K = {5, 10, 30, 50}, and for LRA-",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 372,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 376,
      "text": "fdbk, we fixed K = 1. The entries of the feedback matrix for LRA-fdbk were generated",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 375,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 377,
      "text": "independently from a Gaussian with standard deviation σE. We tried the following settings",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 376,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 378,
      "text": "of σE = {1.0, 1.25, 1.5, 2.0} (note in the plots this is denoted sd ). Both variants of LRA",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 377,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 379,
      "text": "employed the Cauchy loss (see Equation 4) as the metric for measuring discrepancy between",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 378,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 380,
      "text": "12",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 379,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 381,
      "text": "Local Representation Alignment",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 382,
      "text": "representation and target. Networks were trained over 100 ep ochs but we only show the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 375,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 383,
      "text": "first 5 epo chs, since roughly after this point, the differences in generalization rates were too",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 382,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 384,
      "text": "similar to warrant visualization.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 383,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 385,
      "text": "In Figure 2, for LRA-diff, we see that increasing K leads to ultimately b etter general-",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 375,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 386,
      "text": "ization and sooner on b oth MNIST and Fashion MNIST. However, there is a diminishing",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 385,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 387,
      "text": "return as one dramatically increases the number of steps from K = 30 to K = 50. As K is",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 386,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 388,
      "text": "the number of iterations of the inner loop in Algorithm 1, increasing K leads to significant",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 387,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 389,
      "text": "slowdown. However, more importantly, LRA-fdbk, which uses K = 1, is therefore a far faster",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 388,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 390,
      "text": "variation of LRA and it reaches the same level of generalization. This means LRA-fdbk",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 389,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 391,
      "text": "is able to use the short-circuit feedback connections to create a useful displacement for",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 390,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 392,
      "text": "the model’s current input representation to help lower its lo cal loss. We found that the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 391,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 393,
      "text": "initialization of the error feedback weights affects LRA-fdbk ’s p erformance, though as one",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 392,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 394,
      "text": "raises the standard deviation, the impact is far less severe than varying the number of steps",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 393,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 395,
      "text": "in LRA-diff.5",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 394,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 396,
      "text": "In Figure 3, we show the filters acquired by the feedforward network on Fashion MNIST",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 385,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 397,
      "text": "after a single epoch. To create these filter visualizations, we employ the feature activation",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 396,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 398,
      "text": "maximization approach as presented by Erhan et al. (2010). Furthermore, while this approach",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 397,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 399,
      "text": "generally only applies to the first hidden layer of units, which sit closest to the input pixel",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 398,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 400,
      "text": "no des, we can apply the same technique to the upper hidden layers of the network, such",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 399,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 401,
      "text": "as the third layer, by simply ignoring the nonlinearity at each level of the model. Thus,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 400,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 402,
      "text": "we approximately “linearize” the nonlinear network which allows us to collapse successive",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 401,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 403,
      "text": "weight matrices back into a single matrix (taking advantage of this natural property of deep",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 402,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 404,
      "text": "linear networks). This will incur some minor approximation error, since the network is not",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 403,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 405,
      "text": "truly linear, but we found that this approximation gave us a very fast and reasonably go o d",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 404,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 406,
      "text": "picture of what knowledge might b e captured in the synaptic connections that form the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 405,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 407,
      "text": "memories of the upp er layer no des, i.e., those closest to the output layer. Observe that both",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 406,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 408,
      "text": "versions of LRA (note that for LRA-diff we use the network trained with K = 30) learn",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 407,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 409,
      "text": "reasonably good and clear filters after just one pass through the data. Back-propagation,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 408,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 410,
      "text": "however, learns far noisier filters. Again, it is surprising to see that LRA-fdbk learn so well",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 409,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 411,
      "text": "with only one pass through the data, given that it is far cheaper computationally than",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 410,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 412,
      "text": "LRA-diff. Encouraged by this positive behavior, its low computation requirements, and the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 411,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 413,
      "text": "fact that LRA-fdbk can also handle a far greater range of activations, such as discrete-valued",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 412,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 414,
      "text": "and stochastic ones, we focus on LRA-fdbk (with K = 1, so it only makes once pass through",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 413,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 415,
      "text": "its inner loop, making its computational requirements comparable to back-prop) for the rest",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 414,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 416,
      "text": "of the pap er.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 415,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 417,
      "text": "3.2 Robustness to Initialization",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 364,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 418,
      "text": "It is very difficult to train deep (and thin) networks from simplistic initialization schemes",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 417,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 419,
      "text": "(Romero et al., 2014). Furthermore, LeCun et al. (1998b) showed that using the logistic",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 418,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 420,
      "text": "sigmoid as an activation function can slow down learning considerably, largely due to its",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 418,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 421,
      "text": "non-zero mean, which was further investigated by Glorot and Bengio (2010). Given the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 420,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 422,
      "text": "5. Choosing a value for the standard deviation that is to o low, esp ecially below one, however, can slow down",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 421,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 423,
      "text": "the learning process. We found that naively using a standard deviation of one worked quite well for our",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 422,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 424,
      "text": "preliminary experiments and thus did no further tuning after Experiment 3.1.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 423,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 425,
      "text": "13",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 423,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 426,
      "text": "Ororbia, Mali, Kifer, and Giles",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 13,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 427,
      "text": "020406080100 EpochClassificationError(%) 0 5 10 20 30 40 50 60 70 80 90 10005102030405060708090100 (b) MNIST, sigmoid network. 020406080100 EpochClassificationError(%) 0 10 20 30 40 50 60 70 80 90 100 115 130 14505102030405060708090100 (c) Fashion MNIST, sigmoid network.020406080100 EpochClassificationError(%) 0 5 10 20 30 40 50 60 70 80 90 10005102030405060708090100 (d) MNIST, tanh network. 020406080100 EpochClassificationError(%) 0 10 20 30 40 50 60 70 80 90 100 115 130 14505102030405060708090100 (e) Fashion MNIST, tanh network.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 13,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 428,
      "text": "Figure 4: Generalization of various learning algorithms used to train deep and thin networks",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 427,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 13,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 429,
      "text": "of either sigmoid (top) or tanh (b ottom) units on MNIST (left) and Fashion",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 422,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 13,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 430,
      "text": "MNIST (right). Initial weights were sampled: wi,j ∼ N (µ = 0, σ2 = 0.025). Note",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 429,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 13,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 431,
      "text": "that DTP, FA, and BP often fail or are unstable with this initialization, esp ecially",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 430,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 13,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 432,
      "text": "when training deep sigmoidal networks on Fashion MNIST.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 431,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 13,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 433,
      "text": "14",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 432,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 13,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 434,
      "text": "Local Representation Alignment",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 14,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 435,
      "text": "0246810 EpochDepth(#SubgraphsUpdated) 0 5 10 20 30 40 50 60 70 80 90 100012345678910 High Cost Low Cost(b) MNIST mean depth. 0246810 EpochDepth(#SubgraphsUpdated) 0 5 10 20 30 40 50 60 70 80 90 100012345678910 High Cost Low Cost(c) Fashion MNIST mean depth.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 434,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 14,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 436,
      "text": "Figure 5: Average depth of the credit assignment carried out by LRA on w.r.t. deep",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 435,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 14,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 437,
      "text": "sigmoidal and tanh networks (if the backwards pass is stopped whenever the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 436,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 14,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 438,
      "text": "feed-forward activations are similar to the targets), compared against backprop’s",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 437,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 14,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 439,
      "text": "fixed depth.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 438,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 14,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 440,
      "text": "Table 1: Robustness to po or initialization. Training and generalization error (%) of deep",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 436,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 14,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 441,
      "text": "sigmoid or tanh networks, initialized with 0-mean Gaussians with std of 0, 0.025,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 440,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 14,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 442,
      "text": "0.05, and 0.1",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 441,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 14,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 443,
      "text": "MNISTϕ(·) = Sigmoid ϕ(·) = TanhAlgorithm std: 0 0.025 0.05 0.1 0 0.025 0.05 0.1Backprop X 88.66 88.66 88.66 X 88.65 2.67 2.48FA 88.65 88.65 88.65 88.65 16.1 12.73 11.86 11.81DFA 30.84 31.78 31.77 26.81 5.24 4.11 5.12 4.9DTP X 18.06 16.83 17.06 X 67.51 5.38 3.19LRA-fdbk 2.75 2.85 2.89 2.96 2.69 2.87 2.82 3.72Fashion MNISTϕ(·) = Sigmoid ϕ(·) = TanhAlgorithm std: 0 0.025 0.05 0.1 0 0.025 0.05 0.1Backprop X 90.00 90.00 90.00 X 29.04 11.48 11.38FA 90.00 90.00 90.00 89.97 24.82 24.57 21.67 20.01DFA 35.91 37.4 41.62 37.7 13.92 12.82 13.95 14.13DTP X 41.1 26.26 24.69 X 79.84 60.19 13.59LRA-fdbk 13.27 12.85 12.94 12.96 11.85 12.14 12.62 12.65",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 442,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 14,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 444,
      "text": "problems that come with unit saturation and vanishing gradients (Bengio et al., 1994),",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 442,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 14,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 445,
      "text": "training a very deep and thin network, especially composed of logistic sigmoid units, with",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 444,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 14,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 446,
      "text": "only back-propagation, can be very difficult.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 445,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 14,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 447,
      "text": "To investigate LRA’s robustness to poor initialization, we use it and competing methods",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 444,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 14,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 448,
      "text": "to train deep nonlinear networks consisting of either logistic sigmoid or hyp erbolic tan-",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 447,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 14,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 449,
      "text": "gent activation functions with mo del parameters (i.e. network weights) initialized from a",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 448,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 14,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 450,
      "text": "15",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 449,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 14,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 451,
      "text": "Ororbia, Mali, Kifer, and Giles",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 452,
      "text": "parametrized, zero-mean Gaussian distribution. The Gaussian distribution is a very simple,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 451,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 453,
      "text": "common way to initialize the parameters of a neural mo del, and controlling its standard",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 452,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 454,
      "text": "deviation, σ , allows us to probe different cases when back-propagation fails. In this exp eri-",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 453,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 455,
      "text": "ment, we investigate the settings σ = {0.025, 0.05, 0.1}, and compare LRA and backprop",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 454,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 456,
      "text": "(BP ) to algorithms such as Difference Target Propagation (DTP ) from Lee et al. (2015),",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 455,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 457,
      "text": "Feedback Alignment (FA) from Lillicrap et al. (2016), and Direct Feedback Alignment (DFA)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 456,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 458,
      "text": "from Nøkland (2016). Furthermore, we also show the situation where the weights are simply",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 457,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 459,
      "text": "initialized to zero. Whenever it was not possible to learn from zero with a given algorithm,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 458,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 460,
      "text": "such as BP and DTP, we simply marked the appropriate slot with an X . To initialize the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 459,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 461,
      "text": "feedback weights of DFA and FA, we follow the protocol prescribed by Nøkland (2016).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 460,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 462,
      "text": "The network architecture each algorithm is responsible for training is the same: a",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 460,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 463,
      "text": "multilayer perceptron containing eight hidden layers of 128 processing elements. We examine",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 462,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 464,
      "text": "the ability of each algorithm to train the same architecture employing logistic sigmoid (a",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 463,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 465,
      "text": "non-zero mean activation function) and the hyperbolic tangent (a zero-mean activation",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 464,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 466,
      "text": "function).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 465,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 467,
      "text": "In Table 1, we present the best found generalization error rate for the deep architecture",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 462,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 468,
      "text": "learned with each algorithm under each initialization setting. Observe that LRA-fdbk is",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 467,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 469,
      "text": "rather robust to the initializations scheme, and more importantly, is able to train to go od",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 468,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 470,
      "text": "generalization regardless of which unit type is used. Furthermore, even at initializations close",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 469,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 471,
      "text": "to or at zero, LRA-fdbk is able to train deep networks of both logistic sigmoid and hyperbolic",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 470,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 472,
      "text": "tangent networks. In Figure 4, we focus on the setting with σ = 0.025 for the Gaussian",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 471,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 473,
      "text": "distribution used to initialize the networks (i.e. the lowest non-zero setting). Observe that",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 472,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 474,
      "text": "despite p o or initialization, even within 10 epochs, LRA-fdbk is able to consistently reach",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 473,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 475,
      "text": "go od classification error (≈ 5% on MNIST and < 20% on Fashion MNIST), while the other",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 474,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 476,
      "text": "methods struggle to reach those numbers even after 100 epochs. DFA is competitive with",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 475,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 477,
      "text": "LRA-fdbk when using hyp erbolic tangent units but trains the same network composed of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 476,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 478,
      "text": "sigmoid units poorly.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 477,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 479,
      "text": "According to our results, for DTP, the inverse mapping used to reconstruct the underlying",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 477,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 480,
      "text": "layerwise targets does not work all that well when weights are initialized from a purely",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 479,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 481,
      "text": "random Gaussian, esp ecially with a low standard deviation. As observed in Table 1, DTP",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 480,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 482,
      "text": "struggles to train these deep networks, even when given the advantage and allowed to use",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 481,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 483,
      "text": "an adaptive learning rate unlike the other algorithms. DTP’s struggle might b e the result of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 482,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 484,
      "text": "losing too much of its layerwise target information to o soon–the inverse mapping (or decoder",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 483,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 485,
      "text": "of the layerwise auto-associative structure) requires a strong signal at each iteration to learn",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 484,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 486,
      "text": "and if the signal is too weak or lost, the target pro duced for reconstruction becomes rather",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 485,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 487,
      "text": "useless. However, DTP do es far better than FA across the scenarios, although its lacks",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 486,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 488,
      "text": "the ability to train from zero initialization. Our preliminary experimentation with DTP",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 487,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 489,
      "text": "also uncovered that, in addition to requiring a more complex outer optimization pro cedure",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 488,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 490,
      "text": "(like RMSprop) to achieve decent results, the learning procedure is highly dep endent on its",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 479,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 491,
      "text": "conditions and internal hyper-parameter settings (and there exist few heuristics on good",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 490,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 492,
      "text": "starting points). To make DTP work well, significant tweaking of its settings would b e",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 491,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 493,
      "text": "required on a p er-dataset/p er-architecture basis in order to improve targets for the inverse",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 492,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 494,
      "text": "mapping. Since the error of DTP (or target propagation algorithms in general) is represented",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 493,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 495,
      "text": "as the change in activities of the same set of neurons, if any neural activity is unstable, the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 494,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 496,
      "text": "16",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": 495,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 15,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 497,
      "text": "Local Representation Alignment",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": 495,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 16,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 498,
      "text": "overall algorithm will fail to train the underlying model effectively. Furthermore, because of",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 495,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 16,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 499,
      "text": "the extra computation involved in DTP, it is also far slower than LRA.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 498,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 16,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 500,
      "text": "With respect to DFA, the layers are no longer related through a sequential backward",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 498,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 16,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 501,
      "text": "pathway. This means that the lower-level neurons are disconnected from the forward",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 500,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 16,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 502,
      "text": "propagation pathway when errors are calculated using the feedback pro jection weights. In",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 501,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 16,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 503,
      "text": "contrast, we find that in FA the error signal is still created by a backward pass as in BP, but",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 502,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 16,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 504,
      "text": "this time with the final per-neuron derivatives approximated by the feedback weights that",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 503,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 16,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 505,
      "text": "replace the transpose of the forward weights in the BP global feedback pathway. Hence FA",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 504,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 16,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 506,
      "text": "fails in cases where we have instability or few gradients are acting on participating neurons.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 505,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 16,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 507,
      "text": "DFA actually works fairly well compared to the other baselines if the activation function",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 506,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 16,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 508,
      "text": "is the hyperb olic tangent, and does outperform FA when the logistic sigmoid is utilized.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 507,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 16,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 509,
      "text": "Furthermore, unlike DTP and BP, DFA and FA can train networks from zero, although",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 508,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 16,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 510,
      "text": "LRA does a much better job.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 509,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 16,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 511,
      "text": "Another interesting and imp ortant prop erty of LRA-fdbk is that, unlike all of the other",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 500,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 16,
      "box": [
        0,
        0,
        0,
        0
      ]
    }
  ]
}