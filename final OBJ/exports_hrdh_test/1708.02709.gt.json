{
  "doc_id": "1708.02709",
  "nodes": [
    {
      "id": 0,
      "text": "1",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 1,
      "text": "Recent Trends in Deep Learning Based",
      "label_id": 0,
      "label_name": "Title",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 2,
      "text": "Natural Language Processing",
      "label_id": 0,
      "label_name": "Title",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 3,
      "text": "Tom Young†≡ , Devamanyu Hazarika‡≡, Soujanya Poria⊕≡ , Erik Cambria▽∗",
      "label_id": 1,
      "label_name": "Author",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 4,
      "text": "† School of Information and Electronics, Beijing Institute of Technology, China",
      "label_id": 3,
      "label_name": "Affiliation",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 5,
      "text": "‡ School of Computing, National University of Singapore, Singapore",
      "label_id": 3,
      "label_name": "Affiliation",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 6,
      "text": "⊕ Temasek Laboratories, Nanyang Technological University, Singapore",
      "label_id": 3,
      "label_name": "Affiliation",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 7,
      "text": "▽ School of Computer Science and Engineering, Nanyang Technological University, Singapore",
      "label_id": 3,
      "label_name": "Affiliation",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 8,
      "text": "Abstract",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 9,
      "text": "Deep learning methods employ multiple processing layers to learn hierarchical representations of data, and have produced",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 8,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 10,
      "text": "state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 9,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 11,
      "text": "natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 10,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 12,
      "text": "employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 11,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 13,
      "text": "various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 12,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 14,
      "text": "Index Terms",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 8,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 15,
      "text": "Natural Language Processing, Deep Learning, Word2Vec, Attention, Recurrent Neural Networks, Convolutional Neural Net-",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 14,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 16,
      "text": "works, LSTM, Sentiment Analysis, Question Answering, Dialogue Systems, Parsing, Named-Entity Recognition, POS Tagging,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 15,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 17,
      "text": "Semantic Role Labeling",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 16,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 18,
      "text": "I. INT RO D UC TIO N",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 14,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 19,
      "text": "Natural language processing (NLP) is a theory-motivated range of computational techniques for the automatic analysis and",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 18,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 20,
      "text": "representation of human language. NLP research has evolved from the era of punch cards and batch processing, in which the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 19,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 21,
      "text": "analysis of a sentence could take up to 7 minutes, to the era of Google and the likes of it, in which millions of webpages can",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 20,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 22,
      "text": "be processed in less than a second [1]. NLP enables computers to perform a wide range of natural language related tasks at",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 21,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 23,
      "text": "all levels, ranging from parsing and part-of-speech (POS) tagging, to machine translation and dialogue systems.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 22,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 24,
      "text": "Deep learning architectures and algorithms have already made impressive advances in fields such as computer vision and",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 19,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 25,
      "text": "pattern recognition. Following this trend, recent NLP research is now increasingly focusing on the use of new deep learning",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 24,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 26,
      "text": "methods (see Figure 1). For decades, machine learning approaches targeting NLP problems have been based on shallow models",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 25,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 27,
      "text": "(e.g., SVM and logistic regression) trained on very high dimensional and sparse features. In the last few years, neural networks",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 26,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 28,
      "text": "based on dense vector representations have been producing superior results on various NLP tasks. This trend is sparked by",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 27,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 29,
      "text": "the success of word embeddings [2, 3] and deep learning methods [4]. Deep learning enables multi-level automatic feature",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 28,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 30,
      "text": "representation learning. In contrast, traditional machine learning based NLP systems liaise heavily on hand-crafted features.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 29,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 31,
      "text": "Such hand-crafted features are time-consuming and often incomplete.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 30,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 32,
      "text": "Collobert et al. [5] demonstrated that a simple deep learning framework outperforms most state-of-the-art approaches in",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 24,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 33,
      "text": "several NLP tasks such as named-entity recognition (NER), semantic role labeling (SRL), and POS tagging. Since then,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 32,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 34,
      "text": "numerous complex deep learning based algorithms have been proposed to solve difficult NLP tasks. We review major deep",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 33,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 35,
      "text": "learning related models and methods applied to natural language tasks such as convolutional neural networks (CNNs), recurrent",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 34,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 36,
      "text": "neural networks (RNNs), and recursive neural networks. We also discuss memory-augmenting strategies, attention mechanisms",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 35,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 37,
      "text": "and how unsupervised models, reinforcement learning methods and recently, deep generative models have been employed for",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 36,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 38,
      "text": "language-related tasks.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 37,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 39,
      "text": "To the best of our knowledge, this work is the first of its type to comprehensively cover the most popular deep learning",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 32,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 40,
      "text": "methods in NLP research today 1. The work by Goldberg [6] only presented the basic principles for applying neural networks",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 39,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 41,
      "text": "to NLP in a tutorial manner. We believe this paper will give readers a more comprehensive idea of current practices in this",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 40,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 42,
      "text": "domain.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 41,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 43,
      "text": "The structure of the paper is as follows: Section II introduces the concept of distributed representation, the basis of",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 39,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 44,
      "text": "sophisticated deep learning models; next, Sections III, IV, and V discuss popular models such as convolutional, recurrent,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 43,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 45,
      "text": "and recursive neural networks, as well as their use in various NLP tasks; following, Section VI lists recent applications of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 44,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 46,
      "text": "reinforcement learning in NLP and new developments in unsupervised sentence representation learning; later, Section VII",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 45,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 47,
      "text": "≡ means authors contributed equally",
      "label_id": 13,
      "label_name": "Footnote",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 48,
      "text": "∗ Corresponding author (e-mail: cambria@ntu.edu.sg)",
      "label_id": 13,
      "label_name": "Footnote",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 49,
      "text": "1 We intend to update this article with time as and when significant advances are proposed and used by the community",
      "label_id": 13,
      "label_name": "Footnote",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 50,
      "text": "2",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 51,
      "text": "IMAGE",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 52,
      "text": "Fig. 1: Percentage of deep learning papers in ACL, EMNLP, EACL, NAACL over the last 6 years (long papers).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 51,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 53,
      "text": "illustrates the recent trend of coupling deep learning models with memory modules; finally, Section VIII summarizes the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 46,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 54,
      "text": "performance of a series of deep learning methods on standard datasets about major NLP topics.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 53,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 55,
      "text": "I I . DI S T R I B U TE D RE P RE S ENTAT I ON",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 18,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 56,
      "text": "Statistical NLP has emerged as the primary option for modeling complex natural language tasks. However, in its beginning,",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 55,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 57,
      "text": "it often used to suffer from the notorious curse of dimensionality while learning joint probability functions of language models.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 56,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 58,
      "text": "This led to the motivation of learning distributed representations of words existing in low-dimensional space [7].",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 57,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 59,
      "text": "A. Word Embeddings",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 55,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 60,
      "text": "Distributional vectors or word embeddings (Fig. 2) essentially follow the distributional hypothesis, according to which words",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 59,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 61,
      "text": "with similar meanings tend to occur in similar context. Thus, these vectors try to capture the characteristics of the neighbors of a",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 60,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 62,
      "text": "word. The main advantage of distributional vectors is that they capture similarity between words. Measuring similarity between",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 61,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 63,
      "text": "vectors is possible, using measures such as cosine similarity. Word embeddings are often used as the first data processing layer",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 62,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 64,
      "text": "in a deep learning model. Typically, word embeddings are pre-trained by optimizing an auxiliary objective in a large unlabeled",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 63,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 65,
      "text": "corpus, such as predicting a word based on its context [8, 3], where the learned word vectors can capture general syntactical",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 64,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 66,
      "text": "and semantic information. Thus, these embeddings have proven to be efficient in capturing context similarity, analogies and",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 65,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 67,
      "text": "due to its smaller dimensionality, are fast and efficient in processing core NLP tasks.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 66,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 68,
      "text": "Over the years, the models that create such embeddings have been shallow neural networks and there has not been need",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 60,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 69,
      "text": "for deep networks to create good embeddings. However, deep learning based NLP models invariably represent their words,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 68,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 70,
      "text": "phrases and even sentences using these embeddings. This is in fact a major difference between traditional word count based",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 69,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 71,
      "text": "models and deep learning based models. Word embeddings have been responsible for state-of-the-art results in a wide range",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 70,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 72,
      "text": "of NLP tasks [9, 10, 11, 12].",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 71,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 73,
      "text": "For example, Glorot et al. [13] used embeddings along with stacked denoising autoencoders for domain adaptation in senti-",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 68,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 74,
      "text": "ment classification and Hermann and Blunsom [14] presented combinatory categorial autoencoders to learn the compositionality",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 73,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 75,
      "text": "of sentence. Their wide usage across the recent literature shows their effectiveness and importance in any deep learning model",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 74,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 76,
      "text": "performing a NLP task.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 75,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 77,
      "text": "Distributed representations (embeddings) are mainly learned through context. During 1990s, several research develop-",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 73,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 78,
      "text": "ments [15] marked the foundations of research in distributional semantics. A more detailed summary of these early trends is",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 77,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 79,
      "text": "King(-) Man(+) WomanQueen",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 80,
      "text": "Fig. 2: Distributional vectors represented by a D-dimensional vector where D << V, where V is size of Vocabulary. Figure",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 79,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 81,
      "text": "Source: http://veredshwartz.blogspot.sg.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 80,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 82,
      "text": "3",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 83,
      "text": "Table look-upusing matrix Cword index word indexi th o u t p u t = P (wt = i ∣ c o n t e x t)C (wt−n+1) C (wt−1)wt−n +1 wt−1Softmaxclassification Tanh activationConcatenation",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 84,
      "text": "Fig. 3: Neural Language Model (Figure reproduced from Bengio et al. [7]). C (i) is the ith word embedding.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 83,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 85,
      "text": "provided in [16, 17]. Later developments were adaptations of these early works, which led to creation of topic models like",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 78,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 86,
      "text": "latent Dirichlet allocation [18] and language models [7]. These works laid out the foundations of representation learning in",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 85,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 87,
      "text": "natural language.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 86,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 88,
      "text": "In 2003, Bengio et al. [7] proposed a neural language model which learned distributed representations for words (Fig. 3).",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 77,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 89,
      "text": "Authors argued that these word representations, once compiled into sentence representations using joint probability of word",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 88,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 90,
      "text": "sequences, achieved an exponential number of semantically neighboring sentences. This, in turn, helped in generalization",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 89,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 91,
      "text": "since unseen sentences could now gather higher confidence if word sequences with similar words (in respect to nearby word",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 90,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 92,
      "text": "representation) were already seen.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 91,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 93,
      "text": "Collobert and Weston [19] were the first work to show the utility of pre-trained word embeddings. They proposed a neural",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 88,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 94,
      "text": "network architecture that forms the foundation to many current approaches. The work also establishes word embeddings as",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 93,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 95,
      "text": "a useful tool for NLP tasks. However, the immense popularization of word embeddings was arguably due to Mikolov et al.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 94,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 96,
      "text": "[3] who proposed the continuous bag-of-words (CBOW) and skip-gram models to efficiently construct high-quality distributed",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 95,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 97,
      "text": "vector representations. Propelling their popularity was the unexpected side effect of the vectors exhibiting compositionality, i.e.,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 96,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 98,
      "text": "adding two word vectors results in a vector that is a semantic composite of the individual words, e.g., ‘man’ + ‘royal’ = ‘king’.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 97,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 99,
      "text": "The theoretical justification for this behavior was recently given by Gittens et al. [20], which stated that compositionality is",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 98,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 100,
      "text": "seen only when certain assumptions are held, e.g., the assumption that words need to be uniformly distributed in the embedding",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 99,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 101,
      "text": "space.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 100,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 102,
      "text": "Glove by Pennington et al. [21] is another famous word embedding method which is essentially a “count-based” model.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 93,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 103,
      "text": "Here, the word co-occurrence count matrix is pre-processed by normalizing the counts and log-smoothing operation. This",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 102,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 104,
      "text": "matrix is then factorized to get lower dimensional representations which is done by minimizing a “reconstruction loss”.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 103,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 105,
      "text": "Below, we provide a brief description of the word2vec method proposed by Mikolov et al. [3].",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 102,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 106,
      "text": "B. Word2vec",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 59,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 107,
      "text": "Word embeddings were revolutionized by Mikolov et al. [8, 3] who proposed the CBOW and skip-gram models. CBOW",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 106,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 108,
      "text": "computes the conditional probability of a target word given the context words surrounding it across a window of size k. On",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 107,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 109,
      "text": "the other hand, the skip-gram model does the exact opposite of the CBOW model, by predicting the surrounding context words",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 108,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 110,
      "text": "given the central target word. The context words are assumed to be located symmetrically to the target words within a distance",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 109,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 111,
      "text": "equal to the window size in both directions. In unsupervised settings, the word embedding dimension is determined by the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 110,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 112,
      "text": "accuracy of prediction. As the embedding dimension increases, the accuracy of prediction also increases until it converges at",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 111,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 113,
      "text": "some point, which is considered the optimal embedding dimension as it is the shortest without compromising accuracy.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 112,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 114,
      "text": "Let us consider a simplified version of the CBOW model where only one word is considered in the context. This essentially",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 107,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 115,
      "text": "replicates a bigram language model.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 114,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 116,
      "text": "As shown in Fig. 4, the CBOW model is a simple fully connected neural network with one hidden layer. The input layer,",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 114,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 117,
      "text": "which takes the one-hot vector of context word has V neurons while the hidden layer has N neurons. The output layer is softmax",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 116,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 118,
      "text": "probability over all words in the vocabulary. The layers are connected by weight matrix W ∈ RV ×N and W′ ∈ RH ×V ,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 117,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 119,
      "text": "4",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 120,
      "text": "IMAGE",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 121,
      "text": "Fig. 4: Model for CBOW (Figure source: Rong [22])",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 120,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 122,
      "text": "respectively. Each word from the vocabulary is finally represented as two learned vectors vc and vw , corresponding to context",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 118,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 123,
      "text": "and target word representations, respectively. Thus, kth word in the vocabulary will have",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 122,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 124,
      "text": "vc = W(k,.) and vw = W(.,k) (1)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 123,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 125,
      "text": "Overall, for any word wi with given context word c as input,",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 116,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 126,
      "text": "p (cid:16) wic (cid:17) = yi = eui(cid:80)Vi=1 eui where, ui = v Twi .vc (2)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 125,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 127,
      "text": "The parameters θ = {vw, vc}w,c ∈ Vocab are learned by defining the objective function as the log-likelihood and finding its",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 125,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 128,
      "text": "gradient as",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 127,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 129,
      "text": "l(θ) = (cid:88)w∈Vocab log (cid:16)p (cid:16) wc (cid:17)(cid:17) (3)∂l(θ)∂ vw = vc (cid:16)1 − p (cid:16) wc (cid:17)(cid:17) (4)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 128,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 130,
      "text": "In the general CBOW model, all the one-hot vectors of context words are taken as input simultaneously, i.e,",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 127,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 131,
      "text": "h = WT(x1 + x2 + ... + xc) (5)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 130,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 132,
      "text": "One limitation of individual word embeddings is their inability to represent phrases [3], where the combination of two or",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 130,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 133,
      "text": "more words – e.g., idioms like “hot potato” or named entities such as “Boston Globe’ – does not represent the combination",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 132,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 134,
      "text": "of meanings of individual words. One solution to this problem, as explored by Mikolov et al. [3], is to identify such phrases",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 133,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 135,
      "text": "based on word co-occurrence and train embeddings for them separately. Later methods have explored directly learning n-gram",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 134,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 136,
      "text": "embeddings from unlabeled data [23].",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 135,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 137,
      "text": "Another limitation comes from learning embeddings based only on a small window of surrounding words, sometimes",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 132,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 138,
      "text": "words such as good and bad share almost the same embedding [24], which is problematic if used in tasks such as sentiment",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 137,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 139,
      "text": "analysis [25]. At times these embeddings cluster semantically-similar words which have opposing sentiment polarities. This",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 138,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 140,
      "text": "leads the downstream model used for the sentiment analysis task to be unable to identify this contrasting polarities leading to",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 139,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 141,
      "text": "poor performance. Tang et al. [26] addressed this problem by proposing sentiment specific word embedding (SSWE). Authors",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 140,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 142,
      "text": "incorporated the supervised sentiment polarity of text in their loss functions while learning the embeddings.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 141,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 143,
      "text": "A general caveat for word embeddings is that they are highly dependent on the applications in which it is used. Labutov",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 137,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 144,
      "text": "and Lipson [27] proposed task specific embeddings which retrain the word embeddings to align them in the current task space.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 143,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 145,
      "text": "This is very important as training embeddings from scratch requires large amount of time and resource. Mikolov et al. [8] tried",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 144,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 146,
      "text": "to address this issue by proposing negative sampling which does frequency-based sampling of negative terms while training",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 145,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 147,
      "text": "the word2vec model.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 146,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 148,
      "text": "Traditional word embedding algorithms assign a distinct vector to each word. This makes them unable to account for",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 143,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 149,
      "text": "polysemy. In a recent work, Upadhyay et al. [28] provided an innovative way to address this deficit. The authors leveraged",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 148,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 150,
      "text": "multilingual parallel data to learn multi-sense word embeddings. For example, the English word bank, when translated to",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 149,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 151,
      "text": "French provides two different words: banc and banque representing financial and geographical meanings, respectively. Such",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 150,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 152,
      "text": "multilingual distributional information helped them in accounting for polysemy.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 151,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 153,
      "text": "Table I provides a directory of existing frameworks that are frequently used for creating embeddings which are further",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 148,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 154,
      "text": "incorporated into deep learning models.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 153,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 155,
      "text": "5",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 156,
      "text": "Framework Language URLS-Space Java https://github.com/fozziethebeat/S- SpaceSemanticvectors Java https://github.com/semanticvectors/Gensim Python https://radimrehurek.com/gensim/Pydsm Python https://github.com/jimmycallin/pydsmDissect Python http://clic.cimec.unitn.it/composes/toolkit/FastText Python https://fasttext.cc/Elmo Python https://tfhub.dev/google/elmo/2",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 157,
      "text": "TABLE I: Frameworks providing word embedding tools and methods.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 156,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 158,
      "text": "C. Character Embeddings",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 106,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 159,
      "text": "Word embeddings are able to capture syntactic and semantic information, yet for tasks such as POS-tagging and NER, intra-",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 158,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 160,
      "text": "word morphological and shape information can also be very useful. Generally speaking, building natural language understanding",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 159,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 161,
      "text": "systems at the character level has attracted certain research attention [29, 30, 31, 32]. Better results on morphologically rich",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 160,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 162,
      "text": "languages are reported in certain NLP tasks. Santos and Guimaraes [31] applied character-level representations, along with",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 161,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 163,
      "text": "word embeddings for NER, achieving state-of-the-art results in Portuguese and Spanish corpora. Kim et al. [29] showed positive",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 162,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 164,
      "text": "results on building a neural language model using only character embeddings. Ma et al. [33] exploited several embeddings,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 163,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 165,
      "text": "including character trigrams, to incorporate prototypical and hierarchical information for learning pre-trained label embeddings",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 164,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 166,
      "text": "in the context of NER.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 165,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 167,
      "text": "A common phenomenon for languages with large vocabularies is the unknown word issue, also known as out-of-vocabulary",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 159,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 168,
      "text": "(OOV) words. Character embeddings naturally deal with it since each word is considered as no more than a composition",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 167,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 169,
      "text": "of individual letters. In languages where text is not composed of separated words but individual characters and the semantic",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 168,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 170,
      "text": "meaning of words map to its compositional characters (such as Chinese), building systems at the character level is a natural",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 169,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 171,
      "text": "choice to avoid word segmentation [34]. Thus, works employing deep learning applications on such languages tend to prefer",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 170,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 172,
      "text": "character embeddings over word vectors [35]. For example, Peng et al. [36] proved that radical-level processing could greatly",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 171,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 173,
      "text": "improve sentiment classification performance. In particular, the authors proposed two types of Chinese radical-based hierarchical",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 172,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 174,
      "text": "embeddings, which incorporate not only semantics at radical and character level, but also sentiment information. Bojanowski",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 173,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 175,
      "text": "et al. [37] also tried to improve the representation of words by using character-level information in morphologically-rich",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 174,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 176,
      "text": "languages. They approached the skip-gram method by representing words as bag-of-character n-grams. Their work thus had",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 175,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 177,
      "text": "the effectiveness of the skip-gram model along with addressing some persistent issues of word embeddings. The method was",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 176,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 178,
      "text": "also fast, which allowed training models on large corpora quickly. Popularly known as FastText, such a method stands out over",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 177,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 179,
      "text": "previous methods in terms of speed, scalability, and effectiveness.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 178,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 180,
      "text": "Apart from character embeddings, different approaches have been proposed for OOV handling. Herbelot and Baroni [38]",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 167,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 181,
      "text": "provided on-the-fly OOV handling by initializing the unknown words as the sum of the context words and refining these",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 180,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 182,
      "text": "words with a high learning rate. However, their approach is yet to be tested on typical NLP tasks. Pinter et al. [39] provided",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 181,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 183,
      "text": "an interesting approach of training a character-based model to recreate pre-trained embeddings. This allowed them to learn a",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 182,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 184,
      "text": "compositional mapping form character to word embedding, thus tackling the OOV problem.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 183,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 185,
      "text": "Despite the ever growing popularity of distributional vectors, recent discussions on their relevance in the long run have",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 180,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 186,
      "text": "cropped up. For example, Lucy and Gauthier [40] has recently tried to evaluate how well the word vectors capture the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 185,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 187,
      "text": "necessary facets of conceptual meaning. The authors have discovered severe limitations in perceptual understanding of the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 186,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 188,
      "text": "concepts behind the words, which cannot be inferred from distributional semantics alone. A possible direction for mitigating",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 187,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 189,
      "text": "these deficiencies will be grounded learning, which has been gaining popularity in this research domain.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 188,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 190,
      "text": "D. Contextualized Word Embeddings",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 158,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 191,
      "text": "The quality of word representations is generally gauged by its ability to encode syntactical information and handle polysemic",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 190,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 192,
      "text": "behavior (or word senses). These properties result in improved semantic word representations. Recent approaches in this area",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 191,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 193,
      "text": "encode such information into its embeddings by leveraging the context. These methods provide deeper networks that calculate",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 192,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 194,
      "text": "word representations as a function of its context.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 193,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 195,
      "text": "Traditional word embedding methods such as Word2Vec and Glove consider all the sentences where a word is present in",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 191,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 196,
      "text": "order to create a global vector representation of that word. However, a word can have completely different senses or meanings",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 195,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 197,
      "text": "in the contexts. For example, lets consider these two sentences - 1) “The bank will not be accepting cash on Saturdays” 2)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 196,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 198,
      "text": "“The river overflowed the bank.”. The word senses of bank are different in these two sentences depending on its context.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 197,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 199,
      "text": "Reasonably, one might want two different vector representations of the word bank based on its two different word senses.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 198,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 200,
      "text": "The new class of models adopt this reasoning by diverging from the concept of global word representations and proposing",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 199,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 201,
      "text": "contextual word embeddings instead.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 200,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 202,
      "text": "Embedding from Language Model (ELMo) [41] is one such method that provides deep contextual embeddings. ELMo",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 195,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 203,
      "text": "produces word embeddings for each context where the word is used, thus allowing different representations for varying senses",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 202,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 204,
      "text": "6",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 205,
      "text": "of the same word. Specifically, for N different sentences where a word w is present, ELMo generates N different representations",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 203,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 206,
      "text": "of w i.e., w1, w2, ˙,wN .",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 205,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 207,
      "text": "The mechanism of ELMo is based on the representation obtained from a bidirectional language model. A bidirectional",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 202,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 208,
      "text": "language model (biLM) constitutes of two language models (LM) 1) forward LM and 2) backward LM. A forward LM takes",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 207,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 209,
      "text": "input representation xLMk for each of the kth token and passes it through L layers of forward LSTM to get representations",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 208,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 210,
      "text": "h LMk,j where j = 1, . . . , L. Each of these representations, being hidden representations of recurrent neural networks, is context",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 209,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 211,
      "text": "dependent. A forward LM can be seen as a method to model the joint probability of a sequence of tokens: p (t1, t2 , . . . , tN ) =",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 210,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 212,
      "text": "(cid:81)Nk=1 p (tk |t1, t2 , . . . , tk−1 ). At a timestep k − 1 the forward LM predicts the next token tk given the previous observed tokens",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 211,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 213,
      "text": "t1 , t2, ..., tk . This is typically achieved by placing a softmax layer on top of the final LSTM in a forward LM. On the other",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 212,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 214,
      "text": "hand, a backward LM models the same joint probability of the sequence by predicting the previous token given the future",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 213,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 215,
      "text": "tokens: p (t1, t2 , . . . , tN ) = (cid:81)Nk=1 p (tk |tk+1 , tk+2 , . . . , tN ). In other words, a backward LM is similar to forward LM which",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 214,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 216,
      "text": "processes a sequence with the order being reversed. The training of the biLM model involves modeling the log-likelihood of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 215,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 217,
      "text": "both the sentence orientations. Finally, hidden representations from both LMs are concetenated to compose the final token",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 216,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 218,
      "text": "vectors [42].",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 217,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 219,
      "text": "For each tokem, ELMo extracts the intermediate layer representations from the biLM and performs a linear combination",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 207,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 220,
      "text": "based on the given downstream task. A L-layer biLM contains 2L + 1 set of representations as shown below -",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 219,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 221,
      "text": "Rk = (cid:110) xLMk , −→ h LMk,j , ←− h LMk,j |j = 1, . . . , L (cid:111) = (cid:8)hLMk,j |j = 0, . . . , L(cid:9) (6)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 220,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 222,
      "text": "Here, hLMk,0 is the token representation at the lowest level. One can use either character or word embeddings to initialize",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 219,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 223,
      "text": "hLMk ,0 . For other values of j ,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 222,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 224,
      "text": "hLMk,j = (cid:104)−→ h LMk,j , ←− h LMk,j (cid:105) ∀j = 1, . . . , L. (7)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 223,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 225,
      "text": "ELMo flattens all layers in R in a single vector such that -",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 222,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 226,
      "text": "ELMotaskk = E (cid:0)Rk ; Θtask (cid:1) = γ task L (cid:88) j=0 staskj hLMk,j (8)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 225,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 227,
      "text": "In Eq. 8, staskj is the softmax-normalized weight vector to combine the representations of different layers. γtask is a",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 225,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 228,
      "text": "hyperparameter which helps in optimization and task specific scaling of the ELMo representation. ELMo produces varied",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 227,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 229,
      "text": "word representations for the same word in different sentences. According to Peters et al. [41], it is always beneficial to",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 228,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 230,
      "text": "combine ELMo word representations with standard global word representations like Glove and Word2Vec.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 229,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 231,
      "text": "Off-late, there has been a surge of interest in pre-trained language models for myriad of natural language tasks [43].",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 227,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 232,
      "text": "Language modeling is chosen as the pre-training objective as it is widely considered to incorporate multiple traits of natual",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 231,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 233,
      "text": "language understanding and generation. A good language model requires learning complex characteristics of language involving",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 232,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 234,
      "text": "syntactical properties and also semantical coherence. Thus, it is believed that unsupervised training on such objectives would",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 233,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 235,
      "text": "infuse better linguistic knowledge into the networks than random initialization. The generative pre-training and discriminative",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 234,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 236,
      "text": "fine-tuning procedure is also desirable as the pre-training is unsupervised and does not require any manual labeling.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 235,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 237,
      "text": "Radford et al. [44] proposed similar pre-trained model, the OpenAI-GPT, by adapting the Transformer (see section IV-E).",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 231,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 238,
      "text": "Recently, Devlin et al. [45] proposed BERT which utilizes a transformer network to pre-train a language model for extracting",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 237,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 239,
      "text": "contextual word embeddings. Unlike ELMo and OpenAI-GPT, BERT uses different pre-training tasks for language modeling.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 238,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 240,
      "text": "In one of the tasks, BERT randomly masks a percentage of words in the sentences and only predicts those masked words. In",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 239,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 241,
      "text": "the other task, BERT predicts the next sentence given a sentence. This task in particular tries to model the relationship among",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 240,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 242,
      "text": "two sentences which is supposedly not captured by traditional bidirectional language models. Consequently, this particular",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 241,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 243,
      "text": "pre-training scheme helps BERT to outperform state-of-the-art techniques by a large margin on key NLP tasks such as QA,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 242,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 244,
      "text": "Natural Language Inference (NLI) where understanding relation among two sentences is very important. We discuss the impact",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 243,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 245,
      "text": "of these proposed models and the performance achieved by them in section VIII-I.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 244,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 246,
      "text": "The described approaches for contextual word embeddings promises better quality representations for words. The pre-trained",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 237,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 247,
      "text": "deep language models also provide a headstart for downstream tasks in the form of transfer learning. This approach has been",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 246,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 248,
      "text": "extremely popular in computer vision tasks. Whether there would be similar trends in the NLP community, where researchers",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 247,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 249,
      "text": "and practitioners would prefer such models over traditional variants remains to be seen in the future.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 248,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 250,
      "text": "III . CON V O LUT I ONAL N EURA L NET W O R KS",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 55,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 251,
      "text": "Following the popularization of word embeddings and its ability to represent words in a distributed space, the need arose",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 250,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 252,
      "text": "for an effective feature function that extracts higher-level features from constituting words or n-grams. These abstract features",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 251,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 253,
      "text": "7",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 254,
      "text": "wo wN−1InputSentenceLookup tableFeature 1Feature kConvolutionlayerMax-poolover timeFully Connected LayerSoftmax Classificationw1",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 255,
      "text": "Fig. 5: CNN framework used to perform word wise class prediction (Figure source: Collobert and Weston [19])",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 254,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 256,
      "text": "would then be used for numerous NLP tasks such as sentiment analysis, summarization, machine translation, and question",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 252,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 257,
      "text": "answering (QA). CNNs turned out to be the natural choice given their effectiveness in computer vision tasks [46, 47, 48].",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 256,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 258,
      "text": "The use of CNNs for sentence modeling traces back to Collobert and Weston [19]. This work used multi-task learning to",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 251,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 259,
      "text": "output multiple predictions for NLP tasks such as POS tags, chunks, named-entity tags, semantic roles, semantically-similar",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 258,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 260,
      "text": "words and a language model. A look-up table was used to transform each word into a vector of user-defined dimensions.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 259,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 261,
      "text": "Thus, an input sequence {s1 , s2, ...sn} of n words was transformed into a series of vectors {ws1 , ws2 , ...wsn } by applying",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 260,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 262,
      "text": "the look-up table to each of its words (Fig. 5).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 261,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 263,
      "text": "This can be thought of as a primitive word embedding method whose weights were learned in the training of the network.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 258,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 264,
      "text": "In [5], Collobert extended his work to propose a general CNN-based framework to solve a plethora of NLP tasks. Both these",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 263,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 265,
      "text": "works triggered a huge popularization of CNNs amongst NLP researchers. Given that CNNs had already shown their mettle",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 264,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 266,
      "text": "for computer vision tasks, it was easier for people to believe in their performance.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 265,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 267,
      "text": "CNNs have the ability to extract salient n-gram features from the input sentence to create an informative latent semantic",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 263,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 268,
      "text": "representation of the sentence for downstream tasks. This application was pioneered by Collobert et al. [5], Kalchbrenner et al.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 267,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 269,
      "text": "[49], Kim [50], which led to a huge proliferation of CNN-based networks in the succeeding literature. Below, we describe the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 268,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 270,
      "text": "working of a simple CNN-based sentence modeling network:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 269,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 271,
      "text": "A. Basic CNN",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 250,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 272,
      "text": "1) Sentence Modeling: For each sentence, let wi ∈ Rd represent the word embedding for the ith word in the sentence,",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 271,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 273,
      "text": "where d is the dimension of the word embedding. Given that a sentence has n words, the sentence can now be represented as",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 272,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 274,
      "text": "an embedding matrix W ∈ Rn×d. Fig. 6 depicts such a sentence as an input to the CNN framework.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 273,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 275,
      "text": "Let wi:i+j refer to the concatenation of vectors wi, wi+1, ...wj . Convolution is performed on this input embedding layer.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 272,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 276,
      "text": "It involves a filter k ∈ Rhd which is applied to a window of h words to produce a new feature. For example, a feature ci is",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 275,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 277,
      "text": "generated using the window of words wi:i+h−1 by",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 276,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 278,
      "text": "ci = f (wi:i+h−1.kT + b) (9)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 277,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 279,
      "text": "Here, b ∈ R is the bias term and f is a non-linear activation function, for example the hyperbolic tangent. The filter k is",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 275,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 280,
      "text": "applied to all possible windows using the same weights to create the feature map.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 279,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 281,
      "text": "c = [c1, c2, ..., cn−h+1] (10)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 280,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 282,
      "text": "8",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 283,
      "text": "",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 284,
      "text": "Fig. 6: CNN modeling on text (Figure source: Zhang and Wallace [51])",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 283,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 285,
      "text": "In a CNN, a number of convolutional filters, also called kernels (typically hundreds), of different widths slide over the",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 279,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 286,
      "text": "entire word embedding matrix. Each kernel extracts a specific pattern of n-gram. A convolution layer is usually followed by",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 285,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 287,
      "text": "a max-pooling strategy, ˆc = max{c}, which subsamples the input typically by applying a max operation on each filter. This",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 286,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 288,
      "text": "strategy has two primary reasons.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 287,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 289,
      "text": "Firstly, max pooling provides a fixed-length output which is generally required for classification. Thus, regardless the size of",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 285,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 290,
      "text": "the filters, max pooling always maps the input to a fixed dimension of outputs. Secondly, it reduces the output’s dimensionality",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 289,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 291,
      "text": "while keeping the most salient n-gram features across the whole sentence. This is done in a translation invariant manner where",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 290,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 292,
      "text": "each filter is now able to extract a particular feature (e.g., negations) from anywhere in the sentence and add it to the final",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 291,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 293,
      "text": "sentence representation.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 292,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 294,
      "text": "The word embeddings can be initialized randomly or pre-trained on a large unlabeled corpora (as in Section II). The",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 289,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 295,
      "text": "latter option is sometimes found beneficial to performance, especially when the amount of labeled data is limited [50]. This",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 294,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 296,
      "text": "combination of convolution layer followed by max pooling is often stacked to create deep CNN networks. These sequential",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 295,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 297,
      "text": "convolutions help in improved mining of the sentence to grasp a truly abstract representations comprising rich semantic",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 296,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 298,
      "text": "information. The kernels through deeper convolutions cover a larger part of the sentence until finally covering it fully and",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 297,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 299,
      "text": "creating a global summarization of the sentence features.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 298,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 300,
      "text": "2) Window Approach: The above-mentioned architecture allows for modeling of complete sentences into sentence repre-",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 294,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 301,
      "text": "sentations. However, many NLP tasks, such as NER, POS tagging, and SRL, require word-based predictions. To adapt CNNs",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 300,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 302,
      "text": "for such tasks, a window approach is used, which assumes that the tag of a word primarily depends on its neighboring words.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 301,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 303,
      "text": "For each word, thus, a fixed-size window surrounding itself is assumed and the sub-sentence ranging within the window is",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 302,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 304,
      "text": "considered. A standalone CNN is applied to this sub-sentence as explained earlier and predictions are attributed to the word",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 303,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 305,
      "text": "in the center of the window. Following this approach, Poria et al. [52] employed a multi-level deep CNN to tag each word in",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 304,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 306,
      "text": "a sentence as a possible aspect or non-aspect. Coupled with a set of linguistic patterns, their ensemble classifier managed to",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 305,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 307,
      "text": "perform well in aspect detection.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 306,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 308,
      "text": "The ultimate goal of word-level classification is generally to assign a sequence of labels to the entire sentence. In such cases,",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 300,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 309,
      "text": "structured prediction techniques such as conditional random field (CRF) are sometimes employed to better capture dependencies",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 308,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 310,
      "text": "between adjacent class labels and finally generate cohesive label sequence giving maximum score to the whole sentence [53].",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 309,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 311,
      "text": "To get a larger contextual range, the classic window approach is often coupled with a time-delay neural network (TDNN) [54].",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 308,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 312,
      "text": "Here, convolutions are performed across all windows throughout the sequence. These convolutions are generally constrained",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 311,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 313,
      "text": "by defining a kernel having a certain width. Thus, while the classic window approach only considers the words in the window",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 312,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 314,
      "text": "around the word to be labeled, TDNN considers all windows of words in the sentence at the same time. At times, TDNN",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 313,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 315,
      "text": "layers are also stacked like CNN architectures to extract local features in lower layers and global features in higher layers [5].",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 314,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 316,
      "text": "B. Applications",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 271,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 317,
      "text": "In this section, we present some of the crucial works that employed CNNs on NLP tasks to set state-of-the-art benchmarks",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 316,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 318,
      "text": "in their respective times.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 317,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 319,
      "text": "Kim [50] explored using the above architecture for a variety of sentence classification tasks, including sentiment, subjectivity",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 317,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 320,
      "text": "and question type classification, showing competitive results. This work was quickly adapted by researchers given its simple",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 319,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 7,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 321,
      "text": "9",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 322,
      "text": "(a) Figure A (b) Figure B",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 323,
      "text": "Fig. 7: Top 7-grams by four learned 7-gram kernels; each kernel is sensitive to a specific kind of 7-gram (Figure",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 322,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 324,
      "text": "Source: Kalchbrenner et al. [49])",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 323,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 325,
      "text": "yet effective network. After training for a specific task, the randomly initialized convolutional kernels became specific n-gram",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 320,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 326,
      "text": "feature detectors that were useful for that target task (Fig. 7). This simple network, however, had many shortcomings with the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 325,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 327,
      "text": "CNN’s inability to model long distance dependencies standing as the main issue.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 326,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 328,
      "text": "This issue was partly handled by Kalchbrenner et al. [49], who published a prominent paper where they proposed a dynamic",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 319,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 329,
      "text": "convolutional neural network (DCNN) for semantic modeling of sentences. They proposed dynamic k-max pooling strategy",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 328,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 330,
      "text": "which, given a sequence p selects the k most active features. The selection preserved the order of the features but was insensitive",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 329,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 331,
      "text": "to their specific positions (Fig. 8). Built on the concept of TDNN, they added this dynamic k-max pooling strategy to create",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 330,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 332,
      "text": "a sentence model. This combination allowed filters with small width to span across a long range within the input sentence,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 331,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 333,
      "text": "thus accumulating crucial information across the sentence. In the induced subgraph (Fig. 8), higher order features had highly",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 332,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 334,
      "text": "variable ranges that could be either short and focused or global and long as the input sentence. They applied their model on",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 333,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 335,
      "text": "multiple tasks, including sentiment prediction and question type classification, achieving significant results. Overall, this work",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 334,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 336,
      "text": "commented on the range of individual kernels while trying to model contextual semantics and proposed a way to extend their",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 335,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 337,
      "text": "reach.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 336,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 338,
      "text": "Tasks involving sentiment analysis also require effective extraction of aspects along with their sentiment polarities [55].",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 328,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 339,
      "text": "Ruder et al. [56] applied a CNN where in the input they concatenated an aspect vector with the word embeddings to get",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 338,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 340,
      "text": "competitive results. CNN modeling approach varies amongst different length of texts. Such differences were seen in many",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 339,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 341,
      "text": "works like Johnson and Zhang [23], where performance on longer text worked well as opposed to shorter texts. Wang et al.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 340,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 342,
      "text": "[57] proposed the usage of CNN for modeling representations of short texts, which suffer from the lack of available context",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 341,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 343,
      "text": "and, thus, require extra efforts to create meaningful representations. The authors proposed semantic clustering which introduced",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 342,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 344,
      "text": "multi-scale semantic units to be used as external knowledge for the short texts. CNN was used to combine these units and",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 343,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 345,
      "text": "form the overall representation. In fact, this requirement of high context information can be thought of as a caveat for CNN-",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 344,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 346,
      "text": "based models. NLP tasks involving microtexts using CNN-based methods often require the need of additional information and",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 345,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 347,
      "text": "external knowledge to perform as per expectations. This fact was also observed in [58], where authors performed sarcasm",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 346,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 348,
      "text": "detection in Twitter texts using a CNN network. Auxiliary support, in the form of pre-trained networks trained on emotion,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 347,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 349,
      "text": "sentiment and personality datasets was used to achieve state-of-the-art performance.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 348,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 350,
      "text": "CNNs have also been extensively used in other tasks. For example, Denil et al. [59] applied DCNN to map meanings of",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 338,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 351,
      "text": "words that constitute a sentence to that of documents for summarization. The DCNN learned convolution filters at both the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 350,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 352,
      "text": "sentence and document level, hierarchically learning to capture and compose low-level lexical features into high-level semantic",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 351,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 353,
      "text": "concepts. The focal point of this work was the introduction of a novel visualization technique of the learned representations,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 352,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 354,
      "text": "which provided insights not only in the learning process but also for automatic summarization of texts.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 353,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 355,
      "text": "CNN models are also suitable for certain NLP tasks that require semantic matching beyond classification [60]. A similar",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 350,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 356,
      "text": "model to the above CNN architecture (Fig. 6) was explored in [61] for information retrieval. The CNN was used for projecting",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 355,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 357,
      "text": "queries and documents to a fixed-dimension semantic space, where cosine similarity between the query and documents was",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 356,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 358,
      "text": "used for ranking documents regarding a specific query. The model attempted to extract rich contextual structures in a query",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 357,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 359,
      "text": "or a document by considering a temporal context window in a word sequence. This captured the contextual features at the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 358,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 360,
      "text": "word n-gram level. The salient word n-grams is then discovered by the convolution and max-pooling layers which are then",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 359,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 361,
      "text": "aggregated to form the overall sentence vector.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 360,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 362,
      "text": "In the domain of QA, Yih et al. [62] proposed to measure the semantic similarity between a question and entries in a",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 355,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 363,
      "text": "knowledge base (KB) to determine what supporting fact in the KB to look for when answering a question. To create semantic",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 362,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 364,
      "text": "representations, a CNN similar to the one in Fig. 6 was used. Unlike the classification setting, the supervision signal came",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 363,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 365,
      "text": "from positive or negative text pairs (e.g., query-document), instead of class labels. Subsequently, Dong et al. [63] introduced",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 364,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 366,
      "text": "a multi-column CNN (MCCNN) to analyze and understand questions from multiple aspects and create their representations.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 365,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 367,
      "text": "MCCNN used multiple column networks to extract information from aspects comprising answer types and context from the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 366,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 368,
      "text": "input questions. By representing entities and relations in the KB with low-dimensional vectors, they used question-answer",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 367,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 8,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 369,
      "text": "10",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 370,
      "text": "xnx2x1 xnx 2x1",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 371,
      "text": "Fig. 8: DCNN subgraph. With dynamic pooling, a filter with small width at the higher layers can relate phrases far apart in",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 370,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 372,
      "text": "the input sentence (Figure Source: Kalchbrenner et al. [49])",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 371,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 373,
      "text": "pairs to train the CNN model so as to rank candidate answers. Severyn and Moschitti [64] also used CNN network to model",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 368,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 374,
      "text": "optimal representations of question and answer sentences. They proposed additional features in the embeddings in the form",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 373,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 375,
      "text": "of relational information given by matching words between the question and answer pair. These parameters were tuned by the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 374,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 376,
      "text": "network. This simple network was able to produce comparable results to state-of-the-art methods.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 375,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 377,
      "text": "CNNs are wired in a way to capture the most important information in a sentence. Traditional max-pooling strategies",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 362,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 378,
      "text": "perform this in a translation invariant form. However, this often misses valuable information present in multiple facts within",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 377,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 379,
      "text": "the sentence. To overcome this loss of information for multiple-event modeling, Chen et al. [65] proposed a modified pooling",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 378,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 380,
      "text": "strategy: dynamic multi-pooling CNN (DMCNN). This strategy used a novel dynamic multi-pooling layer that, as the name",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 379,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 381,
      "text": "suggests, incorporates event triggers and arguments to reserve more crucial information from the pooling layer.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 380,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 382,
      "text": "CNNs inherently provide certain required features like local connectivity, weight sharing, and pooling. This puts forward",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 377,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 383,
      "text": "some degree of invariance which is highly desired in many tasks. Speech recognition also requires such invariance and, thus,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 382,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 384,
      "text": "Abdel-Hamid et al. [66] used a hybrid CNN-HMM model which provided invariance to frequency shifts along the frequency",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 383,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 385,
      "text": "axis. This variability is often found in speech signals due to speaker differences. They also performed limited weight sharing",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 384,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 386,
      "text": "which led to a smaller number of pooling parameters, resulting in lower computational complexity. Palaz et al. [67] performed",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 385,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 387,
      "text": "extensive analysis of CNN-based speech recognition systems when given raw speech as input. They showed the ability of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 386,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 388,
      "text": "CNNs to directly model the relationship between raw input and phones, creating a robust automatic speech recognition system.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 387,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 389,
      "text": "Tasks like machine translation require perseverance of sequential information and long-term dependency. Thus, structurally",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 382,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 390,
      "text": "they are not well suited for CNN networks, which lack these features. Nevertheless, Tu et al. [68] addressed this task by",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 389,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 391,
      "text": "considering both the semantic similarity of the translation pair and their respective contexts. Although this method did not",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 390,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 392,
      "text": "address the sequence perseverance problem, it allowed them to get competitive results amongst other benchmarks.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 391,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 393,
      "text": "Overall, CNNs are extremely effective in mining semantic clues in contextual windows. However, they are very data heavy",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 389,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 394,
      "text": "models. They include a large number of trainable parameters which require huge training data. This poses a problem when",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 393,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 395,
      "text": "scarcity of data arises. Another persistent issue with CNNs is their inability to model long-distance contextual information and",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 394,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 396,
      "text": "preserving sequential order in their representations [49, 68]. Other networks like recursive models (explained below) reveal",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 395,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 397,
      "text": "themselves as better suited for such learning.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 396,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 398,
      "text": "IV. R EC URR ENT N EU RA L NE T W OR KS",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 250,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 399,
      "text": "RNNs [69] use the idea of processing sequential information. The term “recurrent” applies as they perform the same task",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 398,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 400,
      "text": "over each instance of the sequence such that the output is dependent on the previous computations and results. Generally, a",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 399,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 401,
      "text": "fixed-size vector is produced to represent a sequence by feeding tokens one by one to a recurrent unit. In a way, RNNs have",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 400,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 402,
      "text": "“memory” over previous computations and use this information in current processing. This template is naturally suited for",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 401,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 403,
      "text": "many NLP tasks such as language modeling [2, 70, 71], machine translation [72, 73, 74], speech recognition [75, 76, 77, 78],",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 402,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 404,
      "text": "image captioning [79]. This made RNNs increasingly popular for NLP applications in recent years.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 403,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 405,
      "text": "A. Need for Recurrent Networks",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 398,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 406,
      "text": "In this section, we analyze the fundamental properties that favored the popularization of RNNs in a multitude of NLP tasks.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 405,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 407,
      "text": "Given that an RNN performs sequential processing by modeling units in sequence, it has the ability to capture the inherent",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 406,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 408,
      "text": "sequential nature present in language, where units are characters, words or even sentences. Words in a language develop their",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 407,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 9,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 409,
      "text": "11",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 410,
      "text": "W otot−1 ot+1UnfoldV V V VW WU U UWU xtxt−1 xt+1xoh htht−1 ht+1",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 411,
      "text": "Fig. 9: Simple RNN network (Figure Source: LeCun et al. [90])",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 410,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 412,
      "text": "semantical meaning based on the previous words in the sentence. A simple example stating this would be the difference in",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 408,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 413,
      "text": "meaning between “dog” and “hot dog”. RNNs are tailor-made for modeling such context dependencies in language and similar",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 412,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 414,
      "text": "sequence modeling tasks, which resulted to be a strong motivation for researchers to use RNNs over CNNs in these areas.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 413,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 415,
      "text": "Another factor aiding RNN’s suitability for sequence modeling tasks lies in its ability to model variable length of text,",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 406,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 416,
      "text": "including very long sentences, paragraphs and even documents [80]. Unlike CNNs, RNNs have flexible computational steps",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 415,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 417,
      "text": "that provide better modeling capability and create the possibility to capture unbounded context. This ability to handle input of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 416,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 418,
      "text": "arbitrary length became one of the selling points of major works using RNNs [81].",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 417,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 419,
      "text": "Many NLP tasks require semantic modeling over the whole sentence. This involves creating a gist of the sentence in",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 415,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 420,
      "text": "a fixed dimensional hyperspace. RNN’s ability to summarize sentences led to their increased usage for tasks like machine",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 419,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 421,
      "text": "translation [82] where the whole sentence is summarized to a fixed vector and then mapped back to the variable-length target",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 420,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 422,
      "text": "sequence.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 421,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 423,
      "text": "RNN also provides the network support to perform time distributed joint processing. Most of the sequence labeling",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 419,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 424,
      "text": "tasks like POS tagging [32] come under this domain. More specific use cases include applications such as multi-label text",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 423,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 425,
      "text": "categorization [83], multimodal sentiment analysis [84, 85, 86], and subjectivity detection [87].",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 424,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 426,
      "text": "The above points enlist some of the focal reasons that motivated researchers to opt for RNNs. However, it would be gravely",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 423,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 427,
      "text": "wrong to make conclusions on the superiority of RNNs over other deep networks. Recently, several works provided contrasting",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 426,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 428,
      "text": "evidence on the superiority of CNNs over RNNs. Even in RNN-suited tasks like language modeling, CNNs achieved competitive",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 427,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 429,
      "text": "performance over RNNs [88]. Both CNNs and RNNs have different objectives when modeling a sentence. While RNNs try",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 428,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 430,
      "text": "to create a composition of an arbitrarily long sentence along with unbounded context, CNNs try to extract the most important",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 429,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 431,
      "text": "n-grams. Although they prove an effective way to capture n-gram features, which is approximately sufficient in certain sentence",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 430,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 432,
      "text": "classification tasks, their sensitivity to word order is restricted locally and long-term dependencies are typically ignored.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 431,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 433,
      "text": "Yin et al. [89] provided interesting insights on the comparative performance between RNNs and CNNs. After testing on",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 426,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 434,
      "text": "multiple NLP tasks that included sentiment classification, QA, and POS tagging, they concluded that there is no clear winner:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 433,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 435,
      "text": "the performance of each network depends on the global semantics required by the task itself.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 434,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 436,
      "text": "Below, we discuss some of the RNN models extensively used in the literature.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 433,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 437,
      "text": "B. RNN models",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 405,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 438,
      "text": "1) Simple RNN: In the context of NLP, RNNs are primarily based on Elman network [69] and they are originally three-",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 437,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 439,
      "text": "layer networks. Fig. 9 illustrates a more general RNN which is unfolded across time to accommodate a whole sequence. In",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 438,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 440,
      "text": "the figure, xt is taken as the input to the network at time step t and st represents the hidden state at the same time step.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 439,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 441,
      "text": "Calculation of st is based as per the equation:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 440,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 442,
      "text": "st = f (U xt + W st−1) (11)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 441,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 443,
      "text": "Thus, st is calculated based on the current input and the previous time step’s hidden state. The function f is taken to be a",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 438,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 444,
      "text": "non-linear transformation such as tanh, ReLU and U, V, W account for weights that are shared across time. In the context of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 443,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 445,
      "text": "NLP, xt typically comprises of one-hot encodings or embeddings. At times, they can also be abstract representations of textual",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 444,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 446,
      "text": "content. ot illustrates the output of the network which is also often subjected to non-linearity, especially when the network",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 445,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 447,
      "text": "contains further layers downstream.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 446,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 448,
      "text": "The hidden state of the RNN is typically considered to be its most crucial element. As stated before, it can be considered",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 443,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 449,
      "text": "as the network’s memory element that accumulates information from other time steps. In practice, however, these simple RNN",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 448,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 450,
      "text": "networks suffer from the infamous vanishing gradient problem, which makes it really hard to learn and tune the parameters",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 449,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 451,
      "text": "of the earlier layers in the network.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 450,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 452,
      "text": "This limitation was overcome by various networks such as long short-term memory (LSTM), gated recurrent units (GRUs),",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 448,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 453,
      "text": "and residual networks (ResNets), where the first two are the most used RNN variants in NLP applications.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 452,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 10,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 454,
      "text": "12",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 455,
      "text": "tanhxt ˜C oi Cht−1 htxt s zr ht(1) Long Short-Term Memory(2) Gated Recurrent Unit",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 456,
      "text": "Fig. 10: Illustration of an LSTM and GRU gate (Figure Source: Chung et al. [81])",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 455,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 457,
      "text": "2) Long Short-Term Memory: LSTM [91, 92] (Fig. 10) has additional “forget” gates over the simple RNN. Its unique",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 452,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 458,
      "text": "mechanism enables it to overcome both the vanishing and exploding gradient problem.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 457,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 459,
      "text": "Unlike the vanilla RNN, LSTM allows the error to back-propagate through unlimited number of time steps. Consisting of",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 457,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 460,
      "text": "three gates: input, forget and output gates, it calculates the hidden state by taking a combination of these three gates as per",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 459,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 461,
      "text": "the equations below:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 460,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 462,
      "text": "x = (cid:20) ht−1xt (cid:21) (12)ft = σ(Wf .x + bf ) (13)it = σ(Wi .x + bi) (14)ot = σ (Wo.x + bo) (15)ct = ft ⊙ ct−1 + it ⊙ tanh(Wc .X + bc) (16)ht = ot ⊙ tanh(ct) (17)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 461,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 463,
      "text": "3) Gated Recurrent Units: Another gated RNN variant called GRU [82] (Fig. 10) of lesser complexity was invented with",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 459,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 464,
      "text": "empirically similar performances to LSTM in most tasks. GRU comprises of two gates, reset gate and update gate, and handles",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 463,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 465,
      "text": "the flow of information like an LSTM sans a memory unit. Thus, it exposes the whole hidden content without any control.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 464,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 466,
      "text": "Being less complex, GRU can be a more efficient RNN than LSTM. The working of GRU is as follows:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 465,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 467,
      "text": "z = σ (Uz .xt + Wz .ht−1 ) (18)r = σ (Ur .xt + Wr .ht−1) (19)st = tanh(Uz .xt + Ws.(ht−1 ⊙ r)) (20)ht = (1 − z) ⊙ st + z ⊙ ht−1 (21)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 466,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 468,
      "text": "Researchers often face the dilemma of choosing the appropriate gated RNN. This also extends to developers working in",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 463,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 469,
      "text": "NLP. Throughout the history, most of the choices over the RNN variant tended to be heuristic. Chung et al. [81] did a critical",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 468,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 470,
      "text": "comparative evaluation of the three RNN variants mentioned above, although not on NLP tasks. They evaluated their work on",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 469,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 471,
      "text": "tasks relating to polyphonic music modeling and speech signal modeling. Their evaluation clearly demonstrated the superiority",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 470,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 472,
      "text": "of the gated units (LSTM and GRU) over the traditional simple RNN (in their case, using tanh activation) (Fig. 11). However,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 471,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 473,
      "text": "they could not make any concrete conclusion about which of the two gating units was better. This fact has been noted in other",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 472,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 474,
      "text": "works too and, thus, people often leverage on other factors like computing power while choosing between the two.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 473,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 475,
      "text": "C. Applications",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 437,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 476,
      "text": "1) RNN for word-level classification: RNNs have had a huge presence in the field of word-level classification. Many of",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 475,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 477,
      "text": "their applications stand as state of the art in their respective tasks. Lample et al. [93] proposed to use bidirectional LSTM",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 476,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 11,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 478,
      "text": "13",
      "label_id": 12,
      "label_name": "Page-Header",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 479,
      "text": "IMAGE",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 480,
      "text": "Fig. 11: Learning curves for training and validation sets of different types of units with respect to (top) the number of iterations",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 479,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 481,
      "text": "and (bottom) the wall clock time. y-axis corresponds to the negative log likelihood of the model shown in log-scale (Figure",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 480,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 482,
      "text": "source: Chung et al. [81])",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 481,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 483,
      "text": "for NER. The network captured arbitrarily long context information around the target word (curbing the limitation of a fixed",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 477,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 484,
      "text": "window size) resulting in two fixed-size vector, on top of which another fully-connected layer was built. They used a CRF",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 483,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 485,
      "text": "layer at last for the final entity tagging.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 484,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 486,
      "text": "RNNs have also shown considerable improvement in language modeling over traditional methods based on count statistics.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 476,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 487,
      "text": "Pioneering work in this field was done by Graves [94], who introduced the effectiveness of RNNs in modeling complex",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 486,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 488,
      "text": "sequences with long range context structures. He also proposed deep RNNs where multiple layers of hidden states were used",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 487,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 489,
      "text": "to enhance the modeling. This work established the usage of RNNs on tasks beyond the context of NLP. Later, Sundermeyer",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 488,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 490,
      "text": "et al. [95] compared the gain obtained by replacing a feed-forward neural network with an RNN when conditioning the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 489,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 491,
      "text": "prediction of a word on the words ahead. In their work, they proposed a typical hierarchy in neural network architectures",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 490,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 492,
      "text": "where feed-forward neural networks gave considerable improvement over traditional count-based language models, which in",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 491,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 493,
      "text": "turn were superseded by RNNs and later by LSTMs. An important point that they mentioned was the applicability of their",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 492,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 494,
      "text": "conclusions to a variety of other tasks such as statistical machine translation [96].",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 493,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 495,
      "text": "2) RNN for sentence-level classification: Wang et al. [25] proposed encoding entire tweets with LSTM, whose hidden",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 486,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 496,
      "text": "state is used for predicting sentiment polarity. This simple strategy proved competitive to the more complex DCNN structure",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 495,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 497,
      "text": "by Kalchbrenner et al. [49] designed to endow CNN models with ability to capture long-term dependencies. In a special case",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 496,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 498,
      "text": "studying negation phrase, the authors also showed that the dynamics of LSTM gates can capture the reversal effect of the word",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 497,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 499,
      "text": "not.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 498,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 500,
      "text": "Similar to CNN, the hidden state of an RNN can also be used for semantic matching between texts. In dialogue systems,",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 495,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 501,
      "text": "Lowe et al. [97] proposed to match a message with candidate responses with Dual-LSTM, which encodes both as fixed-size",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 500,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 502,
      "text": "vectors and then measure their inner product as the basis to rank candidate responses.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 501,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 503,
      "text": "3) RNN for generating language: A challenging task in NLP is generating natural language, which is another natural",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 500,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 504,
      "text": "application of RNNs. Conditioned on textual or visual data, deep LSTMs have been shown to generate reasonable task-specific",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 503,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 505,
      "text": "text in tasks such as machine translation, image captioning, etc. In such cases, the RNN is termed a decoder.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 504,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 506,
      "text": "In [74], the authors proposed a general deep LSTM encoder-decoder framework that maps a sequence to another sequence.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 503,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 507,
      "text": "One LSTM is used to encode the “source” sequence as a fixed-size vector, which can be text in the original language (machine",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 506,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 508,
      "text": "translation), the question to be answered (QA) or the message to be replied to (dialogue systems). The vector is used as the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 507,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 509,
      "text": "initial state of another LSTM, named the decoder. During inference, the decoder generates tokens one by one, while updating",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 508,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 510,
      "text": "its hidden state with the last generated token. Beam search is often used to approximate the optimal sequence.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 509,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 511,
      "text": "Sutskever et al. [74] experimented with 4-layer LSTM on a machine translation task in an end-to-end fashion, showing",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 506,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 12,
      "box": [
        0,
        0,
        0,
        0
      ]
    }
  ]
}