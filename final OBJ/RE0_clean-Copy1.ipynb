{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45bbd454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ====== label space（沿用你之前那套 14 类；后续你想换可以改）======\n",
    "ID2LABEL_14 = [\n",
    "    \"Title\", \"Author\", \"Mail\", \"Affiliation\", \"Section\",\n",
    "    \"First-Line\", \"Para-Line\", \"Equation\", \"Table\", \"Figure\",\n",
    "    \"Caption\", \"Page-Footer\", \"Page-Header\", \"Footnote\"\n",
    "]\n",
    "LABEL2ID_14 = {k:i for i,k in enumerate(ID2LABEL_14)}\n",
    "\n",
    "REL2ID = {\"connect\": 0, \"contain\": 1, \"equality\": 2, \"meta\": 3}\n",
    "ID2REL = {v:k for k,v in REL2ID.items()}\n",
    "\n",
    "\n",
    "def map_hrd_class_to_14(c: str) -> int:\n",
    "    c = (c or \"\").lower().strip()\n",
    "    if c == \"title\":\n",
    "        return LABEL2ID_14[\"Title\"]\n",
    "    if c == \"author\":\n",
    "        return LABEL2ID_14[\"Author\"]\n",
    "    if c in (\"affili\", \"affiliation\"):\n",
    "        return LABEL2ID_14[\"Affiliation\"]\n",
    "    if c in (\"header\",):\n",
    "        return LABEL2ID_14[\"Page-Header\"]\n",
    "    if c in (\"footer\",):\n",
    "        return LABEL2ID_14[\"Page-Footer\"]\n",
    "    if c in (\"fnote\",):\n",
    "        return LABEL2ID_14[\"Footnote\"]\n",
    "    if c.startswith(\"sec\"):\n",
    "        return LABEL2ID_14[\"Section\"]\n",
    "    if c in (\"fstline\",):\n",
    "        return LABEL2ID_14[\"First-Line\"]\n",
    "    if c in (\"para\", \"opara\"):\n",
    "        return LABEL2ID_14[\"Para-Line\"]\n",
    "    # 兜底\n",
    "    return LABEL2ID_14[\"Para-Line\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_image_path(image_root, doc_id, page_id):\n",
    "    \"\"\"\n",
    "    Try multiple naming conventions:\n",
    "    1) HRDH-style: <image_root>/<doc_id>/<page_id>.png\n",
    "    2) HRDH-style with other ext: <image_root>/<doc_id>/<page_id>.(jpg/jpeg/png)\n",
    "    3) HRDS-style: <image_root>/<doc_id>/<doc_id>_<page_id>.(jpg/jpeg/png)\n",
    "    4) HRDS-style flat (just in image_root): <image_root>/<doc_id>_<page_id>.(jpg/jpeg/png)\n",
    "\n",
    "    Returns existing path; otherwise raises FileNotFoundError with helpful info.\n",
    "    \"\"\"\n",
    "    exts = (\"png\", \"jpg\", \"jpeg\", \"webp\")\n",
    "\n",
    "    # --- A) in subfolder <doc_id>/ ---\n",
    "    doc_dir = os.path.join(image_root, doc_id)\n",
    "\n",
    "    # 1) <page_id>.<ext>\n",
    "    for ext in exts:\n",
    "        p = os.path.join(doc_dir, f\"{page_id}.{ext}\")\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "\n",
    "    # 2) <doc_id>_<page_id>.<ext>\n",
    "    for ext in exts:\n",
    "        p = os.path.join(doc_dir, f\"{doc_id}_{page_id}.{ext}\")\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "\n",
    "    # --- B) flat in image_root ---\n",
    "    for ext in exts:\n",
    "        p = os.path.join(image_root, f\"{doc_id}_{page_id}.{ext}\")\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "\n",
    "    # --- C) glob fallback (covers weird ext/case) ---\n",
    "    # in doc folder\n",
    "    pats = [\n",
    "        os.path.join(doc_dir, f\"{page_id}.*\"),\n",
    "        os.path.join(doc_dir, f\"{doc_id}_{page_id}.*\"),\n",
    "        os.path.join(image_root, f\"{doc_id}_{page_id}.*\"),\n",
    "    ]\n",
    "    for pat in pats:\n",
    "        hits = glob.glob(pat)\n",
    "        if hits:\n",
    "            # pick the first deterministically (sorted)\n",
    "            hits = sorted(hits)\n",
    "            return hits[0]\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing page image for doc_id={doc_id}, page_id={page_id}. \"\n",
    "        f\"Tried under: {doc_dir} and {image_root}. \"\n",
    "        f\"Example expected: {os.path.join(doc_dir, str(page_id)+'.png')} or {os.path.join(doc_dir, f'{doc_id}_{page_id}.jpg')}\"\n",
    "    )\n",
    "\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "\n",
    "def topo_sort_with_reading_priority(items):\n",
    "    \"\"\"\n",
    "    items: HRDH json list，每个元素至少包含 box/page/parent_id\n",
    "    返回：满足 parent 一定在 child 前的顺序（同时尽量贴近阅读序）\n",
    "    \"\"\"\n",
    "    n = len(items)\n",
    "\n",
    "    # reading priority rank\n",
    "    ranks = []\n",
    "    for i, it in enumerate(items):\n",
    "        x0, y0, x1, y1 = it[\"box\"]\n",
    "        ranks.append((int(it[\"page\"]), float(y0), float(x0), i))\n",
    "\n",
    "    # parent -> child graph\n",
    "    indeg = [0] * n\n",
    "    g = defaultdict(list)\n",
    "    for child in range(n):\n",
    "        p = int(items[child].get(\"parent_id\", -1))\n",
    "        if p < 0:\n",
    "            continue\n",
    "        if 0 <= p < n:\n",
    "            g[p].append(child)\n",
    "            indeg[child] += 1\n",
    "\n",
    "    # Kahn with heap prioritized by reading rank\n",
    "    heap = []\n",
    "    for i in range(n):\n",
    "        if indeg[i] == 0:\n",
    "            heapq.heappush(heap, (ranks[i], i))\n",
    "\n",
    "    order = []\n",
    "    while heap:\n",
    "        _, u = heapq.heappop(heap)\n",
    "        order.append(u)\n",
    "        for v in g[u]:\n",
    "            indeg[v] -= 1\n",
    "            if indeg[v] == 0:\n",
    "                heapq.heappush(heap, (ranks[v], v))\n",
    "\n",
    "    # fallback if cycles/noise exist\n",
    "    if len(order) < n:\n",
    "        remaining = [i for i in range(n) if i not in set(order)]\n",
    "        remaining.sort(key=lambda i: ranks[i])\n",
    "        order.extend(remaining)\n",
    "\n",
    "    return order\n",
    "\n",
    "\n",
    "def load_hrdh_json(json_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    读取单个 HRDH json，返回 doc dict（units + labels + parent + relation），并重映射 parent 到新排序 index。\n",
    "    当前排序：按 (page, y0, x0) 近似阅读顺序（足够用于训练闭环；后续可替换为双栏阅读序）。\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        items = json.load(f)\n",
    "\n",
    "    def sort_key(idx_item):\n",
    "        idx, it = idx_item\n",
    "        x0, y0, x1, y1 = it[\"box\"]\n",
    "        return (int(it[\"page\"]), int(y0), int(x0), idx)\n",
    "\n",
    "    order_old = topo_sort_with_reading_priority(items)\n",
    "    indexed_sorted = [(i, items[i]) for i in order_old]\n",
    "\n",
    "\n",
    "    old2new = {old_i: new_i for new_i, (old_i, _) in enumerate(indexed_sorted)}\n",
    "\n",
    "    units = []\n",
    "    y_parent, y_rel, y_cls, is_meta = [], [], [], []\n",
    "\n",
    "    for new_i, (old_i, it) in enumerate(indexed_sorted):\n",
    "        text = (it.get(\"text\") or \"\").strip()\n",
    "        x0, y0, x1, y1 = it[\"box\"]\n",
    "        page_id = int(it[\"page\"])\n",
    "        cls_raw = it.get(\"class\", \"para\")\n",
    "        rel_raw = it.get(\"relation\", \"connect\")\n",
    "        meta_flag = bool(it.get(\"is_meta\", False))\n",
    "        parent_old = int(it.get(\"parent_id\", -1))\n",
    "        parent_new = -1 if parent_old == -1 else old2new.get(parent_old, -1)\n",
    "\n",
    "        units.append({\n",
    "            \"text\": text,\n",
    "            \"bbox\": (float(x0), float(y0), float(x1), float(y1)),  # pixel bbox\n",
    "            \"page_id\": page_id,\n",
    "            \"order_id\": new_i,\n",
    "            \"class_raw\": cls_raw,\n",
    "        })\n",
    "        y_parent.append(parent_new)\n",
    "        y_rel.append(REL2ID.get(rel_raw, 0))\n",
    "        y_cls.append(map_hrd_class_to_14(cls_raw))\n",
    "        is_meta.append(meta_flag)\n",
    "\n",
    "    doc_id = os.path.basename(json_path).replace(\".json\", \"\")\n",
    "    return {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"json_path\": json_path,\n",
    "        \"units\": units,\n",
    "        \"y_parent\": y_parent,\n",
    "        \"y_rel\": y_rel,\n",
    "        \"y_cls\": y_cls,\n",
    "        \"is_meta\": is_meta,\n",
    "    }\n",
    "\n",
    "\n",
    "class HRDHDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, split: str = \"train\", max_len: int = 512):\n",
    "        \"\"\"\n",
    "        root_dir: .../HRDH\n",
    "        split: train 或 test\n",
    "        max_len: 截断长度（论文常用 512）\n",
    "        \"\"\"\n",
    "        assert split in (\"train\", \"test\")\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.json_dir = os.path.join(root_dir, split)\n",
    "        self.image_root = os.path.join(root_dir, \"images\")  \n",
    "\n",
    "        self.json_paths = sorted(glob.glob(os.path.join(self.json_dir, \"*.json\")))\n",
    "        if not self.json_paths:\n",
    "            raise FileNotFoundError(f\"No json files found in: {self.json_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.json_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        json_path = self.json_paths[idx]\n",
    "        doc = load_hrdh_json(json_path)\n",
    "\n",
    "        # 截断（保持 parent 合法）\n",
    "        if len(doc[\"units\"]) > self.max_len:\n",
    "            keep = self.max_len\n",
    "            doc[\"units\"] = doc[\"units\"][:keep]\n",
    "            doc[\"y_cls\"] = doc[\"y_cls\"][:keep]\n",
    "            doc[\"y_rel\"] = doc[\"y_rel\"][:keep]\n",
    "            doc[\"is_meta\"] = doc[\"is_meta\"][:keep]\n",
    "            # parent: 超出范围的 parent 置为 0；指向被截断的也置 0（或 -1）\n",
    "            y_parent = []\n",
    "            for i, p in enumerate(doc[\"y_parent\"][:keep]):\n",
    "                if p == -1:\n",
    "                    y_parent.append(-1)\n",
    "                elif 0 <= p < keep:\n",
    "                    y_parent.append(p)\n",
    "                else:\n",
    "                    y_parent.append(-1)\n",
    "            doc[\"y_parent\"] = y_parent\n",
    "\n",
    "        # 为每个 unit 找到对应页图路径（按 page_id）\n",
    "        # 注意：同一页只加载一次图片，训练时可以再做缓存/预处理\n",
    "        page_ids = sorted(set(u[\"page_id\"] for u in doc[\"units\"]))\n",
    "        page_images = {}\n",
    "        for pid in page_ids:\n",
    "            page_images[pid] = get_image_path(self.image_root, doc[\"doc_id\"], pid)\n",
    "        doc[\"page_images\"] = page_images\n",
    "        for i, p in enumerate(doc[\"y_parent\"]):\n",
    "            if p >= 0:\n",
    "                assert p < i, f\"parent not causal: i={i}, p={p}\"\n",
    "            if \"is_meta\" in doc:\n",
    "                for i, flag in enumerate(doc[\"is_meta\"]):\n",
    "                    if flag:\n",
    "                        doc[\"y_parent\"][i] = -1  # ROOT\n",
    "        \n",
    "        return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c91fa554",
   "metadata": {},
   "outputs": [],
   "source": [
    "REL3 = {\"connect\": 0, \"contain\": 1, \"equality\": 2}  # 论文三类\n",
    "REL3_INV = {v:k for k,v in REL3.items()}\n",
    "\n",
    "def filter_meta_and_remap(units_raw):\n",
    "    \"\"\"\n",
    "    units_raw: list of dict from json, each has:\n",
    "      text, box, class, page, is_meta, parent_id, relation\n",
    "    return:\n",
    "      units_kept: list of dict with keys {text, box, page_id, cls_name, parent, rel}\n",
    "      old2new: dict old_index -> new_index  (only for kept)\n",
    "    \"\"\"\n",
    "    keep_idx = [i for i,u in enumerate(units_raw) if not u.get(\"is_meta\", False)]\n",
    "    old2new = {old:new for new,old in enumerate(keep_idx)}\n",
    "\n",
    "    units_kept = []\n",
    "    for old_i in keep_idx:\n",
    "        u = units_raw[old_i]\n",
    "        p_old = int(u.get(\"parent_id\", -1))\n",
    "        # 如果 parent 被过滤掉，或本来就是 -1，则设为 ROOT(-1)\n",
    "        p_new = old2new.get(p_old, -1) if p_old >= 0 else -1\n",
    "\n",
    "        rel = u.get(\"relation\", None)\n",
    "        # 过滤后不应该再出现 meta；如果仍然出现，直接跳过该样本或置默认\n",
    "        if rel == \"meta\":\n",
    "            # 这里选择：直接将该单元丢弃（更干净）\n",
    "            continue\n",
    "\n",
    "        if rel not in REL3:\n",
    "            raise ValueError(f\"Unknown relation: {rel}\")\n",
    "\n",
    "        units_kept.append({\n",
    "            \"text\": u.get(\"text\",\"\"),\n",
    "            \"box\": u.get(\"box\"),\n",
    "            \"page_id\": int(u.get(\"page\", 0)),\n",
    "            \"cls_name\": u.get(\"class\"),\n",
    "            \"parent\": p_new,\n",
    "            \"rel\": REL3[rel],\n",
    "        })\n",
    "\n",
    "    return units_kept, old2new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea27de2",
   "metadata": {},
   "source": [
    "## 1. 环境与设备检查\n",
    "\n",
    "- 检查 PyTorch / CUDA 可用性\n",
    "- 设置 device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d2adcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1\n",
      "cuda available: True\n",
      "cuda device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "cuda capability: (8, 9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"cuda capability:\", torch.cuda.get_device_capability(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060880a",
   "metadata": {},
   "source": [
    "## 2. 数据集路径与快速 sanity-check\n",
    "\n",
    "- 指定 HRDH_ROOT\n",
    "- 初始化数据集与样本查看\n",
    "\n",
    "说明：原 Notebook 中此部分出现了两段几乎等价的 quick-check（为保持对齐，这里全部保留）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4a5516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id: 1401.6399\n",
      "num_units: 512\n",
      "page_images sample: [(0, 'C:\\\\Users\\\\tomra\\\\Desktop\\\\PAPER\\\\final OBJ\\\\HRDH\\\\images\\\\1401.6399\\\\0.png'), (1, 'C:\\\\Users\\\\tomra\\\\Desktop\\\\PAPER\\\\final OBJ\\\\HRDH\\\\images\\\\1401.6399\\\\1.png'), (2, 'C:\\\\Users\\\\tomra\\\\Desktop\\\\PAPER\\\\final OBJ\\\\HRDH\\\\images\\\\1401.6399\\\\2.png')]\n"
     ]
    }
   ],
   "source": [
    "HRDH_ROOT = r\"C:\\Users\\tomra\\Desktop\\PAPER\\final OBJ\\HRDH\"\n",
    "ds = HRDHDataset(HRDH_ROOT, split=\"train\", max_len=512)\n",
    "\n",
    "doc = ds[0]\n",
    "print(\"doc_id:\", doc[\"doc_id\"])\n",
    "print(\"num_units:\", len(doc[\"units\"]))\n",
    "print(\"page_images sample:\", list(doc[\"page_images\"].items())[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "089da0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id: 1401.6399\n",
      "num_units: 512\n",
      "pages: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "missing_images: []\n",
      "bad_parent_count: 0 example: []\n",
      "0 p 0 Title par -1 | SIMD Compression and the Intersection\n",
      "1 p 0 Title par -1 | of Sorted Integers\n",
      "2 p 0 Author par -1 | D. Lemire1 *, L. Boytsov2, N. Kurz3\n",
      "3 p 0 Affiliation par -1 | 1LICEF Research Center, TELUQ, Montreal, QC, Canada\n",
      "4 p 0 Affiliation par -1 | 2 Carnegie Mellon University, Pittsburgh, PA USA\n",
      "5 p 0 Affiliation par -1 | 3 Verse Communications, Orinda, CA USA\n",
      "6 p 0 First-Line par -1 | KEY WORDS: performance; measurement; index compression; vector processing\n",
      "7 p 0 Section par -1 | 1. INTRODUCTION\n",
      "8 p 0 First-Line par 7 | An inverted index maps terms to lists of document identifiers. A column index in\n",
      "9 p 0 Para-Line par 8 | might, similarly, map attribute values to row identifiers. Storing all these lis\n"
     ]
    }
   ],
   "source": [
    "HRDH_ROOT = r\"C:\\Users\\tomra\\Desktop\\PAPER\\final OBJ\\HRDH\"\n",
    "\n",
    "ds = HRDHDataset(HRDH_ROOT, split=\"train\", max_len=512)\n",
    "sample = ds[0]\n",
    "\n",
    "print(\"doc_id:\", sample[\"doc_id\"])\n",
    "print(\"num_units:\", len(sample[\"units\"]))\n",
    "print(\"pages:\", sorted(sample[\"page_images\"].keys()))\n",
    "print(\"missing_images:\", [p for p,v in sample[\"page_images\"].items() if v is None][:10])\n",
    "\n",
    "# parent 合法性检查\n",
    "L = len(sample[\"units\"])\n",
    "bad = [(i,p) for i,p in enumerate(sample[\"y_parent\"]) if not (p==-1 or 0<=p<L)]\n",
    "print(\"bad_parent_count:\", len(bad), \"example:\", bad[:5])\n",
    "\n",
    "# 简单预览\n",
    "for i in range(min(10, L)):\n",
    "    u = sample[\"units\"][i]\n",
    "    print(i, \"p\", u[\"page_id\"], ID2LABEL_14[sample[\"y_cls\"][i]], \"par\", sample[\"y_parent\"][i], \"|\", u[\"text\"][:80])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca6bce",
   "metadata": {},
   "source": [
    "## 3. 先验统计：从数据集中估计 $M_{cp}$（Class Prior / Conditional Prior）\n",
    "\n",
    "说明：原 Notebook 内存在两个版本的 `compute_M_cp_from_dataset`（一个较简版、一个带 `defaultdict`）。为保证“一个逻辑都不许少”，两段均保留，按原顺序排列。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e8729e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_cp shape: (15, 14) colsum (should be 1): [1. 1. 1. 1. 1.]\n",
      "saved: M_cp_hrdh.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_M_cp_from_dataset(dataset: HRDHDataset, num_classes: int, pseudo_count: float = 5.0):\n",
    "    \"\"\"\n",
    "    返回 M_cp: shape (num_classes+1, num_classes)\n",
    "    行：parent_class + ROOT\n",
    "    列：child_class\n",
    "    \"\"\"\n",
    "    ROOT = num_classes  # extra row index\n",
    "    counts = np.zeros((num_classes + 1, num_classes), dtype=np.float64)\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        doc = dataset[i]\n",
    "        y_cls = doc[\"y_cls\"]\n",
    "        y_parent = doc[\"y_parent\"]\n",
    "        L = len(y_cls)\n",
    "\n",
    "        for child in range(L):\n",
    "            c = y_cls[child]\n",
    "            p = y_parent[child]\n",
    "            if p == -1:\n",
    "                pc = ROOT\n",
    "            else:\n",
    "                pc = y_cls[p]\n",
    "            counts[pc, c] += 1.0\n",
    "\n",
    "    # additive smoothing per column\n",
    "    counts += pseudo_count\n",
    "    # normalize columns -> probability\n",
    "    col_sum = counts.sum(axis=0, keepdims=True)\n",
    "    M_cp = counts / np.clip(col_sum, 1e-12, None)\n",
    "    return M_cp\n",
    "\n",
    "train_ds = HRDHDataset(HRDH_ROOT, split=\"train\", max_len=512)\n",
    "M_cp = compute_M_cp_from_dataset(train_ds, num_classes=len(ID2LABEL_14), pseudo_count=5.0)\n",
    "print(\"M_cp shape:\", M_cp.shape, \"colsum (should be 1):\", M_cp.sum(axis=0)[:5])\n",
    "\n",
    "np.save(\"M_cp_hrdh.npy\", M_cp)\n",
    "print(\"saved: M_cp_hrdh.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae03824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_cp shape: (15, 14)\n",
      "col sums (first 8): [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Top 25 (parent, child) by COUNT (after smoothing included):\n",
      "00  Para-Line    -> Para-Line     count=288776.0\n",
      "01  First-Line   -> Para-Line     count=57570.0\n",
      "02  First-Line   -> First-Line    count=54224.0\n",
      "03  ROOT         -> Para-Line     count=14250.0\n",
      "04  Section      -> First-Line    count=10697.0\n",
      "05  Section      -> Section       count=10588.0\n",
      "06  ROOT         -> Page-Header   count=6025.0\n",
      "07  ROOT         -> Affiliation   count=3185.0\n",
      "08  ROOT         -> Footnote      count=2470.0\n",
      "09  ROOT         -> Title         count=1762.0\n",
      "10  ROOT         -> Author        count=1641.0\n",
      "11  ROOT         -> Section       count=1005.0\n",
      "12  ROOT         -> First-Line    count=566.0\n",
      "13  Section      -> Para-Line     count=455.0\n",
      "14  Title        -> Title         count=5.0\n",
      "15  Title        -> Author        count=5.0\n",
      "16  Title        -> Mail          count=5.0\n",
      "17  Title        -> Affiliation   count=5.0\n",
      "18  Title        -> Section       count=5.0\n",
      "19  Title        -> First-Line    count=5.0\n",
      "20  Title        -> Para-Line     count=5.0\n",
      "21  Title        -> Equation      count=5.0\n",
      "22  Title        -> Table         count=5.0\n",
      "23  Title        -> Figure        count=5.0\n",
      "24  Title        -> Caption       count=5.0\n",
      "\n",
      "[Child] 0 Title\n",
      "  parent=ROOT          P=0.9618\n",
      "  parent=Title         P=0.0027\n",
      "  parent=Mail          P=0.0027\n",
      "  parent=Author        P=0.0027\n",
      "  parent=Section       P=0.0027\n",
      "  parent=First-Line    P=0.0027\n",
      "\n",
      "[Child] 1 Author\n",
      "  parent=ROOT          P=0.9591\n",
      "  parent=Title         P=0.0029\n",
      "  parent=Mail          P=0.0029\n",
      "  parent=Author        P=0.0029\n",
      "  parent=Section       P=0.0029\n",
      "  parent=First-Line    P=0.0029\n",
      "\n",
      "[Child] 2 Mail\n",
      "  parent=Title         P=0.0667\n",
      "  parent=Author        P=0.0667\n",
      "  parent=Mail          P=0.0667\n",
      "  parent=Affiliation   P=0.0667\n",
      "  parent=Section       P=0.0667\n",
      "  parent=First-Line    P=0.0667\n",
      "\n",
      "[Child] 3 Affiliation\n",
      "  parent=ROOT          P=0.9785\n",
      "  parent=Title         P=0.0015\n",
      "  parent=Mail          P=0.0015\n",
      "  parent=Author        P=0.0015\n",
      "  parent=Section       P=0.0015\n",
      "  parent=First-Line    P=0.0015\n",
      "\n",
      "[Child] 4 Section\n",
      "  parent=Section       P=0.9082\n",
      "  parent=ROOT          P=0.0862\n",
      "  parent=Mail          P=0.0004\n",
      "  parent=Title         P=0.0004\n",
      "  parent=Affiliation   P=0.0004\n",
      "  parent=First-Line    P=0.0004\n",
      "\n",
      "[Child] 5 First-Line\n",
      "  parent=First-Line    P=0.8273\n",
      "  parent=Section       P=0.1632\n",
      "  parent=ROOT          P=0.0086\n",
      "  parent=Title         P=0.0001\n",
      "  parent=Affiliation   P=0.0001\n",
      "  parent=Mail          P=0.0001\n",
      "\n",
      "[Child] 6 Para-Line\n",
      "  parent=Para-Line     P=0.7997\n",
      "  parent=First-Line    P=0.1594\n",
      "  parent=ROOT          P=0.0395\n",
      "  parent=Section       P=0.0013\n",
      "  parent=Affiliation   P=0.0000\n",
      "  parent=Mail          P=0.0000\n",
      "\n",
      "[Child] 7 Equation\n",
      "  parent=Title         P=0.0667\n",
      "  parent=Author        P=0.0667\n",
      "  parent=Mail          P=0.0667\n",
      "  parent=Affiliation   P=0.0667\n",
      "  parent=Section       P=0.0667\n",
      "  parent=First-Line    P=0.0667\n",
      "\n",
      "[Child] 8 Table\n",
      "  parent=Title         P=0.0667\n",
      "  parent=Author        P=0.0667\n",
      "  parent=Mail          P=0.0667\n",
      "  parent=Affiliation   P=0.0667\n",
      "  parent=Section       P=0.0667\n",
      "  parent=First-Line    P=0.0667\n",
      "\n",
      "[Child] 9 Figure\n",
      "  parent=Title         P=0.0667\n",
      "  parent=Author        P=0.0667\n",
      "  parent=Mail          P=0.0667\n",
      "  parent=Affiliation   P=0.0667\n",
      "  parent=Section       P=0.0667\n",
      "  parent=First-Line    P=0.0667\n",
      "\n",
      "[Child] 10 Caption\n",
      "  parent=Title         P=0.0667\n",
      "  parent=Author        P=0.0667\n",
      "  parent=Mail          P=0.0667\n",
      "  parent=Affiliation   P=0.0667\n",
      "  parent=Section       P=0.0667\n",
      "  parent=First-Line    P=0.0667\n",
      "\n",
      "[Child] 11 Page-Footer\n",
      "  parent=Title         P=0.0667\n",
      "  parent=Author        P=0.0667\n",
      "  parent=Mail          P=0.0667\n",
      "  parent=Affiliation   P=0.0667\n",
      "  parent=Section       P=0.0667\n",
      "  parent=First-Line    P=0.0667\n",
      "\n",
      "[Child] 12 Page-Header\n",
      "  parent=ROOT          P=0.9885\n",
      "  parent=Title         P=0.0008\n",
      "  parent=Mail          P=0.0008\n",
      "  parent=Author        P=0.0008\n",
      "  parent=Section       P=0.0008\n",
      "  parent=First-Line    P=0.0008\n",
      "\n",
      "[Child] 13 Footnote\n",
      "  parent=ROOT          P=0.9724\n",
      "  parent=Title         P=0.0020\n",
      "  parent=Mail          P=0.0020\n",
      "  parent=Author        P=0.0020\n",
      "  parent=Section       P=0.0020\n",
      "  parent=First-Line    P=0.0020\n",
      "\n",
      "Saved: M_cp_hrdh.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_M_cp_from_dataset(dataset, num_classes: int, pseudo_count: float = 5.0):\n",
    "    \"\"\"\n",
    "    M_cp shape: (num_classes+1, num_classes)\n",
    "      rows: parent_class + ROOT(row = num_classes)\n",
    "      cols: child_class\n",
    "    Column-normalized: sum over rows for each col = 1\n",
    "    \"\"\"\n",
    "    ROOT = num_classes\n",
    "    counts = np.zeros((num_classes + 1, num_classes), dtype=np.float64)\n",
    "\n",
    "    for di in range(len(dataset)):\n",
    "        doc = dataset[di]\n",
    "        y_cls = doc[\"y_cls\"]\n",
    "        y_parent = doc[\"y_parent\"]\n",
    "        L = len(y_cls)\n",
    "\n",
    "        for child in range(L):\n",
    "            c = int(y_cls[child])\n",
    "            p = int(y_parent[child])\n",
    "            if p == -1:\n",
    "                pc = ROOT\n",
    "            else:\n",
    "                pc = int(y_cls[p])\n",
    "            counts[pc, c] += 1.0\n",
    "\n",
    "    # Additive smoothing\n",
    "    counts += float(pseudo_count)\n",
    "\n",
    "    # Column normalize\n",
    "    col_sum = counts.sum(axis=0, keepdims=True)\n",
    "    M_cp = counts / np.clip(col_sum, 1e-12, None)\n",
    "    return M_cp, counts\n",
    "\n",
    "def top_parent_for_each_child(M_cp: np.ndarray, id2label: list, topk: int = 6):\n",
    "    \"\"\"\n",
    "    For each child class, list top-k parent classes (including ROOT).\n",
    "    \"\"\"\n",
    "    num_classes = len(id2label)\n",
    "    ROOT = num_classes\n",
    "\n",
    "    rows, cols = M_cp.shape\n",
    "    assert rows == num_classes + 1 and cols == num_classes\n",
    "\n",
    "    for child in range(num_classes):\n",
    "        probs = M_cp[:, child]\n",
    "        idxs = np.argsort(-probs)[:topk]\n",
    "        child_name = id2label[child]\n",
    "        print(f\"\\n[Child] {child} {child_name}\")\n",
    "        for r in idxs:\n",
    "            parent_name = \"ROOT\" if r == ROOT else id2label[r]\n",
    "            print(f\"  parent={parent_name:<12s}  P={probs[r]:.4f}\")\n",
    "\n",
    "def top_edges_global(counts: np.ndarray, id2label: list, topn: int = 30):\n",
    "    \"\"\"\n",
    "    Print global most frequent (parent, child) pairs from raw counts (before normalization).\n",
    "    \"\"\"\n",
    "    num_classes = len(id2label)\n",
    "    ROOT = num_classes\n",
    "\n",
    "    flat = []\n",
    "    for pc in range(num_classes + 1):\n",
    "        for cc in range(num_classes):\n",
    "            flat.append((counts[pc, cc], pc, cc))\n",
    "    flat.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    print(f\"\\nTop {topn} (parent, child) by COUNT (after smoothing included):\")\n",
    "    for k in range(topn):\n",
    "        cnt, pc, cc = flat[k]\n",
    "        parent_name = \"ROOT\" if pc == ROOT else id2label[pc]\n",
    "        child_name = id2label[cc]\n",
    "        print(f\"{k:02d}  {parent_name:<12s} -> {child_name:<12s}  count={cnt:.1f}\")\n",
    "\n",
    "# ==== run ====\n",
    "HRDH_ROOT = r\"C:\\Users\\tomra\\Desktop\\PAPER\\final OBJ\\HRDH\"\n",
    "train_ds = HRDHDataset(HRDH_ROOT, split=\"train\", max_len=512)\n",
    "\n",
    "M_cp, counts = compute_M_cp_from_dataset(train_ds, num_classes=len(ID2LABEL_14), pseudo_count=5.0)\n",
    "\n",
    "print(\"M_cp shape:\", M_cp.shape)\n",
    "print(\"col sums (first 8):\", np.round(M_cp.sum(axis=0)[:8], 6))\n",
    "\n",
    "# sanity: should be ~1.0\n",
    "assert np.allclose(M_cp.sum(axis=0), 1.0, atol=1e-6), \"Column sums not 1.0\"\n",
    "\n",
    "top_edges_global(counts, ID2LABEL_14, topn=25)\n",
    "top_parent_for_each_child(M_cp, ID2LABEL_14, topk=6)\n",
    "\n",
    "np.save(\"M_cp_hrdh.npy\", M_cp)\n",
    "print(\"\\nSaved: M_cp_hrdh.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967ce62e",
   "metadata": {},
   "source": [
    "## 4. 训练复现性与全局配置\n",
    "\n",
    "- 随机种子\n",
    "- CFG 超参集中管理\n",
    "- bbox 归一化等通用函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1558130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c858ee09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edc7f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CFG:\n",
    "    # data\n",
    "    max_len: int = 512\n",
    "    batch_size: int = 1          # 强烈建议 doc-level batch=1（结构任务依赖整篇文档序列）\n",
    "    num_workers: int = 2\n",
    "\n",
    "    # model dims\n",
    "    d_model: int = 256\n",
    "    nhead: int = 8\n",
    "    num_layers: int = 4\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # embedding vocab sizes\n",
    "    max_1d_pos: int = 512\n",
    "    max_pages: int = 32          # 超过则截断或 clamp\n",
    "    layout_bins: int = 1001      # bbox 归一化到 [0,1000]\n",
    "    \n",
    "    # visual\n",
    "    vis_crop_size: int = 224\n",
    "    vis_out_dim: int = 256       # 视觉向量投影到 d_model\n",
    "    \n",
    "    # losses\n",
    "    alpha_parent: float = 1.0\n",
    "    alpha_rel: float = 1.0\n",
    "    focal_gamma: float = 2.0\n",
    "    focal_alpha: float = 0.25\n",
    "\n",
    "    # optim\n",
    "    lr: float = 2e-4\n",
    "    weight_decay: float = 1e-2\n",
    "    epochs: int = 10\n",
    "    \n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39e2c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_box_xyxy(box, page_w, page_h, bins=1000):\n",
    "    \"\"\"\n",
    "    输入：像素坐标 [x0,y0,x1,y1]\n",
    "    输出：整数归一化到 [0,bins]\n",
    "    \"\"\"\n",
    "    x0, y0, x1, y1 = box\n",
    "    x0 = int(np.clip(round(x0 / max(page_w, 1) * bins), 0, bins))\n",
    "    x1 = int(np.clip(round(x1 / max(page_w, 1) * bins), 0, bins))\n",
    "    y0 = int(np.clip(round(y0 / max(page_h, 1) * bins), 0, bins))\n",
    "    y1 = int(np.clip(round(y1 / max(page_h, 1) * bins), 0, bins))\n",
    "    return [x0, y0, x1, y1]\n",
    "\n",
    "\n",
    "def collate_doc(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    assert len(batch) == 1, \"当前实现是 doc-level batch=1\"\n",
    "    return batch[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b68879",
   "metadata": {},
   "source": [
    "## 5. 特征编码模块\n",
    "\n",
    "- 文本：SBERT/Transformer 句向量 + 投影\n",
    "- 布局：位置/页面 embedding\n",
    "- 视觉：图像 crop embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59ce0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBERTTextEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    用 sentence-transformers 得到每个 unit 的句向量，然后投影到 d_model。\n",
    "    - 支持简单的 dict 缓存：key=(doc_id, unit_idx)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        self._sbert = None\n",
    "        self.proj = nn.Linear(384, d_model)  # all-MiniLM-L6-v2 输出 384\n",
    "        self.cache: Dict[Tuple[str, int], torch.Tensor] = {}\n",
    "\n",
    "    def _lazy_load(self):\n",
    "        if self._sbert is None:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self._sbert = SentenceTransformer(self.model_name)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_texts(self, texts: List[str]) -> torch.Tensor:\n",
    "        self._lazy_load()\n",
    "        emb = self._sbert.encode(texts, convert_to_tensor=True, show_progress_bar=False)  # (L,384)\n",
    "        return emb\n",
    "\n",
    "    def forward(self, doc_id: str, texts: List[str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        返回 (L, d_model)，保证在 self.proj.weight.device 上\n",
    "        cache 存 CPU，取用时搬到目标 device\n",
    "        \"\"\"\n",
    "        device = self.proj.weight.device\n",
    "        L = len(texts)\n",
    "        out = [None] * L\n",
    "        missing_idx, missing_text = [], []\n",
    "\n",
    "        # 1) cache hit：取 CPU -> to(device)\n",
    "        for i, t in enumerate(texts):\n",
    "            key = (doc_id, i)\n",
    "            if key in self.cache:\n",
    "                out[i] = self.cache[key].to(device)\n",
    "            else:\n",
    "                missing_idx.append(i)\n",
    "                missing_text.append(t)\n",
    "\n",
    "        # 2) cache miss：SBERT encode -> detach/clone -> proj -> 存 CPU\n",
    "        if len(missing_idx) > 0:\n",
    "            emb_384 = self.encode_texts(missing_text)          # 可能是 inference tensor\n",
    "            emb_384 = emb_384.detach().clone().to(device)      # 关键：变普通 tensor + 上 GPU\n",
    "            emb_d = self.proj(emb_384)                         # (M,d_model) on GPU\n",
    "\n",
    "            for k, i in enumerate(missing_idx):\n",
    "                key = (doc_id, i)\n",
    "                self.cache[key] = emb_d[k].detach().cpu()      # cache 存 CPU，省显存\n",
    "                out[i] = emb_d[k]                              # 当前 batch 直接用 GPU tensor\n",
    "\n",
    "        return torch.stack(out, dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5a85339",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayoutPosPageEmbedder(nn.Module):\n",
    "    def __init__(self, d_model: int, layout_bins: int = 1001, max_pos: int = 512, max_pages: int = 32):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layout_bins = layout_bins\n",
    "\n",
    "        # LayoutLMv2 风格：x0,x1,w 与 y0,y1,h，各自 embedding 再 concat -> proj\n",
    "        self.emb_x0 = nn.Embedding(layout_bins, d_model//4)\n",
    "        self.emb_x1 = nn.Embedding(layout_bins, d_model//4)\n",
    "        self.emb_w  = nn.Embedding(layout_bins, d_model//4)\n",
    "        self.emb_y0 = nn.Embedding(layout_bins, d_model//4)\n",
    "        self.emb_y1 = nn.Embedding(layout_bins, d_model//4)\n",
    "        self.emb_h  = nn.Embedding(layout_bins, d_model//4)\n",
    "\n",
    "        self.proj_layout = nn.Linear((d_model//4)*6, d_model)\n",
    "\n",
    "        self.emb_pos = nn.Embedding(max_pos, d_model)\n",
    "        self.emb_page = nn.Embedding(max_pages, d_model)\n",
    "\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, units: List[Dict[str, Any]], page_images: Dict[int, str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        return: (L, d_model)\n",
    "        \"\"\"\n",
    "        L = len(units)\n",
    "        # 逐页拿宽高\n",
    "        page_wh = {}\n",
    "        for pid, pth in page_images.items():\n",
    "            img = Image.open(pth)\n",
    "            page_wh[int(pid)] = (img.width, img.height)\n",
    "\n",
    "        layout_vecs = []\n",
    "        pos_ids = []\n",
    "        page_ids = []\n",
    "\n",
    "        for i,u in enumerate(units):\n",
    "            pid = int(u[\"page_id\"])\n",
    "            w, h = page_wh[pid]\n",
    "            b = u.get(\"box\", None)\n",
    "            if b is None:\n",
    "                b = u.get(\"bbox\", None)\n",
    "            if b is None:\n",
    "                raise KeyError(\"Unit missing both 'box' and 'bbox'\")\n",
    "            x0, y0, x1, y1 = b\n",
    "\n",
    "            nb = normalize_box_xyxy([x0,y0,x1,y1], w, h, bins=self.layout_bins-1)\n",
    "            nx0, ny0, nx1, ny1 = nb\n",
    "            nw = int(np.clip(nx1 - nx0, 0, self.layout_bins-1))\n",
    "            nh = int(np.clip(ny1 - ny0, 0, self.layout_bins-1))\n",
    "\n",
    "            layout_vecs.append([nx0, nx1, nw, ny0, ny1, nh])\n",
    "            pos_ids.append(i)\n",
    "            page_ids.append(min(pid, self.emb_page.num_embeddings-1))\n",
    "\n",
    "        layout = torch.tensor(layout_vecs, dtype=torch.long, device=self.emb_pos.weight.device)  # (L,6)\n",
    "        pos = torch.tensor(pos_ids, dtype=torch.long, device=self.emb_pos.weight.device)        # (L,)\n",
    "        pages = torch.tensor(page_ids, dtype=torch.long, device=self.emb_pos.weight.device)     # (L,)\n",
    "\n",
    "        x0 = self.emb_x0(layout[:,0])\n",
    "        x1 = self.emb_x1(layout[:,1])\n",
    "        ww = self.emb_w(layout[:,2])\n",
    "        y0 = self.emb_y0(layout[:,3])\n",
    "        y1 = self.emb_y1(layout[:,4])\n",
    "        hh = self.emb_h(layout[:,5])\n",
    "        layout_cat = torch.cat([x0,x1,ww,y0,y1,hh], dim=-1)\n",
    "        layout_emb = self.proj_layout(layout_cat)\n",
    "\n",
    "        pos_emb = self.emb_pos(pos)\n",
    "        page_emb = self.emb_page(pages)\n",
    "\n",
    "        out = self.ln(layout_emb + pos_emb + page_emb)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3e51dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import roi_align\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "\n",
    "class VisualFPNRoIEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper-style visual embedding:\n",
    "    page image -> ResNet50+FPN -> RoIAlign by bbox -> pooled -> Linear -> (L, d_model)\n",
    "\n",
    "    Interface kept identical to your existing VisualCropEmbedder:\n",
    "      forward(units, page_images) -> Tensor[L, d_model]\n",
    "    where:\n",
    "      - units: list of dict, each must have:\n",
    "          u[\"page_id\"] : int\n",
    "          u[\"box\"]     : [x0, y0, x1, y1] in *pixel coords of the original page image*\n",
    "      - page_images: dict or list-like indexed by page_id -> image path\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        roi_out_size: int = 7,     # RoIAlign output spatial size (7x7 typical)\n",
    "        roi_sampling_ratio: int = 2,\n",
    "        fpn_level: str = \"0\",      # torchvision FPN returns keys like \"0\",\"1\",\"2\",\"3\",\"pool\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.roi_out_size = roi_out_size\n",
    "        self.roi_sampling_ratio = roi_sampling_ratio\n",
    "        self.fpn_level = fpn_level\n",
    "\n",
    "        # ResNet50 + FPN backbone (detection-style)\n",
    "        # returns dict of feature maps: {\"0\":P2,\"1\":P3,\"2\":P4,\"3\":P5,\"pool\":P6}\n",
    "        self.backbone = resnet_fpn_backbone(\n",
    "            backbone_name=\"resnet50\",\n",
    "            weights=torchvision.models.ResNet50_Weights.DEFAULT,\n",
    "            trainable_layers=3,   # 可按需调；最小化侵入先这么设\n",
    "        )\n",
    "\n",
    "        # For resnet_fpn_backbone, each pyramid level channel is 256\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.proj = nn.Linear(256, d_model)\n",
    "\n",
    "        # ImageNet normalization\n",
    "        w = torchvision.models.ResNet50_Weights.DEFAULT\n",
    "        mean, std = w.transforms().mean, w.transforms().std\n",
    "        self.tf = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ])\n",
    "\n",
    "    def _load_page(self, page_path: str):\n",
    "        img = Image.open(page_path).convert(\"RGB\")\n",
    "        return img\n",
    "\n",
    "    def _to_tensor(self, pil_img: Image.Image, device: torch.device):\n",
    "        x = self.tf(pil_img).unsqueeze(0).to(device)  # (1,3,H,W)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _boxes_to_roi_format(boxes_xyxy, batch_idx: int = 0, device=None):\n",
    "        \"\"\"\n",
    "        roi_align expects boxes as Tensor[K,5] = (batch_idx, x1, y1, x2, y2)\n",
    "        in input image pixel coordinates when spatial_scale is set properly.\n",
    "        \"\"\"\n",
    "        b = torch.tensor(boxes_xyxy, dtype=torch.float32, device=device)\n",
    "        if b.numel() == 0:\n",
    "            return b.new_zeros((0, 5))\n",
    "        idx = torch.full((b.shape[0], 1), float(batch_idx), device=device)\n",
    "        return torch.cat([idx, b], dim=1)\n",
    "\n",
    "    def forward(self, units, page_images):\n",
    "        device = self.proj.weight.device\n",
    "\n",
    "        # group unit indices by page_id (so each page runs backbone once)\n",
    "        page_to_indices = {}\n",
    "        for i, u in enumerate(units):\n",
    "            pid = int(u[\"page_id\"])\n",
    "            page_to_indices.setdefault(pid, []).append(i)\n",
    "\n",
    "        out = torch.zeros((len(units), self.d_model), device=device)\n",
    "\n",
    "        for pid, idxs in page_to_indices.items():\n",
    "            page_path = page_images[pid]\n",
    "            pil_img = self._load_page(page_path)\n",
    "            W, H = pil_img.size\n",
    "\n",
    "            x = self._to_tensor(pil_img, device=device)  # (1,3,H,W)\n",
    "\n",
    "            feats = self.backbone(x)  # dict of feature maps\n",
    "            if self.fpn_level not in feats:\n",
    "                # fallback: use the highest-resolution level if key missing\n",
    "                # typical keys: \"0\",\"1\",\"2\",\"3\",\"pool\"\n",
    "                key = sorted([k for k in feats.keys() if k != \"pool\"])[0]\n",
    "            else:\n",
    "                key = self.fpn_level\n",
    "\n",
    "            fmap = feats[key]  # (1,256,hf,wf)\n",
    "            hf, wf = fmap.shape[-2], fmap.shape[-1]\n",
    "\n",
    "            # spatial_scale maps original image coords -> feature map coords\n",
    "            # roi_align uses a single scalar; assume isotropic scaling (works because fmap came from that image)\n",
    "            spatial_scale = wf / float(W)\n",
    "\n",
    "            boxes_xyxy = [units[i][\"box\"] for i in idxs]  # list of [x0,y0,x1,y1] in image pixels\n",
    "            rois = self._boxes_to_roi_format(boxes_xyxy, batch_idx=0, device=device)  # (K,5)\n",
    "\n",
    "            roi_feat = roi_align(\n",
    "                input=fmap,\n",
    "                boxes=rois,\n",
    "                output_size=(self.roi_out_size, self.roi_out_size),\n",
    "                spatial_scale=spatial_scale,\n",
    "                sampling_ratio=self.roi_sampling_ratio,\n",
    "                aligned=True,\n",
    "            )  # (K,256,roi,roi)\n",
    "\n",
    "            roi_feat = self.pool(roi_feat).flatten(1)  # (K,256)\n",
    "            emb = self.proj(roi_feat)                  # (K,d_model)\n",
    "\n",
    "            out[idxs] = emb\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def81414",
   "metadata": {},
   "source": [
    "## 6. DSPS 模型主体\n",
    "\n",
    "- 多模态编码\n",
    "- 单元分类（node class）\n",
    "- 关系分类（edge/rel）\n",
    "- 论文中的 DSPS 思路在此实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2af112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSPSModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        num_rel: int,\n",
    "        M_cp: np.ndarray,\n",
    "        cfg: CFG,\n",
    "        use_text: bool = False,\n",
    "        use_visual: bool = True,\n",
    "        use_softmask: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_softmask = use_softmask\n",
    "        self.num_classes = num_classes\n",
    "        self.num_rel = num_rel\n",
    "        self.cfg = cfg\n",
    "        self.use_text = use_text\n",
    "        self.use_visual = use_visual\n",
    "\n",
    "        # embeddings\n",
    "        self.layout_pos_page = LayoutPosPageEmbedder(\n",
    "            d_model=cfg.d_model,\n",
    "            layout_bins=cfg.layout_bins,\n",
    "            max_pos=cfg.max_1d_pos,\n",
    "            max_pages=cfg.max_pages,\n",
    "        )\n",
    "\n",
    "        if use_text:\n",
    "            self.text_emb = SBERTTextEmbedder(d_model=cfg.d_model)\n",
    "        else:\n",
    "            self.text_emb = None\n",
    "\n",
    "        if use_visual:\n",
    "            self.vis_emb = VisualFPNRoIEmbedder(d_model=cfg.d_model)\n",
    "        else:\n",
    "            self.vis_emb = None\n",
    "\n",
    "        self.fuse_ln = nn.LayerNorm(cfg.d_model)\n",
    "\n",
    "        # encoder (bidirectional)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=cfg.d_model,\n",
    "            nhead=cfg.nhead,\n",
    "            dim_feedforward=cfg.d_model * 4,\n",
    "            dropout=cfg.dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=cfg.num_layers)\n",
    "\n",
    "        # subtask1: class\n",
    "        self.cls_head = nn.Linear(cfg.d_model, num_classes)\n",
    "\n",
    "        # decoder: structure-aware GRU\n",
    "        self.gru = nn.GRU(input_size=cfg.d_model, hidden_size=cfg.d_model, batch_first=True)\n",
    "\n",
    "        # attention projections for parent finding\n",
    "        self.Wq = nn.Linear(cfg.d_model, cfg.d_model, bias=False)\n",
    "        self.Wk = nn.Linear(cfg.d_model, cfg.d_model, bias=False)\n",
    "\n",
    "        # relation head (concat)\n",
    "        self.rel_head = nn.Sequential(\n",
    "            nn.Linear(cfg.d_model * 2, cfg.d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(cfg.dropout),\n",
    "            nn.Linear(cfg.d_model, num_rel),\n",
    "        )\n",
    "\n",
    "        # store M_cp (torch)\n",
    "        # shape: (num_classes+1, num_classes)   rows=parent_class + ROOT(row=num_classes), cols=child_class\n",
    "        M = torch.tensor(M_cp, dtype=torch.float32)\n",
    "        self.register_buffer(\"M_cp\", M)\n",
    "\n",
    "    def forward(self, doc: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        doc keys from your dataset:\n",
    "          doc_id, units(list), y_cls(list), y_parent(list), y_rel(list), page_images(dict)\n",
    "        returns logits:\n",
    "          cls_logits: (L,C)\n",
    "          par_logits: list of length L where par_logits[i] is (i,) logits for j in [0..i-1] + ROOT at index 0?\n",
    "          rel_logits: (L, R) using GT parent during training convenience (可在 loss 里选)\n",
    "        \"\"\"\n",
    "        units = doc[\"units\"]\n",
    "        L = len(units)\n",
    "        doc_id = doc[\"doc_id\"]\n",
    "        page_images = doc[\"page_images\"]\n",
    "\n",
    "        # ---- embeddings sum: text + visual + layout/pos/page ----\n",
    "        x = self.layout_pos_page(units, page_images)  # (L,d)\n",
    "\n",
    "        if self.use_text:\n",
    "            texts = [u.get(\"text\",\"\") for u in units]\n",
    "            x = x + self.text_emb(doc_id, texts)\n",
    "\n",
    "        if self.use_visual:\n",
    "            x = x + self.vis_emb(units, page_images)\n",
    "\n",
    "        x = self.fuse_ln(x)            # (L,d)\n",
    "        x = x.unsqueeze(0)             # (1,L,d) for transformer batch_first\n",
    "\n",
    "        # ---- encoder ----\n",
    "        x_star = self.encoder(x)       # (1,L,d)\n",
    "        x_star = x_star.squeeze(0)     # (L,d)\n",
    "\n",
    "        # ---- class logits ----\n",
    "        cls_logits = self.cls_head(x_star)  # (L,C)\n",
    "        cls_prob = F.softmax(cls_logits, dim=-1)  # (L,C)\n",
    "\n",
    "        # ---- ROOT representation ----\n",
    "        root = x_star.mean(dim=0, keepdim=True)  # (1,d)\n",
    "\n",
    "        # ---- GRU decoder (causal) ----\n",
    "        # 论文写用 x*_{i-1} 驱动，这里用全序列输入 + 自己取 h_i（等价实现）\n",
    "        h_seq, _ = self.gru(x_star.unsqueeze(0))  # (1,L,d)\n",
    "        h_seq = h_seq.squeeze(0)                  # (L,d)\n",
    "\n",
    "        # ---- parent logits with soft-mask ----\n",
    "        # 我们把 candidate set 定义为 [ROOT] + [0..i-1]\n",
    "        # 输出 par_logits[i] 形状 (i+1,) 其中 index0=ROOT, index k>0 对应 parent=j=k-1\n",
    "        par_logits = []\n",
    "        eps = 1e-8\n",
    "\n",
    "        # 预先准备 ROOT 的 class distribution：用 uniform 或者用 mean prob；论文是扩展 P_cls(0) 为 (C+1)，ROOT=1\n",
    "        # 我这里做：P_cls_root_over_parentclass = one-hot at ROOT row\n",
    "        # 计算 P_dom 时使用 rows=parentclass+ROOT\n",
    "        # 具体：P̃_cls(j) = [P_cls(j), 0] for real nodes；ROOT 用 [0..0,1]\n",
    "        root_one = torch.zeros((1, self.num_classes + 1), device=x_star.device)\n",
    "        root_one[0, self.num_classes] = 1.0\n",
    "\n",
    "        # child prob 扩成 (C) 即原本；parent prob 扩成 (C+1)\n",
    "        # 对每个 i:\n",
    "        for i in range(L):\n",
    "            q = self.Wq(h_seq[i:i+1])                # (1,d)\n",
    "            # keys = [ROOT] + past h\n",
    "            k_root = self.Wk(root)                   # (1,d)\n",
    "            if i == 0:\n",
    "                keys = k_root                        # (1,d)\n",
    "            else:\n",
    "                k_past = self.Wk(h_seq[:i])          # (i,d)\n",
    "                keys = torch.cat([k_root, k_past], dim=0)  # (i+1,d)\n",
    "\n",
    "            # dot-product scores\n",
    "            score = (q @ keys.t()).squeeze(0)        # (i+1,)\n",
    "\n",
    "            # ----- soft-mask prior: P_dom(i, j) -----\n",
    "            # child distribution: (C)\n",
    "            p_child = cls_prob[i:i+1]                # (1,C)\n",
    "            # parent distributions:\n",
    "            if i == 0:\n",
    "                p_parent_ext = root_one              # (1,C+1)\n",
    "            else:\n",
    "                p_parent = cls_prob[:i]              # (i,C)\n",
    "                zeros = torch.zeros((i,1), device=x_star.device)\n",
    "                p_parent_ext = torch.cat([p_parent, zeros], dim=1)  # (i,C+1)\n",
    "                p_parent_ext = torch.cat([root_one, p_parent_ext], dim=0)  # (i+1,C+1)\n",
    "\n",
    "            # P_dom = p_parent_ext @ M_cp @ p_child^T\n",
    "            # M_cp: (C+1,C)\n",
    "            prior = (p_parent_ext @ self.M_cp @ p_child.t()).squeeze(-1)  # (i+1,)\n",
    "\n",
    "            if self.use_softmask:\n",
    "                score = score + torch.log(prior + eps)\n",
    "\n",
    "            par_logits.append(score)\n",
    "\n",
    "        # ---- relation logits (用 GT parent 保持训练稳定；推理时再用预测 parent) ----\n",
    "        # 这里先输出一个 (L,R) 的 logits，其中第 i 个是与 GT parent 的关系\n",
    "        # 对于 parent=-1 的（ROOT），relation 按你的数据里常见是 \"contain\"/\"meta\"，这里仍然算一个 rel loss（你也可 mask 掉）\n",
    "        y_parent = doc.get(\"y_parent\", [-1]*L)\n",
    "        rel_logits = []\n",
    "        for i in range(L):\n",
    "            p = y_parent[i]\n",
    "            if p is None or p < 0:\n",
    "                # ROOT\n",
    "                parent_vec = root.squeeze(0)\n",
    "            else:\n",
    "                parent_vec = h_seq[p]\n",
    "            feat = torch.cat([h_seq[i], parent_vec], dim=-1)\n",
    "            rel_logits.append(self.rel_head(feat))\n",
    "        rel_logits = torch.stack(rel_logits, dim=0)  # (L,R)\n",
    "\n",
    "        return {\n",
    "            \"cls_logits\": cls_logits,\n",
    "            \"par_logits\": par_logits,  # list of tensors\n",
    "            \"rel_logits\": rel_logits,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9745d4e",
   "metadata": {},
   "source": [
    "## 7. 损失函数与训练目标\n",
    "\n",
    "- FocalLoss\n",
    "- compute_losses：对分类/关系等分项计算并聚合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eca9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=0.25, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        logits: (N,C)\n",
    "        targets: (N,)\n",
    "        \"\"\"\n",
    "        ce = F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-ce)\n",
    "        focal = self.alpha * (1 - pt) ** self.gamma * ce\n",
    "        if self.reduction == \"mean\":\n",
    "            return focal.mean()\n",
    "        if self.reduction == \"sum\":\n",
    "            return focal.sum()\n",
    "        return focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48f004bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses(\n",
    "    out: Dict[str, Any],\n",
    "    doc: Dict[str, Any],\n",
    "    num_classes: int,\n",
    "    num_rel: int,\n",
    "    cfg: CFG,\n",
    "    focal_cls: nn.Module,\n",
    "    focal_rel: nn.Module,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "\n",
    "    # ---- targets ----\n",
    "    y_cls = torch.tensor(doc[\"y_cls\"], dtype=torch.long, device=device)\n",
    "    y_parent = doc[\"y_parent\"]\n",
    "    y_rel = torch.tensor(doc[\"y_rel\"], dtype=torch.long, device=device)\n",
    "\n",
    "    # ---- logits ----\n",
    "    cls_logits = out[\"cls_logits\"].to(device)   # (L,C)\n",
    "    rel_logits = out[\"rel_logits\"].to(device)   # (L,R)\n",
    "    par_logits_list = out[\"par_logits\"]         # list length L\n",
    "\n",
    "    L = len(par_logits_list)\n",
    "\n",
    "    # ========== sanity checks ==========\n",
    "    # cls range\n",
    "    cls_min = int(y_cls.min().item())\n",
    "    cls_max = int(y_cls.max().item())\n",
    "    if cls_min < 0 or cls_max >= num_classes:\n",
    "        raise ValueError(f\"y_cls out of range: min={cls_min}, max={cls_max}, num_classes={num_classes}\")\n",
    "\n",
    "    # rel range\n",
    "    rel_min = int(y_rel.min().item())\n",
    "    rel_max = int(y_rel.max().item())\n",
    "    if rel_min < 0 or rel_max >= num_rel:\n",
    "        raise ValueError(f\"y_rel out of range: min={rel_min}, max={rel_max}, num_rel={num_rel}\")\n",
    "\n",
    "    # parent range per position\n",
    "    for i in range(L):\n",
    "        p = y_parent[i]\n",
    "        if p is None:\n",
    "            continue\n",
    "        if p >= i:  # 注意：parent 必须 < i（只能指向过去）\n",
    "            raise ValueError(f\"y_parent invalid at i={i}: parent={p} but must be < {i}\")\n",
    "        if p < -1:\n",
    "            raise ValueError(f\"y_parent invalid at i={i}: parent={p} (should be -1 or >=0)\")\n",
    "\n",
    "    # ========== losses ==========\n",
    "    # meta mask：True 表示参与结构监督的单元（非 meta）\n",
    "    is_meta = torch.tensor(doc.get(\"is_meta\", [False]*L), dtype=torch.bool, device=device)\n",
    "    struct_mask = ~is_meta  # (L,)\n",
    "\n",
    "    # Subtask1: class（是否 mask meta 看你后续要不要对齐论文；先不 mask，保证能跑通）\n",
    "    loss_cls = focal_cls(cls_logits, y_cls)\n",
    "\n",
    "    # Subtask2: parent（只对非 meta 计算）\n",
    "    loss_par = 0.0\n",
    "    denom_par = 0\n",
    "    for i, logits_i in enumerate(par_logits_list):\n",
    "        if not bool(struct_mask[i].item()):\n",
    "            continue\n",
    "\n",
    "        p = y_parent[i]\n",
    "        tgt_i = 0 if (p is None or p < 0) else (p + 1)\n",
    "\n",
    "        if tgt_i < 0 or tgt_i >= logits_i.numel():\n",
    "            raise ValueError(\n",
    "                f\"parent target out of range at i={i}: tgt={tgt_i}, \"\n",
    "                f\"logits_len={logits_i.numel()}, raw_parent={p}\"\n",
    "            )\n",
    "\n",
    "        tgt = torch.tensor([tgt_i], dtype=torch.long, device=device)\n",
    "        # logits_i 可能是 shape=(K,) 或 (1,K)。统一压成 (1,K)\n",
    "        logits_ce = logits_i.to(device).reshape(1, -1)\n",
    "        loss_par = loss_par + F.cross_entropy(logits_ce, tgt)\n",
    "\n",
    "        denom_par += 1\n",
    "\n",
    "    if denom_par == 0:\n",
    "        loss_par = torch.zeros((), device=device)\n",
    "    else:\n",
    "        loss_par = loss_par / denom_par\n",
    "\n",
    "    # Subtask3: relation（只对非 meta 计算）\n",
    "    if struct_mask.any():\n",
    "        loss_rel = focal_rel(rel_logits[struct_mask], y_rel[struct_mask])\n",
    "    else:\n",
    "        loss_rel = torch.zeros((), device=device)\n",
    "\n",
    "    loss = loss_cls + cfg.alpha_parent * loss_par + cfg.alpha_rel * loss_rel\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"loss_cls\": loss_cls.detach(),\n",
    "        \"loss_par\": loss_par.detach(),\n",
    "        \"loss_rel\": loss_rel.detach(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49b541",
   "metadata": {},
   "source": [
    "## 8. 训练循环\n",
    "\n",
    "- train_one_epoch：单 epoch 训练\n",
    "- DataLoader 构建\n",
    "- 优化器与训练入口\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "360be9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, cfg, focal_cls, focal_rel):\n",
    "    model.train()\n",
    "    logs = {\"loss\": [], \"loss_cls\": [], \"loss_par\": [], \"loss_rel\": []}\n",
    "\n",
    "    start_time = time.time()\n",
    "    last_print = start_time\n",
    "\n",
    "    print(f\"\\n[Train] start epoch, num_docs = {len(loader)}\")\n",
    "\n",
    "    for step, doc in enumerate(loader):\n",
    "        t0 = time.time()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # ===== forward =====\n",
    "        out = model(doc)\n",
    "\n",
    "        # ===== loss =====\n",
    "        losses = compute_losses(out, doc, model.num_classes, model.num_rel,\n",
    "                                cfg, focal_cls, focal_rel)\n",
    "\n",
    "        # ===== backward =====\n",
    "        losses[\"loss\"].backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # ===== log =====\n",
    "        for k in logs:\n",
    "            logs[k].append(float(losses[k].cpu()))\n",
    "\n",
    "        # ===== progress print =====\n",
    "        if step == 0 or (step + 1) % 5 == 0:\n",
    "            now = time.time()\n",
    "            step_time = now - t0\n",
    "            elapsed = now - start_time\n",
    "\n",
    "            print(\n",
    "                f\"  step {step+1:4d}/{len(loader)} | \"\n",
    "                f\"step_time={step_time:5.2f}s | \"\n",
    "                f\"loss={logs['loss'][-1]:.4f} \"\n",
    "                f\"(cls={logs['loss_cls'][-1]:.3f}, \"\n",
    "                f\"par={logs['loss_par'][-1]:.3f}, \"\n",
    "                f\"rel={logs['loss_rel'][-1]:.3f}) | \"\n",
    "                f\"elapsed={elapsed/60:.1f}min\"\n",
    "            )\n",
    "\n",
    "        # ===== quick sanity check (只在最前面几步) =====\n",
    "        if step < 2:\n",
    "            L = len(doc[\"units\"])\n",
    "            print(f\"    sanity: seq_len={L}, \"\n",
    "                  f\"avg_parent_candidates={sum(len(x) for x in out['par_logits'])/L:.1f}\")\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"[Train] epoch done in {epoch_time/60:.2f} min\")\n",
    "\n",
    "    return {k: sum(v)/max(len(v),1) for k,v in logs.items()}\n",
    "\n",
    "def eval_one_epoch(model, loader, cfg, focal_cls, focal_rel):\n",
    "    model.eval()\n",
    "    logs = {\"loss\": [], \"loss_cls\": [], \"loss_par\": [], \"loss_rel\": []}\n",
    "\n",
    "    for doc in loader:\n",
    "        out = model(doc)\n",
    "        losses = compute_losses(out, doc, model.num_classes, model.num_rel, cfg, focal_cls, focal_rel)\n",
    "        for k in logs:\n",
    "            logs[k].append(float(losses[k].cpu()))\n",
    "    return {k: sum(v)/max(len(v),1) for k,v in logs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acbe49ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "# ===== dataset =====\n",
    "train_ds = HRDHDataset(HRDH_ROOT, split=\"train\", max_len=cfg.max_len)\n",
    "test_ds  = HRDHDataset(HRDH_ROOT, split=\"test\",  max_len=cfg.max_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_doc,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_doc,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "\n",
    "# ===== M_cp =====\n",
    "M_cp = np.load(\"M_cp_hrdh.npy\")  # shape (C+1,C)\n",
    "\n",
    "# ===== model =====\n",
    "num_classes = len(ID2LABEL_14)\n",
    "num_rel = len(REL2ID)  # 你的 REL2ID 包含 meta；如果你想只做三类，把 meta 从标签里移除并重映射\n",
    "\n",
    "model = DSPSModel(\n",
    "    num_classes=num_classes,\n",
    "    num_rel=num_rel,\n",
    "    M_cp=M_cp,\n",
    "    cfg=cfg,\n",
    "    use_text=False,      # 如果本地没 sentence-transformers 可先设 False\n",
    "    use_visual=False,    # 如果显存吃紧可先 False\n",
    ").to(device)\n",
    "\n",
    "# ===== optim =====\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "focal_cls = FocalLoss(gamma=cfg.focal_gamma, alpha=cfg.focal_alpha).to(device)\n",
    "focal_rel = FocalLoss(gamma=cfg.focal_gamma, alpha=cfg.focal_alpha).to(device)\n",
    "\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b04793f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train] start epoch, num_docs = 1000\n",
      "  step    1/1000 | step_time= 1.23s | loss=6.2914 (cls=0.612, par=5.467, rel=0.212) | elapsed=0.0min\n",
      "    sanity: seq_len=512, avg_parent_candidates=256.5\n",
      "    sanity: seq_len=512, avg_parent_candidates=256.5\n",
      "  step    5/1000 | step_time= 0.86s | loss=3.9795 (cls=0.089, par=3.761, rel=0.129) | elapsed=0.1min\n",
      "  step   10/1000 | step_time= 0.83s | loss=4.2965 (cls=0.143, par=3.999, rel=0.155) | elapsed=0.1min\n",
      "  step   15/1000 | step_time= 0.76s | loss=3.5786 (cls=0.100, par=3.391, rel=0.088) | elapsed=0.2min\n",
      "  step   20/1000 | step_time= 0.79s | loss=3.4521 (cls=0.159, par=3.202, rel=0.092) | elapsed=0.2min\n",
      "  step   25/1000 | step_time= 0.80s | loss=3.0337 (cls=0.165, par=2.778, rel=0.091) | elapsed=0.3min\n",
      "  step   30/1000 | step_time= 0.52s | loss=3.0748 (cls=0.153, par=2.833, rel=0.089) | elapsed=0.4min\n",
      "  step   35/1000 | step_time= 0.30s | loss=2.5898 (cls=0.197, par=2.304, rel=0.088) | elapsed=0.4min\n",
      "  step   40/1000 | step_time= 0.80s | loss=2.5969 (cls=0.098, par=2.423, rel=0.075) | elapsed=0.5min\n",
      "  step   45/1000 | step_time= 0.63s | loss=2.9486 (cls=0.057, par=2.839, rel=0.052) | elapsed=0.5min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m focal_rel = FocalLoss(gamma=\u001b[32m2.0\u001b[39m, alpha=\u001b[32m0.25\u001b[39m).to(device)\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, cfg.epochs+\u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     tr = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfocal_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfocal_rel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(ep, tr)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimizer, cfg, focal_cls, focal_rel)\u001b[39m\n\u001b[32m     18\u001b[39m out = model(doc)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# ===== loss =====\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m losses = \u001b[43mcompute_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_rel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfocal_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfocal_rel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# ===== backward =====\u001b[39;00m\n\u001b[32m     25\u001b[39m losses[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m].backward()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mcompute_losses\u001b[39m\u001b[34m(out, doc, num_classes, num_rel, cfg, focal_cls, focal_rel)\u001b[39m\n\u001b[32m     72\u001b[39m     logits_ce = logits_i.to(device).reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m     73\u001b[39m     loss_par = loss_par + F.cross_entropy(logits_ce, tgt)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     denom_par += \u001b[32m1\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m denom_par == \u001b[32m0\u001b[39m:\n\u001b[32m     78\u001b[39m     loss_par = torch.zeros((), device=device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "cfg.epochs = 2\n",
    "cfg.num_workers = 0\n",
    "\n",
    "model = DSPSModel(\n",
    "    num_classes=num_classes,\n",
    "    num_rel=num_rel,\n",
    "    M_cp=M_cp,\n",
    "    cfg=cfg,\n",
    "    use_text=False,\n",
    "    use_visual=False,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-2)\n",
    "focal_cls = FocalLoss(gamma=2.0, alpha=0.25).to(device)\n",
    "focal_rel = FocalLoss(gamma=2.0, alpha=0.25).to(device)\n",
    "\n",
    "for ep in range(1, cfg.epochs+1):\n",
    "    tr = train_one_epoch(model, train_loader, optimizer, cfg, focal_cls, focal_rel)\n",
    "    print(ep, tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e38104",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = 1e9\n",
    "for ep in range(1, cfg.epochs+1):\n",
    "    tr = train_one_epoch(model, train_loader, optimizer, cfg, focal_cls, focal_rel)\n",
    "    te = eval_one_epoch(model, test_loader, cfg, focal_cls, focal_rel)\n",
    "    print(f\"[ep {ep}] train:\", tr, \" | test:\", te)\n",
    "\n",
    "    if te[\"loss\"] < best:\n",
    "        best = te[\"loss\"]\n",
    "        torch.save(model.state_dict(), \"dsps_hrdh_best.pt\")\n",
    "        print(\"saved: dsps_hrdh_best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa9bca",
   "metadata": {},
   "source": [
    "## 9. 推理与预测导出\n",
    "\n",
    "- 单文档推理\n",
    "- 带关系重组/后处理的推理\n",
    "- 批量导出 split 预测结果到目录\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a51892",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_doc(model: DSPSModel, doc: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    model.eval()\n",
    "    out = model(doc)\n",
    "\n",
    "    cls = out[\"cls_logits\"].argmax(dim=-1).cpu().tolist()\n",
    "\n",
    "    # parent: each i logits over [ROOT]+past\n",
    "    par = []\n",
    "    for i, logits_i in enumerate(out[\"par_logits\"]):\n",
    "        idx = int(torch.argmax(logits_i).cpu())\n",
    "        p = -1 if idx == 0 else (idx - 1)\n",
    "        par.append(p)\n",
    "\n",
    "    # relation: 这里的 rel_logits 是用 GT parent 生成的；\n",
    "    # 推理时更严格做法：用预测 parent 重算 rel（下面给你重算版本）\n",
    "    # 先给简版：\n",
    "    rel = out[\"rel_logits\"].argmax(dim=-1).cpu().tolist()\n",
    "\n",
    "    return {\"pred_cls\": cls, \"pred_parent\": par, \"pred_rel\": rel}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ea42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_doc_with_rel_recompute(model, doc, device):\n",
    "    \"\"\"\n",
    "    返回：\n",
    "      pred_cls: (L,)\n",
    "      pred_parent: (L,)  -1 表示 ROOT\n",
    "      pred_rel: (L,)\n",
    "      also return raw probs/logits if needed\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    out = model(doc)\n",
    "\n",
    "    cls_logits = out[\"cls_logits\"].to(device)          # (L,C)\n",
    "    par_logits_list = out[\"par_logits\"]                # list of (i+1,)\n",
    "    L = cls_logits.shape[0]\n",
    "\n",
    "    pred_cls = cls_logits.argmax(dim=-1).detach().cpu().tolist()\n",
    "\n",
    "    # parent decode: candidates = [ROOT] + [0..i-1]\n",
    "    pred_parent = []\n",
    "    for i, logits_i in enumerate(par_logits_list):\n",
    "        idx = int(torch.argmax(logits_i).item())\n",
    "        p = -1 if idx == 0 else (idx - 1)\n",
    "        pred_parent.append(p)\n",
    "\n",
    "    # recompute relation logits using predicted parent\n",
    "    # need access to h_seq and root inside the model; easiest: re-run minimal pieces here\n",
    "    # We can reconstruct h_seq by running embeddings+encoder+gru again using model modules\n",
    "    units = doc[\"units\"]\n",
    "    doc_id = doc[\"doc_id\"]\n",
    "    page_images = doc[\"page_images\"]\n",
    "\n",
    "    # embeddings sum (same as forward)\n",
    "    x = model.layout_pos_page(units, page_images)\n",
    "    if model.use_text:\n",
    "        texts = [u.get(\"text\",\"\") for u in units]\n",
    "        x = x + model.text_emb(doc_id, texts)\n",
    "    if model.use_visual:\n",
    "        x = x + model.vis_emb(units, page_images)\n",
    "    x = model.fuse_ln(x)                       # (L,d)\n",
    "    x_star = model.encoder(x.unsqueeze(0)).squeeze(0)  # (L,d)\n",
    "    root = x_star.mean(dim=0, keepdim=True)    # (1,d)\n",
    "    h_seq = model.gru(x_star.unsqueeze(0))[0].squeeze(0)  # (L,d)\n",
    "\n",
    "    rel_logits = []\n",
    "    for i in range(L):\n",
    "        p = pred_parent[i]\n",
    "        # --- safe parent index for relation recompute ---\n",
    "        # p could be invalid (>=len) during early training; fallback to root for safety\n",
    "        L = h_seq.size(0)\n",
    "        if (p is None) or (p < 0) or (p >= L):\n",
    "            parent_vec = root.squeeze(0)\n",
    "            # 可选：记录一次非法 parent（不影响逻辑）\n",
    "            # doc.setdefault(\"_bad_parent_count\", 0)\n",
    "                    # doc[\"_bad_parent_count\"] += 1\n",
    "        else:\n",
    "            parent_vec = h_seq[p]\n",
    "\n",
    "        feat = torch.cat([h_seq[i], parent_vec], dim=-1)\n",
    "        rel_logits.append(model.rel_head(feat))\n",
    "\n",
    "    rel_logits = torch.stack(rel_logits, dim=0)  # (L,R)\n",
    "\n",
    "    pred_rel = rel_logits.argmax(dim=-1).detach().cpu().tolist()\n",
    "\n",
    "    return {\n",
    "        \"pred_cls\": pred_cls,\n",
    "        \"pred_parent\": pred_parent,\n",
    "        \"pred_rel\": pred_rel,\n",
    "        \"cls_logits\": cls_logits.detach().cpu(),\n",
    "        \"rel_logits\": rel_logits.detach().cpu(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc28d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _id2name(mapping, idx: int) -> str:\n",
    "    \"\"\"\n",
    "    mapping 可以是：\n",
    "      - list: mapping[idx]\n",
    "      - dict: mapping.get(idx)\n",
    "      - None: 返回 idx 字符串\n",
    "    \"\"\"\n",
    "    if mapping is None:\n",
    "        return str(idx)\n",
    "    if isinstance(mapping, dict):\n",
    "        return mapping.get(idx, str(idx))\n",
    "    if isinstance(mapping, (list, tuple)):\n",
    "        if 0 <= idx < len(mapping):\n",
    "            return str(mapping[idx])\n",
    "        return str(idx)\n",
    "    return str(idx)\n",
    "\n",
    "\n",
    "def export_tree_json(doc, pred=None):\n",
    "    \"\"\"\n",
    "    doc: dataset 返回的 doc dict\n",
    "    pred: None => 导出 GT\n",
    "          dict => 导出 pred（需要含 pred_cls/pred_parent/pred_rel）\n",
    "    输出格式：\n",
    "      {\n",
    "        doc_id,\n",
    "        nodes: [\n",
    "          {id, text, label_id, label_name, is_meta, parent, rel_id, rel_name, page_id, box},\n",
    "          ...\n",
    "        ]\n",
    "      }\n",
    "    \"\"\"\n",
    "    doc_id = doc[\"doc_id\"]\n",
    "    units = doc[\"units\"]\n",
    "    L = len(units)\n",
    "\n",
    "    if pred is None:\n",
    "        cls_ids = doc[\"y_cls\"]\n",
    "        parent = doc[\"y_parent\"]\n",
    "        rel_ids = doc[\"y_rel\"]\n",
    "    else:\n",
    "        cls_ids = pred[\"pred_cls\"]\n",
    "        parent = pred[\"pred_parent\"]\n",
    "        rel_ids = pred[\"pred_rel\"]\n",
    "\n",
    "    # 兼容你 notebook 里 ID2LABEL_14 / ID2REL 的类型（list 或 dict）\n",
    "    label_map = globals().get(\"ID2LABEL_14\", None)\n",
    "    rel_map = globals().get(\"ID2REL\", None)\n",
    "\n",
    "    out = {\"doc_id\": doc_id, \"nodes\": []}\n",
    "    is_meta_list = doc.get(\"is_meta\", [False]*L)\n",
    "\n",
    "    for i in range(L):\n",
    "        u = units[i]\n",
    "        is_meta = bool(is_meta_list[i])\n",
    "        cls_id = int(cls_ids[i])\n",
    "        rel_id = int(rel_ids[i]) if rel_ids is not None else -1\n",
    "\n",
    "        out[\"nodes\"].append({\n",
    "            \"id\": i,\n",
    "            \"text\": u.get(\"text\", \"\"),\n",
    "            \"label_id\": cls_id,\n",
    "            \"label_name\": _id2name(label_map, cls_id),\n",
    "            \"is_meta\": is_meta,\n",
    "            \"parent\": int(parent[i]) if parent[i] is not None else -1,\n",
    "            \"rel_id\": rel_id,\n",
    "            \"rel_name\": _id2name(rel_map, rel_id),\n",
    "            \"page_id\": int(u.get(\"page_id\", 0)),\n",
    "            \"box\": [int(x) for x in u.get(\"box\", [0,0,0,0])]\n",
    "        })\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531fe758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "@torch.no_grad()\n",
    "def export_split_predictions(model, loader, save_dir, device):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for doc in tqdm(loader, desc=f\"export -> {save_dir}\"):\n",
    "        pred = predict_doc_with_rel_recompute(model, doc, device)\n",
    "\n",
    "        gt_json = export_tree_json(doc, pred=None)\n",
    "        pr_json = export_tree_json(doc, pred=pred)\n",
    "\n",
    "        doc_id = doc[\"doc_id\"].replace(\"/\", \"_\")\n",
    "        with open(os.path.join(save_dir, f\"{doc_id}.gt.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(gt_json, f, ensure_ascii=False, indent=2)\n",
    "        with open(os.path.join(save_dir, f\"{doc_id}.pred.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(pr_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5b87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载最优模型\n",
    "model.load_state_dict(torch.load(\"dsps_hrdh_best.pt\", map_location=device))\n",
    "\n",
    "export_split_predictions(model, test_loader, save_dir=\"exports_hrdh_test\", device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad150067",
   "metadata": {},
   "source": [
    "## 10. STEDS / TED 评估实现\n",
    "\n",
    "- TNode 数据结构\n",
    "- Zhang-Shasha Tree Edit Distance\n",
    "- compute_steds 与导出目录评估\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9577dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, glob\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "def load_pair_files(export_dir: str) -> List[Tuple[str, str]]:\n",
    "    gt_files = sorted(glob.glob(os.path.join(export_dir, \"*.gt.json\")))\n",
    "    pairs = []\n",
    "    for gt in gt_files:\n",
    "        pred = gt.replace(\".gt.json\", \".pred.json\")\n",
    "        if os.path.exists(pred):\n",
    "            pairs.append((gt, pred))\n",
    "    return pairs\n",
    "\n",
    "pairs = load_pair_files(\"exports_hrdh_test\")\n",
    "len(pairs), pairs[0] if pairs else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "@dataclass\n",
    "class TNode:\n",
    "    label: int\n",
    "    text: str\n",
    "    oid: int                 # original id，用来排序\n",
    "    children: List[\"TNode\"]\n",
    "\n",
    "    def __init__(self, label: int, text: str, oid: int):\n",
    "        self.label = label\n",
    "        self.text = text\n",
    "        self.oid = oid\n",
    "        self.children = []\n",
    "\n",
    "def find_effective_parent(i: int) -> int:\n",
    "    \"\"\"\n",
    "    返回 i 的有效父节点：\n",
    "    - 若 parent 指向越界/非法：视为无父节点（-1），挂到 ROOT\n",
    "    - 若 parent 指向被排除节点（meta 或不在 kept 集）：沿 parent 链向上跳\n",
    "    - 增加循环保护，避免死循环\n",
    "    \"\"\"\n",
    "    n = len(parent)\n",
    "\n",
    "    def _is_valid_idx(x: int) -> bool:\n",
    "        return isinstance(x, int) and (0 <= x < n)\n",
    "\n",
    "    p = parent[i] if 0 <= i < n else -1\n",
    "\n",
    "    # 1) 直接非法或 root\n",
    "    if (p is None) or (not isinstance(p, int)) or (p < 0):\n",
    "        return -1\n",
    "    if not _is_valid_idx(p):\n",
    "        return -1\n",
    "\n",
    "    # 2) 向上跳，直到遇到 kept 的父节点或 root/非法\n",
    "    seen = set()\n",
    "    while True:\n",
    "        if p in kept_set:\n",
    "            return p\n",
    "\n",
    "        if p in seen:\n",
    "            # 出现环，按无父节点处理\n",
    "            return -1\n",
    "        seen.add(p)\n",
    "\n",
    "        # 向上跳\n",
    "        pp = parent[p]\n",
    "        if (pp is None) or (not isinstance(pp, int)) or (pp < 0):\n",
    "            return -1\n",
    "        if not _is_valid_idx(pp):\n",
    "            return -1\n",
    "        p = pp\n",
    "\n",
    "    # 找“跳过 meta 后的最近祖先”\n",
    "    def find_effective_parent(i: int) -> int:\n",
    "        p = parent[i]\n",
    "        seen = set()\n",
    "        while True:\n",
    "            if p < 0:\n",
    "                return -1\n",
    "            if p in seen:         # 防环保护\n",
    "                return -1\n",
    "            seen.add(p)\n",
    "            if kept(p):\n",
    "                return p\n",
    "            # p 是 meta 或被排除节点，继续向上跳\n",
    "            p = parent[p]\n",
    "\n",
    "    # 连接\n",
    "    for i in kept_ids:\n",
    "        ep = find_effective_parent(i)\n",
    "        if ep < 0:\n",
    "            root.children.append(obj[i])\n",
    "        else:\n",
    "            obj[ep].children.append(obj[i])\n",
    "\n",
    "    # children 按原始 reading-order id 排序，确保“有序树”一致\n",
    "    def sort_rec(x: TNode):\n",
    "        x.children.sort(key=lambda c: c.oid)\n",
    "        for ch in x.children:\n",
    "            sort_rec(ch)\n",
    "\n",
    "    sort_rec(root)\n",
    "    return root, len(kept_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadfc478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ted_zhang_shasha(t1: TNode, t2: TNode, match_mode: str = \"strict\") -> int:\n",
    "    \"\"\"\n",
    "    Ordered Tree Edit Distance (Zhang-Shasha)\n",
    "    Costs: ins=1, del=1\n",
    "    match_mode:\n",
    "      - \"strict\": label + text 完全一致 cost=0，否则 1\n",
    "      - \"label\": 只要 label 一致 cost=0，否则 1\n",
    "    \"\"\"\n",
    "    def postorder(root: TNode):\n",
    "        nodes = []\n",
    "        def dfs(x: TNode):\n",
    "            for ch in x.children:\n",
    "                dfs(ch)\n",
    "            nodes.append(x)\n",
    "        dfs(root)\n",
    "        return nodes\n",
    "\n",
    "    A = postorder(t1)\n",
    "    B = postorder(t2)\n",
    "\n",
    "    idxA = {id(node): i+1 for i, node in enumerate(A)}\n",
    "    idxB = {id(node): i+1 for i, node in enumerate(B)}\n",
    "\n",
    "    n, m = len(A), len(B)\n",
    "\n",
    "    def leftmost_indices(nodes, idx_map):\n",
    "        l = [0] * (len(nodes) + 1)  # 1-based\n",
    "        for i, node in enumerate(nodes, start=1):\n",
    "            cur = node\n",
    "            while cur.children:\n",
    "                cur = cur.children[0]\n",
    "            l[i] = idx_map[id(cur)]\n",
    "        return l\n",
    "\n",
    "    l1 = leftmost_indices(A, idxA)\n",
    "    l2 = leftmost_indices(B, idxB)\n",
    "\n",
    "    def keyroots(leftmost):\n",
    "        seen = {}\n",
    "        for i in range(1, len(leftmost)):\n",
    "            seen[leftmost[i]] = i\n",
    "        return sorted(seen.values())\n",
    "\n",
    "    kr1 = keyroots(l1)\n",
    "    kr2 = keyroots(l2)\n",
    "\n",
    "    treedist = [[0] * (m + 1) for _ in range(n + 1)]\n",
    "\n",
    "    def relabel_cost(i: int, j: int) -> int:\n",
    "        a = A[i-1]\n",
    "        b = B[j-1]\n",
    "        if match_mode == \"label\":\n",
    "            return 0 if (a.label == b.label) else 1\n",
    "        return 0 if (a.label == b.label and a.text == b.text) else 1\n",
    "\n",
    "    for i in kr1:\n",
    "        for j in kr2:\n",
    "            i0 = l1[i]\n",
    "            j0 = l2[j]\n",
    "            fd = [[0] * (j - j0 + 2) for _ in range(i - i0 + 2)]\n",
    "\n",
    "            for di in range(1, i - i0 + 2):\n",
    "                fd[di][0] = fd[di-1][0] + 1  # delete\n",
    "            for dj in range(1, j - j0 + 2):\n",
    "                fd[0][dj] = fd[0][dj-1] + 1  # insert\n",
    "\n",
    "            for di in range(1, i - i0 + 2):\n",
    "                for dj in range(1, j - j0 + 2):\n",
    "                    ii = i0 + di - 1\n",
    "                    jj = j0 + dj - 1\n",
    "                    if l1[ii] == i0 and l2[jj] == j0:\n",
    "                        fd[di][dj] = min(\n",
    "                            fd[di-1][dj] + 1,\n",
    "                            fd[di][dj-1] + 1,\n",
    "                            fd[di-1][dj-1] + relabel_cost(ii, jj)\n",
    "                        )\n",
    "                        treedist[ii][jj] = fd[di][dj]\n",
    "                    else:\n",
    "                        fd[di][dj] = min(\n",
    "                            fd[di-1][dj] + 1,\n",
    "                            fd[di][dj-1] + 1,\n",
    "                            fd[di-1][dj-1] + treedist[ii][jj]\n",
    "                        )\n",
    "\n",
    "    return treedist[n][m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff268db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_steds(gt_js: Dict[str,Any], pr_js: Dict[str,Any], exclude_meta: bool = True, match_mode: str = \"strict\") -> float:\n",
    "    gt_root, gt_n = build_tree_from_export(gt_js, exclude_meta=exclude_meta)\n",
    "    pr_root, pr_n = build_tree_from_export(pr_js, exclude_meta=exclude_meta)\n",
    "    dist = ted_zhang_shasha(gt_root, pr_root, match_mode=match_mode)\n",
    "    denom = max(gt_n, pr_n, 1)\n",
    "    return 1.0 - dist / denom\n",
    "\n",
    "def compute_aux_metrics(gt_js: Dict[str,Any], pr_js: Dict[str,Any], exclude_meta: bool = True) -> Dict[str,float]:\n",
    "    gt_nodes = gt_js[\"nodes\"]\n",
    "    pr_nodes = pr_js[\"nodes\"]\n",
    "    L = min(len(gt_nodes), len(pr_nodes))\n",
    "\n",
    "    mask = []\n",
    "    for i in range(L):\n",
    "        is_meta = bool(gt_nodes[i].get(\"is_meta\", False))\n",
    "        mask.append((not is_meta) if exclude_meta else True)\n",
    "\n",
    "    def acc(key: str) -> float:\n",
    "        tot = 0\n",
    "        hit = 0\n",
    "        for i in range(L):\n",
    "            if not mask[i]:\n",
    "                continue\n",
    "            tot += 1\n",
    "            hit += int(gt_nodes[i][key] == pr_nodes[i][key])\n",
    "        return hit / tot if tot else 0.0\n",
    "\n",
    "    # label_id / parent / rel_id\n",
    "    return {\n",
    "        \"cls_acc\": acc(\"label_id\"),\n",
    "        \"parent_acc\": acc(\"parent\"),\n",
    "        \"rel_acc\": acc(\"rel_id\"),\n",
    "        \"n_eval\": float(sum(mask)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def eval_export_dir(export_dir: str, exclude_meta: bool = True):\n",
    "    pairs = load_pair_files(export_dir)\n",
    "    steds_list = []\n",
    "    aux_list = []\n",
    "\n",
    "    for gt_path, pr_path in tqdm(pairs, desc=f\"eval {export_dir}\"):\n",
    "        with open(gt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            gt_js = json.load(f)\n",
    "        with open(pr_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            pr_js = json.load(f)\n",
    "\n",
    "        st = compute_steds(gt_js, pr_js, exclude_meta=exclude_meta)\n",
    "        aux = compute_aux_metrics(gt_js, pr_js, exclude_meta=exclude_meta)\n",
    "\n",
    "        steds_list.append(st)\n",
    "        aux_list.append(aux)\n",
    "\n",
    "    steds_arr = np.array(steds_list, dtype=float)\n",
    "    cls_acc = np.array([a[\"cls_acc\"] for a in aux_list], dtype=float)\n",
    "    par_acc = np.array([a[\"parent_acc\"] for a in aux_list], dtype=float)\n",
    "    rel_acc = np.array([a[\"rel_acc\"] for a in aux_list], dtype=float)\n",
    "\n",
    "    report = {\n",
    "        \"num_docs\": len(pairs),\n",
    "        \"exclude_meta\": exclude_meta,\n",
    "        \"STEDS_mean\": float(steds_arr.mean()) if len(steds_arr) else 0.0,\n",
    "        \"STEDS_std\": float(steds_arr.std()) if len(steds_arr) else 0.0,\n",
    "        \"STEDS_median\": float(np.median(steds_arr)) if len(steds_arr) else 0.0,\n",
    "        \"cls_acc_mean\": float(cls_acc.mean()) if len(cls_acc) else 0.0,\n",
    "        \"parent_acc_mean\": float(par_acc.mean()) if len(par_acc) else 0.0,\n",
    "        \"rel_acc_mean\": float(rel_acc.mean()) if len(rel_acc) else 0.0,\n",
    "    }\n",
    "    return report, steds_list, aux_list\n",
    "\n",
    "report, steds_list, aux_list = eval_export_dir(\"exports_hrdh_test\", exclude_meta=True)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f99a52",
   "metadata": {},
   "source": [
    "## 11. 文本规范化与阅读顺序（Reading Order）辅助\n",
    "\n",
    "- 文本归一化/匹配\n",
    "- 多列布局排序与文档级排序\n",
    "\n",
    "说明：原 Notebook 中此部分包含多段独立工具函数，均按原顺序保留。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b670de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re, numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- text normalization ----------\n",
    "def _norm_text(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = s.replace(\"\\n\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.replace(\"- \", \"-\")\n",
    "    return s\n",
    "\n",
    "\n",
    "# ---------- STEDS variants ----------\n",
    "def steds_original(gt_js, pr_js):\n",
    "    return compute_steds(gt_js, pr_js, exclude_meta=True)\n",
    "\n",
    "\n",
    "def steds_label_only(gt_js, pr_js):\n",
    "    gt_root, gt_n = build_tree_from_export(gt_js, exclude_meta=True)\n",
    "    pr_root, pr_n = build_tree_from_export(pr_js, exclude_meta=True)\n",
    "\n",
    "    def wipe_text(x):\n",
    "        x.text = \"\"\n",
    "        for c in x.children:\n",
    "            wipe_text(c)\n",
    "\n",
    "    wipe_text(gt_root)\n",
    "    wipe_text(pr_root)\n",
    "\n",
    "    dist = ted_zhang_shasha(gt_root, pr_root)\n",
    "    denom = max(gt_n, pr_n, 1)\n",
    "    return 1.0 - dist / denom\n",
    "\n",
    "\n",
    "def steds_text_norm(gt_js, pr_js):\n",
    "    gt_root, gt_n = build_tree_from_export(gt_js, exclude_meta=True)\n",
    "    pr_root, pr_n = build_tree_from_export(pr_js, exclude_meta=True)\n",
    "\n",
    "    def apply_norm(x):\n",
    "        x.text = _norm_text(x.text)\n",
    "        for c in x.children:\n",
    "            apply_norm(c)\n",
    "\n",
    "    apply_norm(gt_root)\n",
    "    apply_norm(pr_root)\n",
    "\n",
    "    dist = ted_zhang_shasha(gt_root, pr_root)\n",
    "    denom = max(gt_n, pr_n, 1)\n",
    "    return 1.0 - dist / denom\n",
    "\n",
    "\n",
    "# ---------- batch evaluation ----------\n",
    "def eval_steds_variants(export_dir: str):\n",
    "    pairs = load_pair_files(export_dir)\n",
    "\n",
    "    res = {\n",
    "        \"original\": [],\n",
    "        \"label_only\": [],\n",
    "        \"text_norm\": [],\n",
    "    }\n",
    "\n",
    "    for gt_path, pr_path in tqdm(pairs, desc=\"STEDS variants\"):\n",
    "        with open(gt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            gt = json.load(f)\n",
    "        with open(pr_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            pr = json.load(f)\n",
    "\n",
    "        res[\"original\"].append(steds_original(gt, pr))\n",
    "        res[\"label_only\"].append(steds_label_only(gt, pr))\n",
    "        res[\"text_norm\"].append(steds_text_norm(gt, pr))\n",
    "\n",
    "    def summary(arr):\n",
    "        arr = np.asarray(arr)\n",
    "        return {\n",
    "            \"mean\": float(arr.mean()),\n",
    "            \"median\": float(np.median(arr)),\n",
    "            \"std\": float(arr.std()),\n",
    "            \"min\": float(arr.min()),\n",
    "            \"max\": float(arr.max()),\n",
    "        }\n",
    "\n",
    "    report = {k: summary(v) for k, v in res.items()}\n",
    "    return report\n",
    "\n",
    "\n",
    "# ---------- run ----------\n",
    "report = eval_steds_variants(\"exports_hrdh_test\")\n",
    "report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d3eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kmeans_1d(x, k, iters=30):\n",
    "    \"\"\"简单 1D kmeans。x: (n,) float\"\"\"\n",
    "    x = x.astype(np.float32)\n",
    "    # init: 均匀取 k 个分位点\n",
    "    qs = np.linspace(0.1, 0.9, k)\n",
    "    centers = np.quantile(x, qs)\n",
    "    for _ in range(iters):\n",
    "        # assign\n",
    "        d = np.abs(x[:, None] - centers[None, :])\n",
    "        labels = d.argmin(axis=1)\n",
    "        new_centers = centers.copy()\n",
    "        for j in range(k):\n",
    "            mask = labels == j\n",
    "            if mask.any():\n",
    "                new_centers[j] = x[mask].mean()\n",
    "        if np.allclose(new_centers, centers, atol=1e-4):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    # inertia\n",
    "    inertia = ((x - centers[labels]) ** 2).sum()\n",
    "    # sort clusters by center (left->right)\n",
    "    order = np.argsort(centers)\n",
    "    remap = np.zeros_like(order)\n",
    "    remap[order] = np.arange(k)\n",
    "    labels = remap[labels]\n",
    "    centers = centers[order]\n",
    "    return labels, centers, inertia\n",
    "\n",
    "def choose_k_1d(x, k_min=1, k_max=3, min_cluster_size=5):\n",
    "    \"\"\"\n",
    "    自动选择列数 k：\n",
    "    - 基于“惯性下降幅度”（elbow-ish）\n",
    "    - 同时避免产生很小的列\n",
    "    \"\"\"\n",
    "    best = None\n",
    "    prev_inertia = None\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        labels, centers, inertia = kmeans_1d(x, k)\n",
    "        # 小簇过滤\n",
    "        ok = True\n",
    "        for j in range(k):\n",
    "            if (labels == j).sum() < min_cluster_size and len(x) >= min_cluster_size * k:\n",
    "                ok = False\n",
    "                break\n",
    "        if not ok:\n",
    "            continue\n",
    "\n",
    "        if prev_inertia is None:\n",
    "            score = 0.0\n",
    "        else:\n",
    "            # inertia 下降比例越大越好\n",
    "            score = (prev_inertia - inertia) / max(prev_inertia, 1e-6)\n",
    "\n",
    "        # 记录：优先更大“下降”，其次更小 inertia\n",
    "        cand = (score, -inertia, k, labels, centers, inertia)\n",
    "        if best is None or cand > best:\n",
    "            best = cand\n",
    "        prev_inertia = inertia\n",
    "\n",
    "    if best is None:\n",
    "        labels, centers, inertia = kmeans_1d(x, 1)\n",
    "        return 1, labels, centers\n",
    "    _, _, k, labels, centers, _ = best\n",
    "    return k, labels, centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a015ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_units_multi_column(units, page_w, page_h, max_cols=3):\n",
    "    \"\"\"\n",
    "    units: list of dict with 'box'=[x0,y0,x1,y1]\n",
    "    返回：按阅读顺序排序后的 index 列表\n",
    "    \"\"\"\n",
    "    n = len(units)\n",
    "    if n <= 1:\n",
    "        return list(range(n))\n",
    "\n",
    "    boxes = np.array([u[\"box\"] for u in units], dtype=np.float32)\n",
    "    x0, y0, x1, y1 = boxes[:,0], boxes[:,1], boxes[:,2], boxes[:,3]\n",
    "    xc = 0.5 * (x0 + x1)\n",
    "    yt = y0\n",
    "    bw = np.clip(x1 - x0, 1.0, None)\n",
    "\n",
    "    # 识别“跨栏大框”：宽度超过页面的一定比例（标题/大图/大表等），单独处理\n",
    "    wide = bw > (0.70 * page_w)\n",
    "\n",
    "    # 对非 wide 的做列聚类\n",
    "    idx_normal = np.where(~wide)[0]\n",
    "    idx_wide = np.where(wide)[0]\n",
    "\n",
    "    ordered = []\n",
    "\n",
    "    # 先把“宽大框”按 y 排到合适位置：通常它们是跨栏标题/大图，阅读上仍按 y 走\n",
    "    # 策略：把 wide 与 normal 一起排序时，wide 作为“单独列”插入：这里用更稳的两阶段合并\n",
    "    if len(idx_normal) > 0:\n",
    "        x_norm = xc[idx_normal]\n",
    "        k, labels, centers = choose_k_1d(x_norm, 1, max_cols, min_cluster_size=4)\n",
    "\n",
    "        # 每一列内部按 y_top 排序\n",
    "        cols = []\n",
    "        for c in range(k):\n",
    "            members = idx_normal[labels == c]\n",
    "            members = members[np.argsort(yt[members])]\n",
    "            cols.append(list(members))\n",
    "\n",
    "        # 列从左到右拼接\n",
    "        seq_normal = [i for col in cols for i in col]\n",
    "    else:\n",
    "        seq_normal = []\n",
    "\n",
    "    # wide 按 y_top 排序\n",
    "    seq_wide = list(idx_wide[np.argsort(yt[idx_wide])]) if len(idx_wide) > 0 else []\n",
    "\n",
    "    # 合并 wide 与 normal：按 y_top 进行稳定插入（wide 的 y 决定位置）\n",
    "    # 做法：对两序列按 y_top 归并，但当 y 接近时优先 wide（跨栏标题通常应先读）\n",
    "    def merge_by_y(a, b):\n",
    "        ia = ib = 0\n",
    "        out = []\n",
    "        while ia < len(a) and ib < len(b):\n",
    "            ya = yt[a[ia]]\n",
    "            yb = yt[b[ib]]\n",
    "            if yb <= ya + 2:  # 容忍 2px 抖动，wide 略优先\n",
    "                out.append(b[ib]); ib += 1\n",
    "            else:\n",
    "                out.append(a[ia]); ia += 1\n",
    "        out.extend(a[ia:])\n",
    "        out.extend(b[ib:])\n",
    "        return out\n",
    "\n",
    "    ordered = merge_by_y(seq_normal, seq_wide)\n",
    "    return ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649604bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_doc_units_by_reading_order(doc_units, page_images):\n",
    "    \"\"\"\n",
    "    doc_units: list[dict], each has page_id, box, text, ...\n",
    "    page_images: dict {page_id: image_path}\n",
    "    return sorted_units\n",
    "    \"\"\"\n",
    "    # 1) 按页收集\n",
    "    by_page = {}\n",
    "    for u in doc_units:\n",
    "        pid = int(u[\"page_id\"])\n",
    "        by_page.setdefault(pid, []).append(u)\n",
    "\n",
    "    # 2) 页按 pid 排序\n",
    "    sorted_units = []\n",
    "    for pid in sorted(by_page.keys()):\n",
    "        # 读 page 尺寸\n",
    "        from PIL import Image\n",
    "        img = Image.open(page_images[pid])\n",
    "        W, H = img.width, img.height\n",
    "\n",
    "        units_p = by_page[pid]\n",
    "        order_idx = order_units_multi_column(units_p, W, H, max_cols=3)\n",
    "        sorted_units.extend([units_p[i] for i in order_idx])\n",
    "\n",
    "    return sorted_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504e50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def _kmeans_1d(x, k, iters=30):\n",
    "    x = x.astype(np.float32)\n",
    "    qs = np.linspace(0.1, 0.9, k)\n",
    "    centers = np.quantile(x, qs)\n",
    "\n",
    "    for _ in range(iters):\n",
    "        d = np.abs(x[:, None] - centers[None, :])\n",
    "        labels = d.argmin(axis=1)\n",
    "        new_centers = centers.copy()\n",
    "        for j in range(k):\n",
    "            m = labels == j\n",
    "            if m.any():\n",
    "                new_centers[j] = x[m].mean()\n",
    "        if np.allclose(new_centers, centers, atol=1e-4):\n",
    "            break\n",
    "        centers = new_centers\n",
    "\n",
    "    inertia = ((x - centers[labels]) ** 2).sum()\n",
    "\n",
    "    order = np.argsort(centers)\n",
    "    remap = np.zeros_like(order)\n",
    "    remap[order] = np.arange(k)\n",
    "    labels = remap[labels]\n",
    "    centers = centers[order]\n",
    "    return labels, centers, inertia\n",
    "\n",
    "\n",
    "def _choose_k_1d(x, k_min=1, k_max=3, min_cluster_size=4):\n",
    "    best = None\n",
    "    prev_inertia = None\n",
    "\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        labels, centers, inertia = _kmeans_1d(x, k)\n",
    "        ok = True\n",
    "        if len(x) >= min_cluster_size * k:\n",
    "            for j in range(k):\n",
    "                if (labels == j).sum() < min_cluster_size:\n",
    "                    ok = False\n",
    "                    break\n",
    "        if not ok:\n",
    "            continue\n",
    "\n",
    "        score = 0.0 if prev_inertia is None else (prev_inertia - inertia) / max(prev_inertia, 1e-6)\n",
    "        cand = (score, -inertia, k, labels, centers)\n",
    "        if best is None or cand > best:\n",
    "            best = cand\n",
    "        prev_inertia = inertia\n",
    "\n",
    "    if best is None:\n",
    "        labels, centers, _ = _kmeans_1d(x, 1)\n",
    "        return 1, labels, centers\n",
    "    _, _, k, labels, centers = best\n",
    "    return k, labels, centers\n",
    "\n",
    "\n",
    "def order_units_multi_column(units, page_w, page_h, max_cols=3):\n",
    "    \"\"\"\n",
    "    units: list of dict with 'box'=[x0,y0,x1,y1]\n",
    "    return: list of indices in reading order\n",
    "    \"\"\"\n",
    "    n = len(units)\n",
    "    if n <= 1:\n",
    "        return list(range(n))\n",
    "\n",
    "    boxes = np.array([u[\"box\"] for u in units], dtype=np.float32)\n",
    "    x0, y0, x1, y1 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    xc = 0.5 * (x0 + x1)\n",
    "    yt = y0\n",
    "    bw = np.clip(x1 - x0, 1.0, None)\n",
    "\n",
    "    # “跨栏大框”识别：标题/大图/大表常见\n",
    "    wide = bw > (0.70 * page_w)\n",
    "\n",
    "    idx_normal = np.where(~wide)[0]\n",
    "    idx_wide = np.where(wide)[0]\n",
    "\n",
    "    seq_normal = []\n",
    "    if len(idx_normal) > 0:\n",
    "        x_norm = xc[idx_normal]\n",
    "        k, labels, _ = _choose_k_1d(x_norm, 1, max_cols, min_cluster_size=4)\n",
    "\n",
    "        cols = []\n",
    "        for c in range(k):\n",
    "            members = idx_normal[labels == c]\n",
    "            members = members[np.argsort(yt[members])]\n",
    "            cols.append(list(members))\n",
    "\n",
    "        seq_normal = [i for col in cols for i in col]\n",
    "\n",
    "    seq_wide = list(idx_wide[np.argsort(yt[idx_wide])]) if len(idx_wide) > 0 else []\n",
    "\n",
    "    # 按 y 归并：wide 在接近同一 y 时略优先（跨栏标题通常应先读）\n",
    "    def merge_by_y(a, b):\n",
    "        ia = ib = 0\n",
    "        out = []\n",
    "        while ia < len(a) and ib < len(b):\n",
    "            ya = yt[a[ia]]\n",
    "            yb = yt[b[ib]]\n",
    "            if yb <= ya + 2:\n",
    "                out.append(b[ib]); ib += 1\n",
    "            else:\n",
    "                out.append(a[ia]); ia += 1\n",
    "        out.extend(a[ia:])\n",
    "        out.extend(b[ib:])\n",
    "        return out\n",
    "\n",
    "    return merge_by_y(seq_normal, seq_wide)\n",
    "\n",
    "\n",
    "def sort_doc_units_by_reading_order(doc_units, page_images, max_cols=3):\n",
    "    \"\"\"\n",
    "    doc_units: list[dict] each has 'page_id','box',...\n",
    "    page_images: dict[int -> image_path]\n",
    "    return: new_units_sorted\n",
    "    \"\"\"\n",
    "    by_page = {}\n",
    "    for u in doc_units:\n",
    "        pid = int(u[\"page_id\"])\n",
    "        by_page.setdefault(pid, []).append(u)\n",
    "\n",
    "    sorted_units = []\n",
    "    for pid in sorted(by_page.keys()):\n",
    "        img = Image.open(page_images[pid])\n",
    "        W, H = img.width, img.height\n",
    "\n",
    "        units_p = by_page[pid]\n",
    "        order_idx = order_units_multi_column(units_p, W, H, max_cols=max_cols)\n",
    "        sorted_units.extend([units_p[i] for i in order_idx])\n",
    "\n",
    "    return sorted_units\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a08b68",
   "metadata": {},
   "source": [
    "## 12. 诊断、对比评估与实验封装\n",
    "\n",
    "- check_causal_violation\n",
    "- eval_export_dir_dual\n",
    "- run_experiment：实验跑批封装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d80eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_causal_violation(dataset, n=100):\n",
    "    cnt = 0\n",
    "    viol = 0\n",
    "    for i in range(min(n, len(dataset))):\n",
    "        d = dataset[i]\n",
    "        yp = d[\"y_parent\"]\n",
    "        for j,p in enumerate(yp):\n",
    "            if p is not None and p >= j:\n",
    "                viol += 1\n",
    "            cnt += 1\n",
    "    return viol / max(cnt,1)\n",
    "\n",
    "print(\"causal violation rate:\", check_causal_violation(train_ds, n=100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180eb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json, os, glob\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_export_dir_dual(export_dir: str, exclude_meta: bool = True,show_progress: bool = True):\n",
    "    pairs = load_pair_files(export_dir)\n",
    "\n",
    "    st_strict, st_label = [], []\n",
    "    cls_acc, par_acc, rel_acc = [], [], []\n",
    "\n",
    "    for gt_path, pr_path in pairs:\n",
    "        with open(gt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            gt_js = json.load(f)\n",
    "        with open(pr_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            pr_js = json.load(f)\n",
    "\n",
    "        st_strict.append(compute_steds(gt_js, pr_js, exclude_meta=exclude_meta, match_mode=\"strict\"))\n",
    "        st_label.append(compute_steds(gt_js, pr_js, exclude_meta=exclude_meta, match_mode=\"label\"))\n",
    "\n",
    "        aux = compute_aux_metrics(gt_js, pr_js, exclude_meta=exclude_meta)\n",
    "        cls_acc.append(aux[\"cls_acc\"])\n",
    "        par_acc.append(aux[\"parent_acc\"])\n",
    "        rel_acc.append(aux[\"rel_acc\"])\n",
    "\n",
    "    def agg(x):\n",
    "        x = np.array(x, dtype=float)\n",
    "        return float(x.mean()), float(x.std()), float(np.median(x))\n",
    "\n",
    "    s_mean, s_std, s_med = agg(st_strict)\n",
    "    l_mean, l_std, l_med = agg(st_label)\n",
    "\n",
    "    return {\n",
    "        \"num_docs\": len(pairs),\n",
    "        \"STEDS_strict_mean\": s_mean,\n",
    "        \"STEDS_strict_std\": s_std,\n",
    "        \"STEDS_strict_median\": s_med,\n",
    "        \"STEDS_label_mean\": l_mean,\n",
    "        \"STEDS_label_std\": l_std,\n",
    "        \"STEDS_label_median\": l_med,\n",
    "        \"cls_acc_mean\": float(np.mean(cls_acc)) if cls_acc else 0.0,\n",
    "        \"parent_acc_mean\": float(np.mean(par_acc)) if par_acc else 0.0,\n",
    "        \"rel_acc_mean\": float(np.mean(rel_acc)) if rel_acc else 0.0,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3052e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def run_experiment(exp_name: str, use_softmask: bool, train_loader, test_loader, M_cp, cfg, seed=42, save_root=\"ablation_runs\"):\n",
    "    # 固定随机种子\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    exp_dir = os.path.join(save_root, exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "    # 建模：只改变 use_softmask\n",
    "    model = DSPSModel(\n",
    "        num_classes=len(ID2LABEL_14),\n",
    "        num_rel=len(REL2ID),\n",
    "        M_cp=M_cp,\n",
    "        cfg=cfg,\n",
    "        use_text=True,\n",
    "        use_visual= False,\n",
    "        use_softmask=use_softmask,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    focal_cls = FocalLoss(gamma=cfg.focal_gamma, alpha=cfg.focal_alpha).to(device)\n",
    "    focal_rel = FocalLoss(gamma=cfg.focal_gamma, alpha=cfg.focal_alpha).to(device)\n",
    "\n",
    "    best = 1e18\n",
    "    best_path = os.path.join(exp_dir, \"best.pt\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    for ep in range(1, cfg.epochs + 1):\n",
    "        tr = train_one_epoch(model, train_loader, optimizer, cfg, focal_cls, focal_rel)\n",
    "        te = eval_one_epoch(model, test_loader, cfg, focal_cls, focal_rel)\n",
    "        print(f\"[{exp_name}] ep={ep} train={tr} test={te}\")\n",
    "\n",
    "        if te[\"loss\"] < best:\n",
    "            best = te[\"loss\"]\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"[{exp_name}] saved -> {best_path}\")\n",
    "\n",
    "    # 导出与评测\n",
    "    state = torch.load(best_path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(state)\n",
    "\n",
    "    export_dir = os.path.join(exp_dir, \"exports_test\")\n",
    "    export_split_predictions(model, test_loader, save_dir=export_dir, device=device)\n",
    "\n",
    "    print(f\"[{exp_name}] export done, start eval...\")\n",
    "    metrics = eval_export_dir_dual(export_dir, exclude_meta=True, show_progress=True)\n",
    "    metrics.update({\n",
    "        \"exp_name\": exp_name,\n",
    "        \"use_softmask\": use_softmask,\n",
    "        \"seed\": seed,\n",
    "        \"epochs\": cfg.epochs,\n",
    "        \"elapsed_sec\": round(time.time() - t0, 2),\n",
    "        \"export_dir\": export_dir,\n",
    "    })\n",
    "    print(f\"[{exp_name}] eval done.\")\n",
    "\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ======= 跑两组：soft-mask on/off =======\n",
    "os.makedirs(\"ablation_runs\", exist_ok=True)\n",
    "\n",
    "res_on = run_experiment(\n",
    "    exp_name=\"baseline_softmask_on\",\n",
    "    use_softmask=True,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    M_cp=M_cp,\n",
    "    cfg=cfg,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "res_off = run_experiment(\n",
    "    exp_name=\"ablation_softmask_off\",\n",
    "    use_softmask=False,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    M_cp=M_cp,\n",
    "    cfg=cfg,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "for r in [res_on, res_off]:\n",
    "    print(\"=\" * 60)\n",
    "    for k, v in r.items():\n",
    "        print(f\"{k:25s}: {v}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432f7c6-59ae-48c8-9d6b-18912b959dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "cfg.batch_size = 1\n",
    "\n",
    "def _call_with_compatible_signature(fn, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Call fn with kwargs filtered to those supported by fn's signature.\n",
    "    This prevents errors like: got an unexpected keyword argument 'device'.\n",
    "    \"\"\"\n",
    "    sig = inspect.signature(fn)\n",
    "    accepted = set(sig.parameters.keys())\n",
    "    filtered = {k: v for k, v in kwargs.items() if k in accepted}\n",
    "    return fn(*args, **filtered)\n",
    "\n",
    "def train_one_epoch_compat(model, loader, optimizer, *, device=None, cfg=None, scaler=None):\n",
    "        if focal_cls is None:\n",
    "            focal_cls = FocalLoss(gamma=getattr(cfg, \"focal_gamma\", 2.0),\n",
    "                                  alpha=getattr(cfg, \"focal_alpha\", 0.25)).to(device)\n",
    "        if focal_rel is None:\n",
    "            focal_rel = FocalLoss(gamma=getattr(cfg, \"focal_gamma\", 2.0),\n",
    "                                  alpha=getattr(cfg, \"focal_alpha\", 0.25)).to(device)\n",
    "    # Try calling with keywords first; incompatible keywords are filtered out.\n",
    "        return _call_with_compatible_signature(\n",
    "        train_one_epoch,\n",
    "        model, loader, optimizer,\n",
    "        device=device, cfg=cfg, scaler=scaler\n",
    "    )\n",
    "\n",
    "def eval_one_epoch_compat(model, loader, *, device=None, cfg=None):\n",
    "    return _call_with_compatible_signature(\n",
    "        eval_one_epoch,\n",
    "        model, loader,\n",
    "        device=device, cfg=cfg\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c2d17-3b29-411d-8b67-d3697c48d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# HRDS 全流程：train -> best ckpt -> export -> eval\n",
    "# 依赖你 notebook 已有定义（Run All 到此处后再运行本 cell）：\n",
    "#   HRDHDataset, collate_doc\n",
    "#   compute_M_cp_from_dataset\n",
    "#   DSPSModel\n",
    "#   FocalLoss\n",
    "#   train_one_epoch\n",
    "#   export_split_predictions\n",
    "#   eval_export_dir_dual\n",
    "# =========================\n",
    "\n",
    "import os, json, time, shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --------- A) 你必须改的路径：HRDS_ROOT ----------\n",
    "HRDS_ROOT = r\"C:\\Users\\tomra\\Desktop\\PAPER\\final OBJ\\HRDS\"   # 改成你 HRDoc-Simple 根目录\n",
    "\n",
    "# --------- B) 实验输出目录 ----------\n",
    "EXP_NAME = \"hrds_dsps_fullrun\"\n",
    "RUN_ROOT = \"hrds_runs\"\n",
    "EXP_DIR = os.path.join(RUN_ROOT, EXP_NAME)\n",
    "os.makedirs(EXP_DIR, exist_ok=True)\n",
    "\n",
    "# --------- C) 基础对象检查：确保你前面 cell 已经定义完 ----------\n",
    "required = [\n",
    "    \"HRDHDataset\", \"collate_doc\",\n",
    "    \"compute_M_cp_from_dataset\",\n",
    "    \"DSPSModel\",\n",
    "    \"FocalLoss\",\n",
    "    \"train_one_epoch\",\n",
    "    \"export_split_predictions\",\n",
    "    \"eval_export_dir_dual\",\n",
    "    \"cfg\",\n",
    "]\n",
    "missing = [n for n in required if n not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        \"你还没有运行到包含以下定义的 cell（或变量名不同）：\\n\"\n",
    "        + \"\\n\".join(missing)\n",
    "        + \"\\n\\n请先 Run All 直到模型/数据集/评估函数都定义完，再运行本 cell。\"\n",
    "    )\n",
    "\n",
    "if not os.path.isdir(HRDS_ROOT):\n",
    "    raise FileNotFoundError(f\"HRDS_ROOT 不存在：{HRDS_ROOT}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[Device]\", device)\n",
    "print(\"[EXP_DIR]\", EXP_DIR)\n",
    "\n",
    "# --------- D) 固定随机种子（保持复现稳定） ----------\n",
    "seed = 42\n",
    "import random\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# --------- E) 构建 HRDS 数据集与 dataloader ----------\n",
    "train_ds = HRDHDataset(HRDS_ROOT, split=\"train\", max_len=cfg.max_len)\n",
    "test_ds  = HRDHDataset(HRDS_ROOT, split=\"test\",  max_len=cfg.max_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=1,         \n",
    "    shuffle=True,\n",
    "    num_workers=getattr(cfg, \"num_workers\", 0),\n",
    "    collate_fn=collate_doc,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=getattr(cfg, \"num_workers\", 0),\n",
    "    collate_fn=collate_doc,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "print(f\"[HRDS] train docs = {len(train_loader)}, test docs = {len(test_loader)}\")\n",
    "\n",
    "# --------- F) 类别/关系维度（来自你 dataset 定义） ----------\n",
    "# 你的 dataset cell 里固定 14 类，关系含 meta（4 类）\n",
    "num_classes = len(ID2LABEL_14)\n",
    "num_rel = len(REL2ID)\n",
    "print(\"[Dims] num_classes =\", num_classes, \"num_rel =\", num_rel)\n",
    "\n",
    "# --------- G) 计算/加载 M_cp（soft-mask 先验） ----------\n",
    "MCP_PATH = os.path.join(EXP_DIR, \"M_cp_hrds.npy\")\n",
    "if os.path.exists(MCP_PATH):\n",
    "    M_cp = np.load(MCP_PATH)\n",
    "    print(\"[M_cp] loaded:\", MCP_PATH, \"shape=\", M_cp.shape)\n",
    "else:\n",
    "    print(\"[M_cp] computing from HRDS train ...\")\n",
    "    ret = compute_M_cp_from_dataset(train_ds, num_classes=num_classes, pseudo_count=5.0)\n",
    "\n",
    "    # 兼容：函数可能返回 (M_cp, stats) 或 {\"M_cp\":..., ...} 或直接 ndarray\n",
    "    if isinstance(ret, tuple):\n",
    "        M_cp = ret[0]\n",
    "        mcp_extra = ret[1:]\n",
    "    elif isinstance(ret, dict):\n",
    "        M_cp = ret.get(\"M_cp\", None)\n",
    "        mcp_extra = {k:v for k,v in ret.items() if k != \"M_cp\"}\n",
    "        if M_cp is None:\n",
    "            raise RuntimeError(\"compute_M_cp_from_dataset 返回 dict，但不包含键 'M_cp'\")\n",
    "    else:\n",
    "        M_cp = ret\n",
    "        mcp_extra = None\n",
    "\n",
    "    M_cp = np.asarray(M_cp)\n",
    "    np.save(MCP_PATH, M_cp)\n",
    "    print(\"[M_cp] saved:\", MCP_PATH, \"shape=\", M_cp.shape)\n",
    "\n",
    "    # 可选：把额外统计信息也存下来（不影响训练）\n",
    "    if mcp_extra is not None:\n",
    "        extra_path = os.path.join(EXP_DIR, \"M_cp_hrds_extra.json\")\n",
    "        try:\n",
    "            with open(extra_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(mcp_extra, f, ensure_ascii=False, indent=2, default=str)\n",
    "            print(\"[M_cp] extra saved:\", extra_path)\n",
    "        except Exception as e:\n",
    "            print(\"[M_cp] extra save skipped:\", repr(e))\n",
    "\n",
    "# --------- H) 建模（按你现有 DSPSModel 接口） ----------\n",
    "# 这里默认用你当前复现的设置：use_text=False；视觉是否启用取决于你 notebook 中 DSPSModel 的实现\n",
    "# 若你后续要开视觉或文本，改下面两个开关即可\n",
    "USE_TEXT = False\n",
    "USE_VISUAL = False\n",
    "USE_SOFTMASK = True\n",
    "\n",
    "model = DSPSModel(\n",
    "    num_classes=num_classes,\n",
    "    num_rel=num_rel,\n",
    "    M_cp=M_cp,\n",
    "    cfg=cfg,\n",
    "    use_text=USE_TEXT,\n",
    "    use_visual=USE_VISUAL,\n",
    "    use_softmask=USE_SOFTMASK,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=getattr(cfg, \"lr\", 2e-4),\n",
    "    weight_decay=getattr(cfg, \"weight_decay\", 1e-2),\n",
    ")\n",
    "\n",
    "focal_cls = FocalLoss(gamma=getattr(cfg, \"focal_gamma\", 2.0), alpha=getattr(cfg, \"focal_alpha\", 0.25)).to(device)\n",
    "focal_rel = FocalLoss(gamma=getattr(cfg, \"focal_gamma\", 2.0), alpha=getattr(cfg, \"focal_alpha\", 0.25)).to(device)\n",
    "\n",
    "# --------- I) 训练 + 每 epoch 验证 + 保存 best ----------\n",
    "CKPT_BEST = os.path.join(EXP_DIR, \"best_hrds.pt\")\n",
    "REPORT_PATH = os.path.join(EXP_DIR, \"train_eval_log.json\")\n",
    "\n",
    "history = []\n",
    "best_score = -1.0\n",
    "best_epoch = -1\n",
    "\n",
    "def _extract_macro_strict(metrics_dict):\n",
    "    \"\"\"\n",
    "    兼容 eval_export_dir_dual 的返回格式：\n",
    "    - 若存在宏严格 STEDS 的 key，就取它\n",
    "    - 否则退化为 strict_mean 或第一项可用 strict 指标\n",
    "    \"\"\"\n",
    "    if not isinstance(metrics_dict, dict):\n",
    "        return None\n",
    "\n",
    "    # 常见候选 key（根据你实现的命名习惯做容错）\n",
    "    candidates = [\n",
    "        \"steds_strict_macro\", \"macro_steds_strict\", \"steds_macro_strict\",\n",
    "        \"steds_strict_mean\",  \"strict_mean\",        \"steds_strict\",\n",
    "    ]\n",
    "    for k in candidates:\n",
    "        if k in metrics_dict and isinstance(metrics_dict[k], (int, float)):\n",
    "            return float(metrics_dict[k])\n",
    "\n",
    "    # 兜底：如果 metrics_dict 里有任何包含 \"strict\" 的数值字段，取第一个\n",
    "    for k, v in metrics_dict.items():\n",
    "        if \"strict\" in k.lower() and isinstance(v, (int, float)):\n",
    "            return float(v)\n",
    "\n",
    "    return None\n",
    "\n",
    "for ep in range(1, cfg.epochs + 1):\n",
    "    print(f\"\\n========== [HRDS] Epoch {ep}/{cfg.epochs} ==========\")\n",
    "\n",
    "    # 1) train\n",
    "    tr_logs = train_one_epoch(model, train_loader, optimizer, cfg, focal_cls, focal_rel)\n",
    "\n",
    "    # 2) export + eval on test\n",
    "    export_dir = os.path.join(EXP_DIR, f\"export_ep{ep:02d}\")\n",
    "    # 为避免旧结果干扰，每次先清目录\n",
    "    if os.path.isdir(export_dir):\n",
    "        shutil.rmtree(export_dir, ignore_errors=True)\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "    t0 = time.time()\n",
    "    export_split_predictions(model, test_loader, save_dir=export_dir, device=device)\n",
    "    metrics = eval_export_dir_dual(export_dir, exclude_meta=True)\n",
    "    eval_time = time.time() - t0\n",
    "\n",
    "    # 3) best selection（以 Macro Strict STEDS 为主）\n",
    "    score = _extract_macro_strict(metrics)\n",
    "    if score is None:\n",
    "        print(\"[WARN] 无法从 metrics 中提取 strict 指标字段，将不保存 best。metrics=\", metrics)\n",
    "        score = -1.0\n",
    "\n",
    "    is_best = score > best_score\n",
    "    if is_best:\n",
    "        best_score = score\n",
    "        best_epoch = ep\n",
    "        torch.save({\"model\": model.state_dict(), \"epoch\": ep, \"metrics\": metrics}, CKPT_BEST)\n",
    "        print(f\"[BEST] updated: epoch={ep}, score={best_score:.6f} -> {CKPT_BEST}\")\n",
    "\n",
    "    row = {\n",
    "        \"epoch\": ep,\n",
    "        \"train\": tr_logs,\n",
    "        \"metrics\": metrics,\n",
    "        \"macro_strict_for_select\": score,\n",
    "        \"eval_time_sec\": eval_time,\n",
    "        \"is_best\": is_best,\n",
    "    }\n",
    "    history.append(row)\n",
    "\n",
    "    with open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"exp_name\": EXP_NAME,\n",
    "                \"hrds_root\": HRDS_ROOT,\n",
    "                \"cfg\": cfg.__dict__ if hasattr(cfg, \"__dict__\") else str(cfg),\n",
    "                \"use_text\": USE_TEXT,\n",
    "                \"use_visual\": USE_VISUAL,\n",
    "                \"use_softmask\": USE_SOFTMASK,\n",
    "                \"best_epoch\": best_epoch,\n",
    "                \"best_score_macro_strict\": best_score,\n",
    "                \"best_ckpt\": CKPT_BEST,\n",
    "                \"history\": history,\n",
    "            },\n",
    "            f,\n",
    "            ensure_ascii=False,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "    print(\"[Epoch Summary]\")\n",
    "    print(\"  train loss(last):\", tr_logs[\"loss\"][-1] if isinstance(tr_logs, dict) and \"loss\" in tr_logs else tr_logs)\n",
    "    print(\"  metrics:\", metrics)\n",
    "    print(\"  eval_time_sec:\", eval_time)\n",
    "\n",
    "print(\"\\n========== HRDS Full Flow DONE ==========\")\n",
    "print(\"[BEST] epoch =\", best_epoch, \"macro_strict =\", best_score)\n",
    "print(\"[BEST CKPT]\", CKPT_BEST)\n",
    "print(\"[LOG JSON ]\", REPORT_PATH)\n",
    "\n",
    "print(\"\\nPaper reference (Table 2 HRDS best, Document+Semantic+Vision+Soft-mask):\")\n",
    "print(\"Micro-STEDS ≈ 0.8143\")\n",
    "print(\"Macro-STEDS ≈ 0.8174\")\n",
    "\n",
    "# --------- J) 用 best ckpt 再导出一次（最终留档） ----------\n",
    "FINAL_EXPORT = os.path.join(EXP_DIR, \"export_best_final\")\n",
    "if os.path.isdir(FINAL_EXPORT):\n",
    "    shutil.rmtree(FINAL_EXPORT, ignore_errors=True)\n",
    "os.makedirs(FINAL_EXPORT, exist_ok=True)\n",
    "\n",
    "if os.path.exists(CKPT_BEST):\n",
    "    ckpt = torch.load(CKPT_BEST, map_location=\"cpu\")\n",
    "    model.load_state_dict(ckpt[\"model\"], strict=False)\n",
    "    model.eval()\n",
    "\n",
    "export_split_predictions(model, test_loader, save_dir=FINAL_EXPORT, device=device)\n",
    "final_metrics = eval_export_dir_dual(FINAL_EXPORT, exclude_meta=True)\n",
    "\n",
    "FINAL_REPORT = os.path.join(\"final_reports\", f\"{EXP_NAME}_HRDS_report.json\")\n",
    "os.makedirs(\"final_reports\", exist_ok=True)\n",
    "with open(FINAL_REPORT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"dataset\": \"HRDoc-Simple (HRDS)\",\n",
    "            \"hrds_root\": HRDS_ROOT,\n",
    "            \"best_ckpt\": CKPT_BEST,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"best_score_macro_strict\": best_score,\n",
    "            \"final_export_dir\": FINAL_EXPORT,\n",
    "            \"final_metrics\": final_metrics,\n",
    "            \"log_path\": REPORT_PATH,\n",
    "        },\n",
    "        f,\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "print(\"\\n===== HRDS Final Validation =====\")\n",
    "print(\"Final export :\", FINAL_EXPORT)\n",
    "print(\"Final report :\", FINAL_REPORT)\n",
    "print(\"Final metrics:\", final_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0013b4-ff70-411f-b615-d85d0b9c53c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# HRDS 全流程：train -> best ckpt -> export -> eval\n",
    "# 依赖你 notebook 已有定义（Run All 到此处后再运行本 cell）：\n",
    "#   HRDHDataset, collate_doc\n",
    "#   compute_M_cp_from_dataset\n",
    "#   DSPSModel\n",
    "#   FocalLoss\n",
    "#   train_one_epoch\n",
    "#   export_split_predictions\n",
    "#   eval_export_dir_dual\n",
    "# =========================\n",
    "\n",
    "import os, json, time, shutil, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --------- A) 你必须改的路径：HRDS_ROOT ----------\n",
    "HRDS_ROOT = r\"C:\\Users\\tomra\\Desktop\\PAPER\\final OBJ\\HRDS\"   # 改成你 HRDoc-Simple 根目录\n",
    "\n",
    "# --------- B) 实验输出目录 ----------\n",
    "EXP_NAME = \"hrds_dsps_fullrun\"\n",
    "RUN_ROOT = \"hrds_runs\"\n",
    "EXP_DIR = os.path.join(RUN_ROOT, EXP_NAME)\n",
    "os.makedirs(EXP_DIR, exist_ok=True)\n",
    "\n",
    "# --------- C) 基础对象检查：确保你前面 cell 已经定义完 ----------\n",
    "required = [\n",
    "    \"HRDHDataset\", \"collate_doc\",\n",
    "    \"compute_M_cp_from_dataset\",\n",
    "    \"DSPSModel\",\n",
    "    \"FocalLoss\",\n",
    "    \"train_one_epoch\",\n",
    "    \"export_split_predictions\",\n",
    "    \"eval_export_dir_dual\",\n",
    "    \"cfg\",\n",
    "]\n",
    "missing = [n for n in required if n not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        \"你还没有运行到包含以下定义的 cell（或变量名不同）：\\n\"\n",
    "        + \"\\n\".join(missing)\n",
    "        + \"\\n\\n请先 Run All 直到模型/数据集/评估函数都定义完，再运行本 cell。\"\n",
    "    )\n",
    "\n",
    "if not os.path.isdir(HRDS_ROOT):\n",
    "    raise FileNotFoundError(f\"HRDS_ROOT 不存在：{HRDS_ROOT}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[Device]\", device)\n",
    "print(\"[EXP_DIR]\", EXP_DIR)\n",
    "\n",
    "# --------- D) 固定随机种子（保持复现稳定） ----------\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# --------- E) 构建 HRDS 数据集与 dataloader ----------\n",
    "train_ds = HRDHDataset(HRDS_ROOT, split=\"train\", max_len=cfg.max_len)\n",
    "test_ds  = HRDHDataset(HRDS_ROOT, split=\"test\",  max_len=cfg.max_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=1,         \n",
    "    shuffle=True,\n",
    "    num_workers=getattr(cfg, \"num_workers\", 0),\n",
    "    collate_fn=collate_doc,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=getattr(cfg, \"num_workers\", 0),\n",
    "    collate_fn=collate_doc,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "print(f\"[HRDS] train docs = {len(train_loader)}, test docs = {len(test_loader)}\")\n",
    "\n",
    "# --------- F) 类别/关系维度（来自你 dataset 定义） ----------\n",
    "num_classes = len(ID2LABEL_14)\n",
    "num_rel = len(REL2ID)\n",
    "print(\"[Dims] num_classes =\", num_classes, \"num_rel =\", num_rel)\n",
    "\n",
    "# --------- G) 计算/加载 M_cp（soft-mask 先验） ----------\n",
    "MCP_PATH = os.path.join(EXP_DIR, \"M_cp_hrds.npy\")\n",
    "if os.path.exists(MCP_PATH):\n",
    "    M_cp = np.asarray(np.load(MCP_PATH))\n",
    "    print(\"[M_cp] loaded:\", MCP_PATH, \"shape=\", M_cp.shape)\n",
    "else:\n",
    "    print(\"[M_cp] computing from HRDS train ...\")\n",
    "    ret = compute_M_cp_from_dataset(train_ds, num_classes=num_classes, pseudo_count=5.0)\n",
    "\n",
    "    # 兼容：函数可能返回 (M_cp, stats) 或 {\"M_cp\":..., ...} 或直接 ndarray\n",
    "    if isinstance(ret, tuple):\n",
    "        M_cp = ret[0]\n",
    "        mcp_extra = ret[1:]\n",
    "    elif isinstance(ret, dict):\n",
    "        M_cp = ret.get(\"M_cp\", None)\n",
    "        mcp_extra = {k:v for k,v in ret.items() if k != \"M_cp\"}\n",
    "        if M_cp is None:\n",
    "            raise RuntimeError(\"compute_M_cp_from_dataset 返回 dict，但不包含键 'M_cp'\")\n",
    "    else:\n",
    "        M_cp = ret\n",
    "        mcp_extra = None\n",
    "\n",
    "    M_cp = np.asarray(M_cp)\n",
    "    np.save(MCP_PATH, M_cp)\n",
    "    print(\"[M_cp] saved:\", MCP_PATH, \"shape=\", M_cp.shape)\n",
    "\n",
    "    # 可选：把额外统计信息也存下来（不影响训练）\n",
    "    if mcp_extra is not None:\n",
    "        extra_path = os.path.join(EXP_DIR, \"M_cp_hrds_extra.json\")\n",
    "        try:\n",
    "            with open(extra_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(mcp_extra, f, ensure_ascii=False, indent=2, default=str)\n",
    "            print(\"[M_cp] extra saved:\", extra_path)\n",
    "        except Exception as e:\n",
    "            print(\"[M_cp] extra save skipped:\", repr(e))\n",
    "\n",
    "# --------- H) 建模 ----------\n",
    "USE_TEXT = False\n",
    "USE_VISUAL = False\n",
    "USE_SOFTMASK = True\n",
    "\n",
    "model = DSPSModel(\n",
    "    num_classes=num_classes,\n",
    "    num_rel=num_rel,\n",
    "    M_cp=M_cp,\n",
    "    cfg=cfg,\n",
    "    use_text=USE_TEXT,\n",
    "    use_visual=USE_VISUAL,\n",
    "    use_softmask=USE_SOFTMASK,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=getattr(cfg, \"lr\", 2e-4),\n",
    "    weight_decay=getattr(cfg, \"weight_decay\", 1e-2),\n",
    ")\n",
    "\n",
    "focal_cls = FocalLoss(gamma=getattr(cfg, \"focal_gamma\", 2.0), alpha=getattr(cfg, \"focal_alpha\", 0.25)).to(device)\n",
    "focal_rel = FocalLoss(gamma=getattr(cfg, \"focal_gamma\", 2.0), alpha=getattr(cfg, \"focal_alpha\", 0.25)).to(device)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# [关键整合修正] 安全推理函数：保证 export_tree_json 不再炸\n",
    "# ---------------------------------------------------------\n",
    "def _install_safe_predict_doc_with_rel_recompute():\n",
    "    import torch\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _safe(model, doc, device):\n",
    "        out = model(doc)\n",
    "\n",
    "        # 必需：cls_logits / par_logits\n",
    "        cls_logits = out[\"cls_logits\"]   # (L,C)\n",
    "        par_logits = out[\"par_logits\"]   # list-like\n",
    "        L = int(cls_logits.size(0))\n",
    "\n",
    "        pred_cls = torch.argmax(cls_logits, dim=-1).detach().cpu().tolist()\n",
    "\n",
    "        # parents: argmax + clamp\n",
    "        pred_parent = [-1] * L\n",
    "        for i in range(L):\n",
    "            if i == 0:\n",
    "                pred_parent[i] = -1\n",
    "                continue\n",
    "            logits_i = par_logits[i].view(-1)\n",
    "            p = int(torch.argmax(logits_i).item())\n",
    "            pred_parent[i] = p\n",
    "\n",
    "        # clamp parents 防越界/自指/指向未来\n",
    "        for i in range(L):\n",
    "            p = pred_parent[i]\n",
    "            if (p is None) or (not isinstance(p, int)) or (p < 0) or (p >= L) or (p == i) or (p >= i):\n",
    "                pred_parent[i] = -1\n",
    "\n",
    "        # rel logits：优先重算；若找不到 hidden seq 则回退 out[\"rel_logits\"]；否则全零\n",
    "        rel_logits = out.get(\"rel_logits\", None)\n",
    "\n",
    "        # 尝试找 hidden sequence (L,D) —— 不硬编码键名，按形状挑一个\n",
    "        h_seq = None\n",
    "        for k, v in out.items():\n",
    "            if torch.is_tensor(v) and v.dim() == 2 and v.size(0) == L:\n",
    "                if k in (\"cls_logits\", \"rel_logits\"):\n",
    "                    continue\n",
    "                h_seq = v\n",
    "                break\n",
    "\n",
    "        root = out.get(\"root\", None)\n",
    "        if root is None and h_seq is not None:\n",
    "            root = h_seq[0:1]  # (1,D)\n",
    "\n",
    "        if (h_seq is not None) and hasattr(model, \"rel_head\"):\n",
    "            root_vec = root.squeeze(0) if (torch.is_tensor(root) and root.dim() == 2) else root\n",
    "            rel_list = []\n",
    "            for i in range(L):\n",
    "                p = pred_parent[i]\n",
    "                parent_vec = root_vec if (p < 0 or p >= L) else h_seq[p]\n",
    "                feat = torch.cat([h_seq[i], parent_vec], dim=-1)\n",
    "                rel_list.append(model.rel_head(feat))\n",
    "            rel_logits = torch.stack(rel_list, dim=0)\n",
    "\n",
    "        if rel_logits is None:\n",
    "            R = getattr(model, \"num_rel\", num_rel)\n",
    "            rel_logits = torch.zeros((L, R), device=cls_logits.device)\n",
    "\n",
    "        pred_rel = torch.argmax(rel_logits, dim=-1).detach().cpu().tolist()\n",
    "\n",
    "        # 关键：键名必须匹配 export_tree_json\n",
    "        return {\"pred_cls\": pred_cls, \"pred_parent\": pred_parent, \"pred_rel\": pred_rel}\n",
    "\n",
    "    # 覆写全局函数名，供 export_split_predictions 调用\n",
    "    globals()[\"predict_doc_with_rel_recompute\"] = _safe\n",
    "\n",
    "\n",
    "# --------- I) 训练 + 每 epoch 验证 + 保存 best ----------\n",
    "CKPT_BEST = os.path.join(EXP_DIR, \"best_hrds.pt\")\n",
    "REPORT_PATH = os.path.join(EXP_DIR, \"train_eval_log.json\")\n",
    "\n",
    "history = []\n",
    "best_score = -1.0\n",
    "best_epoch = -1\n",
    "\n",
    "def _extract_macro_strict(metrics_dict):\n",
    "    if not isinstance(metrics_dict, dict):\n",
    "        return None\n",
    "    candidates = [\n",
    "        \"steds_strict_macro\", \"macro_steds_strict\", \"steds_macro_strict\",\n",
    "        \"steds_strict_mean\",  \"strict_mean\",        \"steds_strict\",\n",
    "    ]\n",
    "    for k in candidates:\n",
    "        if k in metrics_dict and isinstance(metrics_dict[k], (int, float)):\n",
    "            return float(metrics_dict[k])\n",
    "    for k, v in metrics_dict.items():\n",
    "        if \"strict\" in k.lower() and isinstance(v, (int, float)):\n",
    "            return float(v)\n",
    "    return None\n",
    "\n",
    "for ep in range(1, cfg.epochs + 1):\n",
    "    print(f\"\\n========== [HRDS] Epoch {ep}/{cfg.epochs} ==========\")\n",
    "\n",
    "    # 1) train\n",
    "    tr_logs = train_one_epoch(model, train_loader, optimizer, cfg, focal_cls, focal_rel)\n",
    "\n",
    "    # 2) export + eval on test\n",
    "    export_dir = os.path.join(EXP_DIR, f\"export_ep{ep:02d}\")\n",
    "    if os.path.isdir(export_dir):\n",
    "        shutil.rmtree(export_dir, ignore_errors=True)\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "    # [关键] 每次导出前安装安全推理函数，避免旧定义干扰\n",
    "    _install_safe_predict_doc_with_rel_recompute()\n",
    "\n",
    "    t0 = time.time()\n",
    "    metrics = None\n",
    "    try:\n",
    "        export_split_predictions(model, test_loader, save_dir=export_dir, device=device)\n",
    "        metrics = eval_export_dir_dual(export_dir, exclude_meta=True)\n",
    "    except Exception as e:\n",
    "        # 不中断训练：记录错误，继续下一个 epoch\n",
    "        print(\"[WARN] export/eval failed at epoch\", ep, \"error:\", repr(e))\n",
    "        metrics = {\"_error\": repr(e)}\n",
    "    eval_time = time.time() - t0\n",
    "\n",
    "    # 3) best selection（以 Macro Strict STEDS 为主）\n",
    "    score = _extract_macro_strict(metrics) if isinstance(metrics, dict) else None\n",
    "    if score is None:\n",
    "        print(\"[WARN] 无法从 metrics 中提取 strict 指标字段，本 epoch 不更新 best。metrics=\", metrics)\n",
    "        score = -1.0\n",
    "\n",
    "    is_best = score > best_score\n",
    "    if is_best:\n",
    "        best_score = score\n",
    "        best_epoch = ep\n",
    "        torch.save({\"model\": model.state_dict(), \"epoch\": ep, \"metrics\": metrics}, CKPT_BEST)\n",
    "        print(f\"[BEST] updated: epoch={ep}, score={best_score:.6f} -> {CKPT_BEST}\")\n",
    "\n",
    "    row = {\n",
    "        \"epoch\": ep,\n",
    "        \"train\": tr_logs,\n",
    "        \"metrics\": metrics,\n",
    "        \"macro_strict_for_select\": score,\n",
    "        \"eval_time_sec\": eval_time,\n",
    "        \"is_best\": is_best,\n",
    "    }\n",
    "    history.append(row)\n",
    "\n",
    "    with open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"exp_name\": EXP_NAME,\n",
    "                \"hrds_root\": HRDS_ROOT,\n",
    "                \"cfg\": cfg.__dict__ if hasattr(cfg, \"__dict__\") else str(cfg),\n",
    "                \"use_text\": USE_TEXT,\n",
    "                \"use_visual\": USE_VISUAL,\n",
    "                \"use_softmask\": USE_SOFTMASK,\n",
    "                \"best_epoch\": best_epoch,\n",
    "                \"best_score_macro_strict\": best_score,\n",
    "                \"best_ckpt\": CKPT_BEST,\n",
    "                \"history\": history,\n",
    "            },\n",
    "            f,\n",
    "            ensure_ascii=False,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "    print(\"[Epoch Summary]\")\n",
    "    # 兼容：loss 可能是 float / list / numpy / tensor\n",
    "    loss_last = None\n",
    "    if isinstance(tr_logs, dict) and \"loss\" in tr_logs:\n",
    "        v = tr_logs[\"loss\"]\n",
    "        if isinstance(v, (list, tuple)):\n",
    "            loss_last = v[-1] if len(v) else None\n",
    "        elif torch.is_tensor(v):\n",
    "            loss_last = float(v.detach().cpu().item()) if v.numel() == 1 else float(v.detach().cpu().mean().item())\n",
    "        else:\n",
    "            # float / int / numpy scalar\n",
    "            try:\n",
    "                loss_last = float(v)\n",
    "            except Exception:\n",
    "                loss_last = v\n",
    "    else:\n",
    "            loss_last = tr_logs\n",
    "\n",
    "    print(\"  train loss(last):\", loss_last)\n",
    "\n",
    "    print(\"  metrics:\", metrics)\n",
    "    print(\"  eval_time_sec:\", eval_time)\n",
    "\n",
    "print(\"\\n========== HRDS Full Flow DONE ==========\")\n",
    "print(\"[BEST] epoch =\", best_epoch, \"macro_strict =\", best_score)\n",
    "print(\"[BEST CKPT]\", CKPT_BEST)\n",
    "print(\"[LOG JSON ]\", REPORT_PATH)\n",
    "\n",
    "print(\"\\nPaper reference (Table 2 HRDS best, Document+Semantic+Vision+Soft-mask):\")\n",
    "print(\"Micro-STEDS ≈ 0.8143\")\n",
    "print(\"Macro-STEDS ≈ 0.8174\")\n",
    "\n",
    "# --------- J) 用 best ckpt 再导出一次（最终留档） ----------\n",
    "FINAL_EXPORT = os.path.join(EXP_DIR, \"export_best_final\")\n",
    "if os.path.isdir(FINAL_EXPORT):\n",
    "    shutil.rmtree(FINAL_EXPORT, ignore_errors=True)\n",
    "os.makedirs(FINAL_EXPORT, exist_ok=True)\n",
    "\n",
    "_install_safe_predict_doc_with_rel_recompute()\n",
    "\n",
    "if os.path.exists(CKPT_BEST):\n",
    "    ckpt = torch.load(CKPT_BEST, map_location=\"cpu\")\n",
    "    state = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else ckpt\n",
    "    model.load_state_dict(state, strict=False)\n",
    "    model.eval()\n",
    "\n",
    "export_split_predictions(model, test_loader, save_dir=FINAL_EXPORT, device=device)\n",
    "final_metrics = eval_export_dir_dual(FINAL_EXPORT, exclude_meta=True)\n",
    "\n",
    "FINAL_REPORT = os.path.join(\"final_reports\", f\"{EXP_NAME}_HRDS_report.json\")\n",
    "os.makedirs(\"final_reports\", exist_ok=True)\n",
    "with open(FINAL_REPORT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"dataset\": \"HRDoc-Simple (HRDS)\",\n",
    "            \"hrds_root\": HRDS_ROOT,\n",
    "            \"best_ckpt\": CKPT_BEST,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"best_score_macro_strict\": best_score,\n",
    "            \"final_export_dir\": FINAL_EXPORT,\n",
    "            \"final_metrics\": final_metrics,\n",
    "            \"log_path\": REPORT_PATH,\n",
    "        },\n",
    "        f,\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "print(\"\\n===== HRDS Final Validation =====\")\n",
    "print(\"Final export :\", FINAL_EXPORT)\n",
    "print(\"Final report :\", FINAL_REPORT)\n",
    "print(\"Final metrics:\", final_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb11f2-790e-4be1-9a56-037ebc0d3c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hrdoc)",
   "language": "python",
   "name": "hrdoc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
