{
  "doc_id": "1412.7419",
  "nodes": [
    {
      "id": 0,
      "text": "ADASECANT: Robust Adaptive Secant Method for Stochastic",
      "label_id": 0,
      "label_name": "Title",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 1,
      "text": "Gradient",
      "label_id": 0,
      "label_name": "Title",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 2,
      "text": "Caglar Gulcehre",
      "label_id": 1,
      "label_name": "Author",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 3,
      "text": "Marcin Moczulski",
      "label_id": 1,
      "label_name": "Author",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 4,
      "text": "Universit ´e de Montr´eal",
      "label_id": 3,
      "label_name": "Affiliation",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 5,
      "text": "University of Oxford",
      "label_id": 3,
      "label_name": "Affiliation",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 6,
      "text": "caglar.gulcehre@umontreal.ca",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 7,
      "text": "marcin.moczulski@stcatz.ox.ac.uk",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 8,
      "text": "Yoshua Bengio",
      "label_id": 1,
      "label_name": "Author",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 9,
      "text": "Universit ´e de Montr´eal",
      "label_id": 3,
      "label_name": "Affiliation",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 10,
      "text": "bengioy@iro.umontreal.ca",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 11,
      "text": "Abstract",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 12,
      "text": "Stochastic gradient algorithms have been the main focus of large-scale learning problems and they led to",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 11,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 13,
      "text": "important successes in machine learning. The convergence of SGD depends on the careful choice of learning",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 12,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 14,
      "text": "rate and the amount of the noise in stochastic estimates of the gradients. In this paper, we propose a new",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 13,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 15,
      "text": "adaptive learning rate algorithm, which utilizes curvature information for automatically tuning the learning",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 14,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 16,
      "text": "rates. The information about the element-wise curvature of the loss function is estimated from the local",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 15,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 17,
      "text": "statistics of the stochastic first order gradients. We further propose a new variance reduction technique to",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 16,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 18,
      "text": "speed up the convergence. In our preliminary experiments with deep neural networks, we obtained better",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 17,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 19,
      "text": "performance compared to the popular stochastic gradient algorithms.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 18,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 20,
      "text": "1 Introduction",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 11,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 21,
      "text": "In this paper we develop a stochastic gradient algorithm that reduces the burden of extensive hyper-parameter search for the",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 20,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 22,
      "text": "optimization algorithm. The proposed algorithm exploits a low variance estimator of curvature of the cost function and uses",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 21,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 23,
      "text": "it to obtain an automatically tuned adaptive learning rate for each parameter.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 22,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 24,
      "text": "In the deep learning and numerical optimization, several papers suggest using a diagonal approximation of the Hessian (second",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 21,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 25,
      "text": "derivative matrix of the cost function with respect to parameters), in order to estimate optimal learning rates for stochastic",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 24,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 26,
      "text": "gradient descent over high dimensional parameter spaces [3, 14, 9]. A fundamental advantage of using such approximation",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 25,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 27,
      "text": "is that inverting it is a trivial and cheap operation. However generally, for neural networks, the inverse of the diagonal",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 26,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 28,
      "text": "Hessian is usually a bad approximation of the diagonal of the inverse of Hessian. Examples options of obtaining a diagonal",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 27,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 29,
      "text": "approximation of Hessian are the Gauss-Newton matrix [10] or by finite differences [13]. Such estimations may however be",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 28,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 30,
      "text": "sensitive to noise due to the stochastic gradient. [14] suggested a reliable way to estimate the local curvature in the stochastic",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 29,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 31,
      "text": "setting by keeping track of the variance and average of the gradients.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 30,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 32,
      "text": "In this paper, we followed a different approach: instead of using a diagonal estimate of Hessian, we proposed to estimate",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 24,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 33,
      "text": "curvature along the direction of the gradient and we apply a new variance reduction technique to compute it reliably. By using",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 32,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 34,
      "text": "root mean square statistics, the variance of gradients are reduced adaptively with a simple transformation. We keep track",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 33,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 35,
      "text": "of the estimation of curvature using a technique similar to that proposed by [14], which uses the variability of the expected",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 34,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 36,
      "text": "loss. Standard adaptive learning rate algorithms only scale the gradients, but regular Newton-like second order methods, can",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 35,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 37,
      "text": "perform more complicate transformations, e.g. rotating the gradient vector. Newton and quasi-newton methods can also be",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 36,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 38,
      "text": "invariant to affine transformations in the parameter space. Our proposed Adasecant algorithm is basically a stochastic rank-1",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 37,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 39,
      "text": "quasi-Newton method. But in comparison with other adaptive learning algorithms, instead of just scaling the gradient of each",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 38,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 40,
      "text": "parameter, Adasecant can also perform an affine transformation on them.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 39,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 41,
      "text": "1",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 0,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 42,
      "text": "2 Directional Secant Approximation",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 20,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 43,
      "text": "Directional Newton is a method proposed for solving equations with multiple variables [11]. The advantage of directional",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 42,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 44,
      "text": "Newton method proposed in [11], compared to Newton’s method is that, it does not require a matrix inversion and still",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 43,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 45,
      "text": "maintains a quadratic rate of convergence.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 44,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 46,
      "text": "In this paper, we developed a second-order directional Newton method for nonlinear optimization. Step-size tk of update ∆k",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 45,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 47,
      "text": "for step k can be written as if it was a diagonal matrix:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 46,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 48,
      "text": "∆k = tk ⊙ ∇θ f(θk ) = diag(tk )∇θ f(θk) = − diag(dk)(diag(Hdk ))−1∇θf(θ k) (1)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 47,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 49,
      "text": "where θk is the parameter vector at update k, f is the objective function and dk is a unit vector of direction that the optimization",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 48,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 50,
      "text": "algorithm should follow. Denoting by hi = ∇θ ∂ f(θk)∂θi the ith row of the Hessian matrix H and by ∇θi f (θk ) the ith element",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 49,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 51,
      "text": "of the gradient vector at update k, a reformulation of Equation 1 for each diagonal element of the step-size diag(tk) is:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 50,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 52,
      "text": "∆ki = −tki ∇θi f(θ k) = −dki ∇θ i f(θk) hki dk , so effectively tki = dki hki dk (2)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 51,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 53,
      "text": "We can approximate the per-parameter learning rate tki following [1]:",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 43,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 54,
      "text": "tki = dki hki dk ≈ tki dki ∇θi f(θk + tkdk ) − ∇θ i f (θk ) (3)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 53,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 55,
      "text": "Please note that alternatively one might use the R-op to compute the Hessian-vector product for the denominator in Equation",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 53,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 56,
      "text": "3 [15].",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 55,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 57,
      "text": "To choose a good direction dk in the stochastic setting, we use a block-normalized gradient vector for each weight matrix",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 55,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 58,
      "text": "Wik and bias vector bik where θ = (cid:8)W ik, bik (cid:9)i=1···k at each layer i and update k, i.e. dkWik = ∇W ik f(θ)||∇W ik f(θ)||2 and dk =",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 57,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 59,
      "text": "(cid:104)dkW 0k dkb0k · · · dkblk (cid:105) for a neural network with l layers. Block normalization of the gradient adds an additional noise, but in",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 58,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 60,
      "text": "practice we did not observe any negative impact of it. We conjecture that this is due to the angle between the stochastic",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 59,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 61,
      "text": "gradient and the block-normalized gradient still being less than 90 degrees. The update step is defined as ∆ki = tki dki . The",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 60,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 62,
      "text": "per-parameter learning rate tki can be estimated with the finite difference approximation,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 61,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 63,
      "text": "tki = ∆ki ∇θ i f(θk + ∆k ) − ∇θ i f(θk) , (4)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 62,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 64,
      "text": "since, in the vicinity of the quadratic local minima,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 63,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 65,
      "text": "∇θ f(θ k + ∆k ) − ∇θ f(θ k ) ≈ Hk ∆k (5)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 64,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 66,
      "text": "We can therefore recover tk as",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 57,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 67,
      "text": "tk = diag(∆k)(diag (Hk∆k ))−1 . (6)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 66,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 68,
      "text": "The directional secant method basically scales the gradient of each parameter with the curvature along the direction of the",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 66,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 69,
      "text": "gradient vector and it is numerically stable.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 68,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 70,
      "text": "3 Variance Reduction for Robust Stochastic Gradient Descent",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 42,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 71,
      "text": "Variance reduction techniques for stochastic gradient estimators have been well-studied in the machine learning literature.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 70,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 72,
      "text": "Both [16] and [8] proposed new ways of dealing with this problem. In this paper, we proposed a new variance reduction",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 71,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 73,
      "text": "technique for stochastic gradient descent that relies only on basic statistics related to the gradient. Let gi refer to the ith",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 72,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 74,
      "text": "element of the gradient vector g with respect to the parameters θ and E[·] be an expectation taken over minibatches and",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 73,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 75,
      "text": "different trajectories of parameters.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 74,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 76,
      "text": "We propose to apply the following transformation to reduce the variance of the stochastic gradients:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 75,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 77,
      "text": "˜gi = gi + γiE[gi ] 1 + γi (7)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 76,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 78,
      "text": "where γi is strictly a positive real number. Let us note that:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 77,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 79,
      "text": "E[ ˜gi ] = E[gi] and Var( ˜gi ) = 1 (1 + γi)2 Var(gi) (8)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 78,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 80,
      "text": "2",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 1,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 81,
      "text": "So variance is reduced by a factor of (1 + γi)2 compared to Var(gi ). In practice we do not have access to E[gi ], therefore a",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 71,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 82,
      "text": "biased estimator gi based on past values of gi will be used instead. We can rewrite the ˜gi as:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 81,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 83,
      "text": "˜gi = 1 1 + γi gi + (1 − 1 1 + γi )E[gi] (9)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 82,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 84,
      "text": "After substitution βi = 11+γi , we will have:",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 81,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 85,
      "text": "˜gi = βigi + (1 − βi )E[gi] (10)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 84,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 86,
      "text": "By adapting γi or βi, it is possible to control the influence of high variance, unbiased gi and low variance, biased gi on ˜gi.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 84,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 87,
      "text": "Denoting by g′ the stochastic gradient obtained on the next minibatch, the γi that well balances those two influences is the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 86,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 88,
      "text": "one that keeps the ˜gi as close as possible to the true gradient E[g′i] with g ′i being the only sample of E[g ′i ] available:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 87,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 89,
      "text": "arg min βi E[|| ˜gi − g ′i ||22 ] (11)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 88,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 90,
      "text": "It can be shown that this a convex problem in βi with a closed-form solution (details in appendix) and we can obtain the γi",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 86,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 91,
      "text": "from it:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 90,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 92,
      "text": "γi = E[(gi − g ′i )(gi − E[gi])] E[(gi − E[gi ])(gi′ − E[gi]))] (12)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 91,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 93,
      "text": "E[(gi−E[gi])(g′i −E[gi ]))] during training. As a result, in order to estimate γ for each dimension, we keep track of a estimation of E[(gi −g′i)(gi−E[gi])]",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 90,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 94,
      "text": "The necessary and sufficient condition here, for the variance reduction is to keep γ positive, to achieve a positive estimate of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 93,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 95,
      "text": "γ we used the root mean square statistics for the expectations.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 94,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 96,
      "text": "4 Adaptive Step-size in Stochastic Case",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 70,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 97,
      "text": "In the stochastic gradient case, the step-size of the directional secant can be computed by using an expectation over the",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 96,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 98,
      "text": "minibatches:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 97,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 99,
      "text": "Ek[ti] = Ek [ ∆ki ∇θ i f(θk + ∆k ) − ∇θi f(θ k) ] (13)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 98,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 100,
      "text": "The Ek [·] that is used to compute the secant update, is taken over the minibatches at the past values of the parameters.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 97,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 101,
      "text": "Computing the expectation in Eq 13 was numerically unstable in stochastic setting. We decided to use a more stable second",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 100,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 102,
      "text": "Ek[(αki )2 ], (cid:112)Ek[(∆ki )2 ]), with αki = ∇θi f(θk + ∆k ) − ∇θi f(θ k).",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 100,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 103,
      "text": "order Taylor approximation of Equation 13 around (",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 101,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 104,
      "text": "(cid:112)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 103,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 105,
      "text": "Assuming (cid:112)Ek [(αki )2] ≈ Ek [αki ] and (cid:112)Ek [(∆ki )2 ] ≈ Ek [∆ki ] we obtain always non-negative approximation of Ek[ti ]:",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 102,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 106,
      "text": "Ek [ti] ≈ (cid:112)Ek[(∆ki )2] (cid:112) Ek[(αki )2 ] − Cov(αki , ∆ki ) Ek [(αki )2] (14)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 105,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 107,
      "text": "In our experiments, we used a simpler approximation, which in practice worked as well as formulations in Eq 14:",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 105,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 108,
      "text": "Ek [ti ] ≈ (cid:112) Ek[(∆ki )2 ] (cid:112) Ek[(αki )2 ] − Ek [αki ∆ki ] Ek [(αki )2] (15)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 107,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 109,
      "text": "5 Algorithmic Details",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 96,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 110,
      "text": "5.1 Approximate Variability",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 109,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 111,
      "text": "To compute the moving averages as also adopted by [14], we used an algorithm to dynamically decide the time constant based",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 110,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 112,
      "text": "on the step size being taken. As a result algorithm that we used will give bigger weights to the updates that have large step-size",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 111,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 113,
      "text": "and smaller weights to the updates that have smaller step-size.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 112,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 114,
      "text": "By assuming that ¯∆i[k ] ≈ E[∆i ]k , the moving average update rule for ¯∆i [k] can be written as,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 113,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 115,
      "text": "¯∆2i [k] = (1 − τ −1i [k ]) ¯∆2i [k − 1] + τ −1i [k ](tki ˜gki ), and ¯∆i [k] = (cid:113) ¯∆2i [k] (16)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 114,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 116,
      "text": "This rule for each update assigns a different weight to each element of the gradient vector . At each iteration a scalar",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 111,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 117,
      "text": "multiplication with τ −1i is performed and τi is adapted using the following equation:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 116,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 118,
      "text": "τi [k] = (1 − E[∆i ]2k−1 E[(∆i)2 ]k−1 )τi[k − 1] + 1 (17)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 117,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 119,
      "text": "3",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 2,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 120,
      "text": "5.2 Outlier Gradient Detection",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 110,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 121,
      "text": "Our algorithm is very similar to [13], but instead of incrementing τi[t + 1] when an outlier is detected, the time-constant is",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 120,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 122,
      "text": "reset to 2.2. Note that when τi [t + 1] ≈ 2, this assigns approximately the same amount of weight to the current and the",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 121,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 123,
      "text": "average of previous observations. This mechanism made learning more stable, because without it outlier gradients saturate τi",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 122,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 124,
      "text": "to a large value.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 123,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 125,
      "text": "5.3 Variance Reduction",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 120,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 126,
      "text": "The correction parameters γi (Eq 12) allows for a fine-grained variance reduction for each parameter independently. The",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 125,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 127,
      "text": "noise in the stochastic gradient methods can have advantages both in terms of generalization and optimization. It introduces",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 126,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 128,
      "text": "an exploration and exploitation trade-off, which can be controlled by upper bounding the values of γi with a value ρi, so that",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 127,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 129,
      "text": "thresholded γ ′i = min(ρi , γi ).",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 128,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 130,
      "text": "We block-wise normalized the gradients of each weight matrix and bias vectors in g to compute the ˜g as described in Section 2.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 129,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 131,
      "text": "That makes Adasecant scale-invariant, thus more robust to the scale of the inputs and the number of the layers of the network.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 130,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 132,
      "text": "We observed empirically that it was easier to train very deep neural networks with block normalized gradient descent.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 131,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 133,
      "text": "6 Improving Convergence",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 109,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 134,
      "text": "Classical convergence results for SGD are based on the conditions:",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 133,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 135,
      "text": "(cid:88) i (η(i) )2 < ∞ and (cid:88) i η (i) = ∞ (18)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 134,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 136,
      "text": "such that the learning rate η(i) should decrease [12]. Due to the noise in the estimation of adaptive step-sizes for Adasecant,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 135,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 137,
      "text": "the convergence would not be guaranteed. To ensure it, we developed a new variant of Adagrad [4] with thresholding, such",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 136,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 138,
      "text": "that each scaling factor is lower bounded by 1. Assuming aki is the accumulated norm of all past gradients for ith parameter",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 137,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 139,
      "text": "at update k, it is thresholded from below ensuring that the algorithm will converge:",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 138,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 140,
      "text": "aki = (cid:118) (cid:117) (cid:117) (cid:116) k (cid:88) j=0 (gji )2 and ρki = maximum(1, aki ) giving ∆ki = 1ρi ηki ˜gki (19)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 139,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 141,
      "text": "In the initial stages of training, accumulated norm of the per-parameter gradients can be less than 1. If the accumulated per-",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 134,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 142,
      "text": "parameter norm of a gradient is less than 1, Adagrad will augment the learning-rate determined by Adasecant for that update,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 141,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 143,
      "text": "i.e. ηkiρki > ηki where ηki = Ek [tki ] is the per-parameter learning rate determined by Adasecant. This behavior tends to create",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 142,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 144,
      "text": "unstabilities during the training with Adasecant. Our modification of the Adagrad algorithm is to ensure that, it will reduce",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 143,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 145,
      "text": "the learning rate determined by the Adasecant algorithm at each update, i.e. η kiρki ≤ η ki and the learning rate will be bounded.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 144,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 146,
      "text": "At the beginning of the training, parameters of a neural network can get 0-valued gradients, e.g. in the existence of dropout",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 145,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 147,
      "text": "and ReLU units. However this phenomena can cause the per-parameter learning rate scaled by Adagrad to be unbounded.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 146,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 148,
      "text": "In Algorithm 1, we provide a simple pseudo-code of the Adasecant algorithm.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 141,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 149,
      "text": "7 Experiments",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 133,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 150,
      "text": "We have performed our experiments on MNIST with Maxout Networks [6] comparing Adasecant with popular stochastic",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 149,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 151,
      "text": "gradient learning algorithms: Adagrad, Rmsprop [7], Adadelta [17] and SGD+momentum (with linearly decaying learning",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 150,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 152,
      "text": "rate). Results are summarized in Figure 1 at the appendinx and we showed that Adasecant converges as fast or faster than",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 151,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 153,
      "text": "other techniques, including the use of hand-tuned global learning rate and momentum for SGD, RMSprop, and Adagrad.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 152,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 154,
      "text": "8 Conclusion",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 149,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 155,
      "text": "We described a new stochastic gradient algorithm with adaptive learning rates that is fairly insensitive to the tuning of the",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 154,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 156,
      "text": "hyper-parameters and doesn’t require tuning of learning rates. Furthermore, the variance reduction technique we proposed",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 155,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 157,
      "text": "improves the convergence when the stochastic gradients have high variance. According to preliminary experiments presented,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 156,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 158,
      "text": "we were able to obtain a better training performance compared to other popular, well-tuned stochastic gradient algorithms.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 157,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 159,
      "text": "As a future work, we should do a more comprehensive analysis, which will help us to better understand the algorithm both",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 158,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 160,
      "text": "analytically and empirically.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 159,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 161,
      "text": "4",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 3,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 162,
      "text": "Acknowledgments",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 154,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 163,
      "text": "We thank the developers of Theano [2] and Pylearn2 [5] and the computational resources provided by Compute Canada and",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 162,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 164,
      "text": "Calcul Qu ´ebec. This work has been partially supported by NSERC, CIFAR, and Canada Research Chairs, Project TIN2013-",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 163,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 165,
      "text": "41751, grant 2014-SGR-221. We would like to thank Tom Schaul for the valuable discussions. We also thank Kyunghyun",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 164,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 166,
      "text": "Cho and Orhan Firat for proof-reading and giving feedbacks on the paper.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 165,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 167,
      "text": "References",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 154,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 168,
      "text": "[1] Heng-Bin An and Zhong-Zhi Bai. Directional secant method for nonlinear equations. Journal of computational and applied mathe-",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 167,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 169,
      "text": "matics, 175(2):291–304, 2005.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 168,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 170,
      "text": "[2] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. Goodfellow, A. Bergeron, N. Bouchard, D. Warde-Farley, and Y. Bengio. Theano:",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 168,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 171,
      "text": "new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 170,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 172,
      "text": "[3] Sue Becker and Yann Le Cun. Improving the convergence of back-propagation learning with second order methods. In Proceedings",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 170,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 173,
      "text": "of the 1988 connectionist models summer school, pages 29–37. San Matteo, CA: Morgan Kaufmann, 1988.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 172,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 174,
      "text": "[4] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. The",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 172,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 175,
      "text": "Journal of Machine Learning Research, 12:2121–2159, 2011.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 174,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 176,
      "text": "[5] I. J. Goodfellow, D. Warde-Farley, P. Lamblin, V. Dumoulin, M. Mirza, R. Pascanu, J. Bergstra, F. Bastien, and Y. Bengio. Pylearn2:",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 174,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 177,
      "text": "a machine learning research library. arXiv preprint arXiv:1308.4214, 2013.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 176,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 178,
      "text": "[6] Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. arXiv preprint",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 176,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 179,
      "text": "arXiv:1302.4389, 2013.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 178,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 180,
      "text": "[7] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 178,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 181,
      "text": "[8] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 180,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 182,
      "text": "Information Processing Systems, pages 315–323, 2013.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 181,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 183,
      "text": "[9] Yann LeCun, Patrice Y Simard, and Barak Pearlmutter. Automatic learning rate maximization by on-line estimation of the hessian’s",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 181,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 184,
      "text": "eigenvectors. Advances in neural information processing systems, 5:156–163, 1993.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 183,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 185,
      "text": "[10] Yann A LeCun, L´eon Bottou, Genevieve B Orr, and Klaus-Robert M ¨uller. Efficient backprop. In Neural networks: Tricks of the trade,",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 183,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 186,
      "text": "pages 9–48. Springer, 2012.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 185,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 187,
      "text": "[11] Yuri Levin and Adi Ben-Israel. Directional newton methods in n variables. Mathematics of Computation, 71(237):251–262, 2002.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 185,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 188,
      "text": "[12] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400–407, 1951.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 187,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 189,
      "text": "[13] Tom Schaul and Yann LeCun. Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients. arXiv preprint",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 188,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 190,
      "text": "arXiv:1301.3764, 2013.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 189,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 191,
      "text": "[14] Tom Schaul, Sixin Zhang, and Yann LeCun. No more pesky learning rates. arXiv preprint arXiv:1206.1106, 2012.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 189,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 192,
      "text": "[15] Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural computation, 14(7):1723–1738,",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 191,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 193,
      "text": "2002.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 192,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 194,
      "text": "[16] Chong Wang, Xi Chen, Alex Smola, and Eric Xing. Variance reduction for stochastic gradient optimization. In Advances in Neural",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 192,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 195,
      "text": "Information Processing Systems, pages 181–189, 2013.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 194,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 196,
      "text": "[17] Matthew D Zeiler. Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 194,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 197,
      "text": "A Appendix",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 167,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 198,
      "text": "A.1 Derivation of Eq 11",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 197,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 199,
      "text": "∂ E[(βi gi + (1 − βi)E[gi ] − g ′i )2] ∂βi = 0 (20) E[(βi gi + (1 − βi)E[gi ] − g ′i ) ∂ (βigi + (1 − βi )E[gi] − g′i) ∂ βi ] = 0 (21) E[(βigi + (1 − βi )E[gi ] − g′i)(gi − E[gi ])] = 0 (22) E[(βigi(gi − E[gi ]) + (1 − βi )E[gi](gi − E[gi ]) − E[gi](gi − E[gi ])] = 0 (23) βi = E[(gi − E[gi])(g′i − E[gi ])] E[(gi − E[gi])(gi − E[gi ])] = E[(gi − E[gi])(g ′i − E[gi])] Var(gi) (24)",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 198,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 200,
      "text": "5",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 4,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 201,
      "text": "A.2 Algorithm pseudo-code",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 198,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 202,
      "text": "Algorithm 1 contains the pseudo-code of the Adasecant algorithm.",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 201,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 203,
      "text": "Algorithm 1: Adasecant: minibatch-Adasecant for adaptive learning rates with variance reductionrepeatdraw n samples, compute the gradients g(j ) where g(j) ∈ Rn for each minibatch j, g(j ) is computedas, 1n (cid:80)nk=1 ∇(k)θ f(θ)block-wise normalize gradients of each weight matrix and bias vectorfor parameter i ∈ {1, . . . , n} docompute the correction term by using, γ ki = E[(gi−g ′i )(gi −E[gi ])]kE[(gi −E[gi ])(g′i−E[gi]))]kcompute corrected gradients ˜gi = gi +γi E[gi]1+γiif |g(j)i − E[gi ]| > 2(cid:112)E[(gi )2] − (E[gi ])2 or (cid:12)(cid:12)(cid:12)α(j)i − E[αi](cid:12)(cid:12)(cid:12) > 2(cid:112)E[(αi)2 ] − (E[αi ])2 thenreset the memory size for outliers τi ← 2.2endupdate moving averages according to Equation 16estimate learning rate η(j )i ← (cid:113)Ek [(∆(k)i )2](cid:112)Ek [(αki )2] − Ek[αki ∆ki ]Ek [(αki )2 ]update memory size as in Equation 17update parameter θ ji ← θj−1i − η(j)i · ˜g(j )ienduntil stopping criterion is met;",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 202,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 204,
      "text": "A.3 Further Experimental Details",
      "label_id": 4,
      "label_name": "Section",
      "is_meta": false,
      "parent": 201,
      "rel_id": 2,
      "rel_name": "equality",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 205,
      "text": "In our experiments with Adasecant algorithm, adaptive momentum term γ ki was clipped at 1.8. In 2-layer Maxout network",
      "label_id": 5,
      "label_name": "First-Line",
      "is_meta": false,
      "parent": 204,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 206,
      "text": "experiments for SGD-momentum experiments, we used the best hyper-parameters reported by [6], for Rmsprop and Adagrad,",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 205,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 207,
      "text": "we crossvalidated learning rate for 15 different learning rates sampled uniformly from the log-space. We crossvalidated 30",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 206,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 208,
      "text": "different pairs of momentum and learning rate for SGD+momentum, for Rmsprop and Adagrad, we crossvalidated 15 different",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 207,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 209,
      "text": "learning rates sampled them from log-space uniformly for deep maxout experiments. In Figure 2, we analyzed the effect of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 208,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 210,
      "text": "using different minibatch sizes for Adasecant and compared its convergence with Adadelta in wall-clock time. For minibatch",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 209,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 211,
      "text": "size 100 Adasecant was able to reach the almost same training negative log-likelihood as Adadelta after the same amount of",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 210,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 212,
      "text": "time, but its convergence took much longer. With minibatches of size 500 Adasecant was able to converge faster in wallclock",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 211,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 213,
      "text": "time to a better local minima.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 212,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 214,
      "text": "6",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 5,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 215,
      "text": "0 50 100 150 200 250 300 350# Epochs10-910-810-710-610-510-410-310-210-1100Trainnll(log-scale) AdagradSGD+momentumAdasecantRmsprop(a) 2 layer Maxout Network 0 50 100 150 200 250 300 350# Epochs0.00.51.01.52.02.5Trainingnll AdagradAdasecantSGD+momentumRmspropAdadelta(b) 16 layer Maxout Network",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 216,
      "text": "Figure 1: Comparison of different stochastic gradient algorithms on MNIST with Maxout Networks. Both a) and b) are",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 215,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 217,
      "text": "trained with dropout and maximum column norm constraint regularization on the weights. Networks are initialized with",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 216,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 218,
      "text": "weights sampled from a Gaussian distribution with 0 mean and standard deviation of 0.05. In both experiments, the proposed",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 217,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 219,
      "text": "algorithm, Adasecant, seems to be converging faster and arrives to a better minima in training set. We trained both networks",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 218,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 220,
      "text": "for 350 epochs over the training set.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 219,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 221,
      "text": "0 500 1000 1500 2000 2500 3000Time elapsed for epochs in secs.10-1110-1010-910-810-710-610-510-410-310-210-1100nllinlogscale. Adadelta minibatch size: 100Adasecant minibatch size: 100Adasecant minibatch size: 500",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": -1,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 222,
      "text": "Figure 2: In this plot, we compared adasecant trained by using minibatch size of 100 and 500 with adadelta using minibatches",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 221,
      "rel_id": 1,
      "rel_name": "contain",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 223,
      "text": "of size 100. We performed these experiments on MNIST with 2-layer maxout MLP using dropout.",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": false,
      "parent": 222,
      "rel_id": 0,
      "rel_name": "connect",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    },
    {
      "id": 224,
      "text": "7",
      "label_id": 6,
      "label_name": "Para-Line",
      "is_meta": true,
      "parent": -1,
      "rel_id": 3,
      "rel_name": "meta",
      "page_id": 6,
      "box": [
        0,
        0,
        0,
        0
      ]
    }
  ]
}