{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9412faf0-c86f-4ea4-a247-3ea3623e200c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       doc_id  split lang                                         label_path  \\\n",
       " 0  024v9632db  train   en  C:\\Users\\tomra\\Desktop\\PAPER\\part2\\dochienet_d...   \n",
       " 1  02nk0izsev   test   zh  C:\\Users\\tomra\\Desktop\\PAPER\\part2\\dochienet_d...   \n",
       " 2  02uch9a7af   test   en  C:\\Users\\tomra\\Desktop\\PAPER\\part2\\dochienet_d...   \n",
       " 3  03uc9l6o7j  train   en  C:\\Users\\tomra\\Desktop\\PAPER\\part2\\dochienet_d...   \n",
       " 4  04xbptr4ih  train   en  C:\\Users\\tomra\\Desktop\\PAPER\\part2\\dochienet_d...   \n",
       " \n",
       "    n_pages                                         page_paths  \n",
       " 0        5  [C:\\Users\\tomra\\Desktop\\PAPER\\part2\\dochienet_...  \n",
       " 1        6  [C:\\Users\\tomra\\Desktop\\PAPER\\part2\\dochienet_...  \n",
       " 2       19  [C:\\Users\\tomra\\Desktop\\PAPER\\part2\\dochienet_...  \n",
       " 3       11  [C:\\Users\\tomra\\Desktop\\PAPER\\part2\\dochienet_...  \n",
       " 4        8  [C:\\Users\\tomra\\Desktop\\PAPER\\part2\\dochienet_...  ,\n",
       " split\n",
       " train    1512\n",
       " test      161\n",
       " Name: count, dtype: int64,\n",
       " lang\n",
       " en    1110\n",
       " zh     563\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "DATA_DIR = ROOT / \"dochienet_dataset\"\n",
    "labels_dir = DATA_DIR / \"labels\"\n",
    "images_dir = DATA_DIR / \"images\"\n",
    "hres_dir = DATA_DIR / \"hres_images\"\n",
    "\n",
    "# splits\n",
    "en_zh = json.loads((DATA_DIR / \"en_zh_split.json\").read_text(encoding=\"utf-8\"))\n",
    "tt = json.loads((DATA_DIR / \"train_test_split.json\").read_text(encoding=\"utf-8\"))\n",
    "en_ids, zh_ids = set(en_zh[\"en\"]), set(en_zh[\"zh\"])\n",
    "train_ids, test_ids = set(tt[\"train\"]), set(tt[\"test\"])\n",
    "\n",
    "def get_lang(doc_id):\n",
    "    return \"en\" if doc_id in en_ids else (\"zh\" if doc_id in zh_ids else \"unknown\")\n",
    "\n",
    "def get_split(doc_id):\n",
    "    return \"train\" if doc_id in train_ids else (\"test\" if doc_id in test_ids else \"unknown\")\n",
    "\n",
    "def list_pages(doc_id, prefer_hres=True):\n",
    "    base = hres_dir if prefer_hres else images_dir\n",
    "    folder = base / doc_id\n",
    "    # page1.jpg, page2.jpg...\n",
    "    pages = sorted(folder.glob(\"page*.jpg\"), key=lambda p: int(p.stem.replace(\"page\",\"\")))\n",
    "    return pages\n",
    "\n",
    "doc_ids = sorted(list(train_ids | test_ids))\n",
    "rows = []\n",
    "for did in doc_ids:\n",
    "    lp = labels_dir / f\"{did}.json\"\n",
    "    pages = list_pages(did, prefer_hres=True)\n",
    "    rows.append({\n",
    "        \"doc_id\": did,\n",
    "        \"split\": get_split(did),\n",
    "        \"lang\": get_lang(did),\n",
    "        \"label_path\": str(lp),\n",
    "        \"n_pages\": len(pages),\n",
    "        \"page_paths\": [str(p) for p in pages],\n",
    "    })\n",
    "\n",
    "index_df = pd.DataFrame(rows)\n",
    "index_df.head(), index_df[\"split\"].value_counts(), index_df[\"lang\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0da24a-7ad8-4b07-afd6-65d05b22489a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages: 5 elements: 93 edges: 93\n",
      "first element keys: dict_keys(['box', 'text', 'page', 'label', 'linking', 'id', 'order'])\n",
      "first 3 edges: [(-1, 1), (-1, 2), (-1, 7)]\n"
     ]
    }
   ],
   "source": [
    "def load_label(label_path):\n",
    "    obj = json.loads(Path(label_path).read_text(encoding=\"utf-8\"))\n",
    "    pages_meta = obj[\"pages\"]          # dict: page1..pageN -> {width,height}\n",
    "    contents = obj[\"contents\"]         # list of elements\n",
    "    \n",
    "    # 1) sort by reading order\n",
    "    contents_sorted = sorted(contents, key=lambda x: x.get(\"order\", 10**9))\n",
    "    \n",
    "    # 2) collect edges from linking\n",
    "    edges = set()\n",
    "    for c in contents_sorted:\n",
    "        for pair in c.get(\"linking\", []):\n",
    "            if isinstance(pair, list) and len(pair) == 2:\n",
    "                edges.add((pair[0], pair[1]))\n",
    "    return pages_meta, contents_sorted, sorted(list(edges))\n",
    "\n",
    "# quick test\n",
    "row = index_df.iloc[0]\n",
    "pages_meta, elements, edges = load_label(row[\"label_path\"])\n",
    "print(\"pages:\", len(pages_meta), \"elements:\", len(elements), \"edges:\", len(edges))\n",
    "print(\"first element keys:\", elements[0].keys())\n",
    "print(\"first 3 edges:\", edges[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af9139e-8ea2-4c90-a566-2c5198563bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> -1\n",
      "2 -> -1\n",
      "7 -> -1\n",
      "8 -> -1\n",
      "16 -> -1\n",
      "53 -> -1\n",
      "66 -> -1\n",
      "64 -> 0\n",
      "65 -> 0\n",
      "92 -> 0\n"
     ]
    }
   ],
   "source": [
    "def build_parent_map(elements, edges, root_id=0):\n",
    "    # 先建 child->parent（若数据保证单父，就应唯一）\n",
    "    parent = {}\n",
    "    for p, c in edges:\n",
    "        # 如果出现多父，先保留第一个，同时统计冲突\n",
    "        if c in parent and parent[c] != p:\n",
    "            pass\n",
    "        else:\n",
    "            parent[c] = p\n",
    "    \n",
    "    # 确保每个 element 都有 parent（没有的挂到 root）\n",
    "    ids = [e[\"id\"] for e in elements]\n",
    "    for cid in ids:\n",
    "        if cid == root_id:\n",
    "            continue\n",
    "        if cid not in parent:\n",
    "            parent[cid] = root_id\n",
    "    return parent\n",
    "\n",
    "parent_map = build_parent_map(elements, edges, root_id=0)\n",
    "# 打印几个例子\n",
    "for cid in list(parent_map.keys())[:10]:\n",
    "    print(cid, \"->\", parent_map[cid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3da84d5-240e-43ec-be47-f0c1171fc383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 5 footer [508, 950, 647, 968]\n",
      "65 5 page-number [666, 950, 680, 966]\n",
      "92 6 page-number [318, 951, 336, 965]\n",
      "93 6 footer [348, 950, 486, 966]\n",
      "1 1 figure [322, 227, 623, 284]\n"
     ]
    }
   ],
   "source": [
    "def norm_box_0_1000(box, page_w, page_h):\n",
    "    x0, y0, x1, y1 = box\n",
    "    # 注意：y轴方向是否与图像一致，需要用可视化确认（下一步会做）\n",
    "    nx0 = int(1000 * x0 / page_w)\n",
    "    nx1 = int(1000 * x1 / page_w)\n",
    "    ny0 = int(1000 * y0 / page_h)\n",
    "    ny1 = int(1000 * y1 / page_h)\n",
    "    # clip\n",
    "    nx0, ny0 = max(0,nx0), max(0,ny0)\n",
    "    nx1, ny1 = min(1000,nx1), min(1000,ny1)\n",
    "    return [nx0, ny0, nx1, ny1]\n",
    "\n",
    "# test first 5 elements on their pages\n",
    "def page_size(pages_meta, page_num):\n",
    "    p = pages_meta[f\"page{page_num}\"]\n",
    "    return p[\"width\"], p[\"height\"]\n",
    "\n",
    "for e in elements[:5]:\n",
    "    w,h = page_size(pages_meta, e[\"page\"])\n",
    "    print(e[\"id\"], e[\"page\"], e[\"label\"], norm_box_0_1000(e[\"box\"], w, h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73533376-1226-48cd-b769-8b6c28ffcaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elements: 93\n",
      "{'elem_id': 64, 'page_id': 5, 'bbox': [508, 950, 647, 968], 'text': 'Living Without Food', 'label': 'footer', 'order': 0, 'linking': [[0, 64]]}\n",
      "{'elem_id': 65, 'page_id': 5, 'bbox': [666, 950, 680, 966], 'text': '3', 'label': 'page-number', 'order': 0, 'linking': [[0, 65]]}\n",
      "{'elem_id': 92, 'page_id': 6, 'bbox': [318, 951, 336, 965], 'text': '4', 'label': 'page-number', 'order': 0, 'linking': [[0, 92]]}\n",
      "{'elem_id': 93, 'page_id': 6, 'bbox': [348, 950, 486, 966], 'text': 'Living Without Food', 'label': 'footer', 'order': 0, 'linking': [[0, 93]]}\n",
      "{'elem_id': 1, 'page_id': 1, 'bbox': [322, 227, 623, 284], 'text': '', 'label': 'figure', 'order': 1, 'linking': [[-1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "def parse_document(label_path):\n",
    "    \"\"\"\n",
    "    返回：\n",
    "      pages_meta: dict(pageX -> {width,height})\n",
    "      elements: list of dict, 按 reading order 排好\n",
    "    \"\"\"\n",
    "    obj = json.loads(Path(label_path).read_text(encoding=\"utf-8\"))\n",
    "    pages_meta = obj[\"pages\"]\n",
    "    contents = obj[\"contents\"]\n",
    "\n",
    "    # 按阅读顺序排序（论文强调 traversal order）\n",
    "    contents = sorted(contents, key=lambda x: x.get(\"order\", 10**9))\n",
    "\n",
    "    elements = []\n",
    "    for c in contents:\n",
    "        page = c[\"page\"]\n",
    "        page_info = pages_meta[f\"page{page}\"]\n",
    "        pw, ph = page_info[\"width\"], page_info[\"height\"]\n",
    "\n",
    "        # box → 0–1000（不做 y 翻转）\n",
    "        x0, y0, x1, y1 = c[\"box\"]\n",
    "        bbox_1000 = [\n",
    "            int(1000 * x0 / pw),\n",
    "            int(1000 * y0 / ph),\n",
    "            int(1000 * x1 / pw),\n",
    "            int(1000 * y1 / ph),\n",
    "        ]\n",
    "\n",
    "        elements.append({\n",
    "            \"elem_id\": c[\"id\"],\n",
    "            \"page_id\": page,\n",
    "            \"bbox\": bbox_1000,\n",
    "            \"text\": c.get(\"text\", \"\"),\n",
    "            \"label\": c.get(\"label\", \"\"),\n",
    "            \"order\": c.get(\"order\", -1),\n",
    "            \"linking\": c.get(\"linking\", []),\n",
    "        })\n",
    "\n",
    "    return pages_meta, elements\n",
    "\n",
    "\n",
    "# sanity check\n",
    "row = index_df.iloc[0]\n",
    "pages_meta, elements = parse_document(row[\"label_path\"])\n",
    "print(\"elements:\", len(elements))\n",
    "for e in elements[:5]:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7719564f-f19f-49eb-99bd-c0f5207cc391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid parents (<0): []\n",
      "total elements: 93\n",
      "total parents: 93\n",
      "64 -> parent 0\n",
      "65 -> parent 0\n",
      "92 -> parent 0\n",
      "93 -> parent 0\n",
      "1 -> parent 0\n",
      "2 -> parent 0\n",
      "3 -> parent 2\n",
      "4 -> parent 2\n",
      "5 -> parent 2\n",
      "6 -> parent 2\n"
     ]
    }
   ],
   "source": [
    "# Cell G (fixed): build parent map and normalize parent ids\n",
    "\n",
    "def build_parent_map(elements, root_id=0):\n",
    "    \"\"\"\n",
    "    elements: list of dict, each must contain:\n",
    "        - 'id'\n",
    "        - 'linking'\n",
    "    return:\n",
    "        parent_map: dict(child_id -> parent_id), parent_id >= 0\n",
    "    \"\"\"\n",
    "    parent_map = {}\n",
    "\n",
    "    # 1. 从 linking 中提取 parent-child\n",
    "    for e in elements:\n",
    "        for pair in e.get(\"linking\", []):\n",
    "            if isinstance(pair, list) and len(pair) == 2:\n",
    "                p, c = pair\n",
    "                if c not in parent_map:\n",
    "                    parent_map[c] = p\n",
    "\n",
    "    # 2. 处理所有 element，补齐 parent\n",
    "    for e in elements:\n",
    "        cid = e[\"elem_id\"]\n",
    "\n",
    "        # 情况 A：完全没有 parent → 挂到 root\n",
    "        if cid not in parent_map:\n",
    "            parent_map[cid] = root_id\n",
    "\n",
    "        # 情况 B：标成 -1 → 映射到 root\n",
    "        if parent_map[cid] == -1:\n",
    "            parent_map[cid] = root_id\n",
    "\n",
    "    return parent_map\n",
    "\n",
    "\n",
    "# ====== sanity check ======\n",
    "parent_map = build_parent_map(elements)\n",
    "\n",
    "bad = [cid for cid, p in parent_map.items() if p < 0]\n",
    "print(\"invalid parents (<0):\", bad[:10])\n",
    "print(\"total elements:\", len(elements))\n",
    "print(\"total parents:\", len(parent_map))\n",
    "\n",
    "# 打印前几个看看\n",
    "for e in elements[:10]:\n",
    "    print(f\"{e['elem_id']} -> parent {parent_map[e['elem_id']]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c1f1c47-b074-49fd-99c7-2253f881fb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num elements: 93\n",
      "num candidates (ROOT + elems): 94\n",
      "bad parent refs (should be empty): []\n",
      "child  64 -> parent   0 | target_idx=0\n",
      "child  65 -> parent   0 | target_idx=0\n",
      "child  92 -> parent   0 | target_idx=0\n",
      "child  93 -> parent   0 | target_idx=0\n",
      "child   1 -> parent   0 | target_idx=0\n",
      "child   2 -> parent   0 | target_idx=0\n",
      "child   3 -> parent   2 | target_idx=6\n",
      "child   4 -> parent   2 | target_idx=6\n",
      "child   5 -> parent   2 | target_idx=6\n",
      "child   6 -> parent   2 | target_idx=6\n"
     ]
    }
   ],
   "source": [
    "# CELL H: build parent classification targets (indices)\n",
    "\n",
    "ROOT_ID = 0\n",
    "\n",
    "# 1) 候选 parent 列表（ROOT + 全部元素 id，保持 elements 的 reading order）\n",
    "elem_ids = [e[\"elem_id\"] for e in elements]\n",
    "candidate_parents = [ROOT_ID] + elem_ids\n",
    "\n",
    "# 2) 建立 id -> index\n",
    "id2idx = {pid: i for i, pid in enumerate(candidate_parents)}\n",
    "\n",
    "# 3) 生成每个元素的监督标签：parent 的 index\n",
    "parent_target_idx = []\n",
    "bad_refs = []\n",
    "\n",
    "for cid in elem_ids:\n",
    "    pid = parent_map[cid]          # parent id\n",
    "    if pid not in id2idx:\n",
    "        bad_refs.append((cid, pid))\n",
    "        parent_target_idx.append(None)\n",
    "    else:\n",
    "        parent_target_idx.append(id2idx[pid])\n",
    "\n",
    "print(\"num elements:\", len(elem_ids))\n",
    "print(\"num candidates (ROOT + elems):\", len(candidate_parents))\n",
    "print(\"bad parent refs (should be empty):\", bad_refs[:10])\n",
    "\n",
    "# 4) 打印前 10 个样本核对\n",
    "for i in range(min(10, len(elem_ids))):\n",
    "    cid = elem_ids[i]\n",
    "    pid = parent_map[cid]\n",
    "    print(f\"child {cid:>3} -> parent {pid:>3} | target_idx={parent_target_idx[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac1522f0-5f84-4dcf-b4db-993e8d9e7105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomra\\anaconda3\\envs\\hrdoc\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC_ID: 0a51k4mj12\n",
      "num elements: 192\n",
      "num chunks: 18\n",
      "chunk lens (first 10): [476, 408, 243, 276, 279, 506, 449, 475, 504, 494]\n",
      "max chunk len: 509\n"
     ]
    }
   ],
   "source": [
    "# CELL J: tokenizer + element-wise packing into chunks (max_tokens=512)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ====== 你需要改的参数 ======\n",
    "MODEL_NAME = \"bert-base-uncased\"   # 先用它跑通；之后换成 GeoLayoutLM 对应 tokenizer\n",
    "MAX_TOKENS = 512\n",
    "DOC_ID = \"0a51k4mj12\"\n",
    "# ===========================\n",
    "\n",
    "# 1) load doc sample (依赖你已有 parse_document / build_parent_map / build_parent_labels 那些 cell 的结果)\n",
    "label_path = DATA_DIR / \"labels\" / f\"{DOC_ID}.json\"\n",
    "obj = json.loads(label_path.read_text(encoding=\"utf-8\"))\n",
    "pages_meta = obj[\"pages\"]\n",
    "contents = sorted(obj[\"contents\"], key=lambda x: x.get(\"order\", 10**9))\n",
    "\n",
    "# 2) normalize boxes to 0-1000 per page (no y-flip)\n",
    "def box_to_1000(box, page_w, page_h):\n",
    "    x0, y0, x1, y1 = box\n",
    "    return [\n",
    "        int(1000 * x0 / page_w),\n",
    "        int(1000 * y0 / page_h),\n",
    "        int(1000 * x1 / page_w),\n",
    "        int(1000 * y1 / page_h),\n",
    "    ]\n",
    "\n",
    "elements = []\n",
    "for c in contents:\n",
    "    p = c[\"page\"]\n",
    "    pw, ph = pages_meta[f\"page{p}\"][\"width\"], pages_meta[f\"page{p}\"][\"height\"]\n",
    "    elements.append({\n",
    "        \"id\": c[\"id\"],\n",
    "        \"page\": p,\n",
    "        \"label\": c.get(\"label\", \"\"),\n",
    "        \"text\": c.get(\"text\", \"\"),\n",
    "        \"bbox\": box_to_1000(c[\"box\"], pw, ph),\n",
    "        \"linking\": c.get(\"linking\", []),\n",
    "        \"order\": c.get(\"order\", -1),\n",
    "    })\n",
    "\n",
    "# 3) build parent targets (你之前已验证过，这里简化直接复用逻辑)\n",
    "def build_parent_map(elements, root_id=0):\n",
    "    parent_map = {}\n",
    "    for e in elements:\n",
    "        for pair in e.get(\"linking\", []):\n",
    "            if isinstance(pair, list) and len(pair) == 2:\n",
    "                p, c = pair\n",
    "                if c not in parent_map:\n",
    "                    parent_map[c] = p\n",
    "    for e in elements:\n",
    "        cid = e[\"id\"]\n",
    "        if cid not in parent_map:\n",
    "            parent_map[cid] = root_id\n",
    "        if parent_map[cid] == -1:\n",
    "            parent_map[cid] = root_id\n",
    "    return parent_map\n",
    "\n",
    "parent_map = build_parent_map(elements, root_id=0)\n",
    "\n",
    "# 4) tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "# 5) pack elements into chunks\n",
    "chunks = []\n",
    "cur = {\"input_ids\": [], \"attention_mask\": [], \"bbox\": [], \"page_id\": [], \"elem_id\": [], \"inner_pos\": []}\n",
    "cur_len = 0\n",
    "\n",
    "def flush():\n",
    "    nonlocal_chunks.append(cur.copy())\n",
    "\n",
    "nonlocal_chunks = []\n",
    "for e in elements:\n",
    "    text = e[\"text\"] if e[\"text\"] is not None else \"\"\n",
    "    if text.strip() == \"\":\n",
    "        # 空文本也保留一个占位 token（避免丢元素）；用 tokenizer 的 unk/cls 可能不一致，这里用 [UNK] 文本占位\n",
    "        text = \"[UNK]\"\n",
    "\n",
    "    enc = tok(\n",
    "        text,\n",
    "        add_special_tokens=False,\n",
    "        return_attention_mask=True,\n",
    "        return_offsets_mapping=False,\n",
    "        truncation=False\n",
    "    )\n",
    "    ids = enc[\"input_ids\"]\n",
    "    am  = enc[\"attention_mask\"]\n",
    "\n",
    "    # inner-layout position：该元素内部 token 的相对位置（0..len-1）\n",
    "    inner = list(range(len(ids)))\n",
    "\n",
    "    # token-level boxes/page_id/elem_id：每个 token 复制该元素的 bbox/page/id\n",
    "    bxs = [e[\"bbox\"]] * len(ids)\n",
    "    pids = [e[\"page\"]] * len(ids)\n",
    "    eids = [e[\"id\"]]  * len(ids)\n",
    "\n",
    "    # 如果这个元素加进去会超 MAX_TOKENS，则先 flush 当前 chunk\n",
    "    if cur_len > 0 and cur_len + len(ids) > MAX_TOKENS:\n",
    "        nonlocal_chunks.append(cur)\n",
    "        cur = {\"input_ids\": [], \"attention_mask\": [], \"bbox\": [], \"page_id\": [], \"elem_id\": [], \"inner_pos\": []}\n",
    "        cur_len = 0\n",
    "\n",
    "    # 若单个元素本身就超过 MAX_TOKENS：硬截断（真实复现应更谨慎，但先跑通）\n",
    "    if len(ids) > MAX_TOKENS:\n",
    "        ids = ids[:MAX_TOKENS]\n",
    "        am  = am[:MAX_TOKENS]\n",
    "        inner = inner[:MAX_TOKENS]\n",
    "        bxs = bxs[:MAX_TOKENS]\n",
    "        pids = pids[:MAX_TOKENS]\n",
    "        eids = eids[:MAX_TOKENS]\n",
    "\n",
    "    cur[\"input_ids\"].extend(ids)\n",
    "    cur[\"attention_mask\"].extend(am)\n",
    "    cur[\"bbox\"].extend(bxs)\n",
    "    cur[\"page_id\"].extend(pids)\n",
    "    cur[\"elem_id\"].extend(eids)\n",
    "    cur[\"inner_pos\"].extend(inner)\n",
    "    cur_len += len(ids)\n",
    "\n",
    "# flush last\n",
    "if cur_len > 0:\n",
    "    nonlocal_chunks.append(cur)\n",
    "\n",
    "chunks = nonlocal_chunks\n",
    "\n",
    "print(\"DOC_ID:\", DOC_ID)\n",
    "print(\"num elements:\", len(elements))\n",
    "print(\"num chunks:\", len(chunks))\n",
    "print(\"chunk lens (first 10):\", [len(c[\"input_ids\"]) for c in chunks[:10]])\n",
    "print(\"max chunk len:\", max(len(c[\"input_ids\"]) for c in chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6884805a-5bb6-4d1f-9154-7201ec521533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num chunks: 18\n",
      "\n",
      "Chunk 0:\n",
      "  num elements in chunk: 40\n",
      "  first 5 elem_ids: [1, 2, 13, 14, 24]\n",
      "  first 5 token idx: [0, 14, 16, 30, 32]\n",
      "\n",
      "Chunk 1:\n",
      "  num elements in chunk: 8\n",
      "  first 5 elem_ids: [9, 10, 11, 12, 15]\n",
      "  first 5 token idx: [0, 93, 123, 234, 275]\n",
      "\n",
      "Chunk 2:\n",
      "  num elements in chunk: 4\n",
      "  first 5 elem_ids: [19, 20, 21, 22]\n",
      "  first 5 token idx: [0, 153, 161, 192]\n",
      "\n",
      "Pooling indices sanity check passed.\n"
     ]
    }
   ],
   "source": [
    "# CELL K: build element-level pooling indices per chunk\n",
    "\n",
    "# 输入假设：\n",
    "# chunks: list of dicts\n",
    "#   每个 chunk 包含：\n",
    "#     - input_ids\n",
    "#     - elem_id  (token-level element id)\n",
    "# elements: list of element dicts（按 reading order）\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def build_pooling_indices(chunks):\n",
    "    \"\"\"\n",
    "    返回：\n",
    "      pooling = list of dicts, 每个 dict 对应一个 chunk：\n",
    "        {\n",
    "          \"elem_ids\": [e1, e2, ...],              # 该 chunk 中出现的 element（按首次出现顺序）\n",
    "          \"first_token_idx\": [i1, i2, ...]        # 每个 element 在该 chunk 中的首 token index\n",
    "        }\n",
    "    \"\"\"\n",
    "    pooling = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        seen = OrderedDict()  # elem_id -> first token index\n",
    "        for idx, eid in enumerate(chunk[\"elem_id\"]):\n",
    "            if eid not in seen:\n",
    "                seen[eid] = idx\n",
    "\n",
    "        pooling.append({\n",
    "            \"elem_ids\": list(seen.keys()),\n",
    "            \"first_token_idx\": list(seen.values())\n",
    "        })\n",
    "\n",
    "    return pooling\n",
    "\n",
    "\n",
    "pooling = build_pooling_indices(chunks)\n",
    "\n",
    "# ====== sanity check ======\n",
    "print(\"num chunks:\", len(pooling))\n",
    "\n",
    "for i in range(min(3, len(pooling))):\n",
    "    p = pooling[i]\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(\"  num elements in chunk:\", len(p[\"elem_ids\"]))\n",
    "    print(\"  first 5 elem_ids:\", p[\"elem_ids\"][:5])\n",
    "    print(\"  first 5 token idx:\", p[\"first_token_idx\"][:5])\n",
    "\n",
    "# 检查：token index 一定 < chunk length\n",
    "for i, (p, c) in enumerate(zip(pooling, chunks)):\n",
    "    assert max(p[\"first_token_idx\"]) < len(c[\"input_ids\"]), f\"Index overflow in chunk {i}\"\n",
    "\n",
    "print(\"\\nPooling indices sanity check passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60584b2e-1864-4cbd-836f-9e4a87be7a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num document elements (no root): 192\n",
      "num decoder elements (with root): 193\n",
      "\n",
      "First 10 decoder element ids:\n",
      "[0, 1, 2, 13, 14, 24, 25, 35, 36, 48]\n",
      "\n",
      "Element 1 comes from (chunk_idx, token_idx): (0, 0)\n",
      "\n",
      "CELL L sanity check passed.\n"
     ]
    }
   ],
   "source": [
    "# CELL L: build document-level element sequence for decoder (with ROOT)\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def build_document_element_sequence(chunks, pooling):\n",
    "    \"\"\"\n",
    "    返回：\n",
    "      doc_elem_ids:        [e1, e2, ..., eM]         # 文档级 element 顺序（去重，按首次出现）\n",
    "      doc_elem_positions: dict elem_id -> (chunk_idx, token_idx)\n",
    "                           表示该 element 的 pooling token 位置\n",
    "    \"\"\"\n",
    "    doc_elem_positions = OrderedDict()\n",
    "\n",
    "    for chunk_idx, (chunk, pool) in enumerate(zip(chunks, pooling)):\n",
    "        for eid, tok_idx in zip(pool[\"elem_ids\"], pool[\"first_token_idx\"]):\n",
    "            # 只记录第一次出现的位置（reading order 保证正确）\n",
    "            if eid not in doc_elem_positions:\n",
    "                doc_elem_positions[eid] = (chunk_idx, tok_idx)\n",
    "\n",
    "    doc_elem_ids = list(doc_elem_positions.keys())\n",
    "    return doc_elem_ids, doc_elem_positions\n",
    "\n",
    "\n",
    "doc_elem_ids, doc_elem_positions = build_document_element_sequence(chunks, pooling)\n",
    "\n",
    "# ====== 加 ROOT ======\n",
    "ROOT_ID = 0\n",
    "decoder_elem_ids = [ROOT_ID] + doc_elem_ids   # decoder 输入顺序\n",
    "# ROOT 的 embedding 位置由模型内部 learnable embedding 提供，这里只占位\n",
    "\n",
    "# ====== sanity checks ======\n",
    "print(\"num document elements (no root):\", len(doc_elem_ids))\n",
    "print(\"num decoder elements (with root):\", len(decoder_elem_ids))\n",
    "\n",
    "# 检查：是否与原始 elements 数量一致\n",
    "orig_elem_ids = [e[\"id\"] for e in elements]\n",
    "assert set(doc_elem_ids) == set(orig_elem_ids), \"Element ID mismatch after merging chunks\"\n",
    "\n",
    "# 打印前几个核对顺序\n",
    "print(\"\\nFirst 10 decoder element ids:\")\n",
    "print(decoder_elem_ids[:10])\n",
    "\n",
    "# 打印一个 element 的 pooling 来源\n",
    "sample_eid = doc_elem_ids[0]\n",
    "print(f\"\\nElement {sample_eid} comes from (chunk_idx, token_idx):\",\n",
    "      doc_elem_positions[sample_eid])\n",
    "\n",
    "print(\"\\nCELL L sanity check passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa40df4c-18bb-4bc4-b81a-342c821baee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomra\\anaconda3\\envs\\hrdoc\\Lib\\site-packages\\transformers\\modeling_utils.py:1621: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 10.119296073913574\n",
      "logits shape: (192, 193)\n"
     ]
    }
   ],
   "source": [
    "# CELL O: wrap into an nn.Module (single-document), reuse existing prepared inputs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class DHFormerMini(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal DHFormer-like wrapper:\n",
    "      - real text-layout encoder (LayoutLMv3/GeoLayoutLM/etc.)\n",
    "      - element pooling by first-token\n",
    "      - root learnable embedding\n",
    "      - bilinear parent scorer\n",
    "    Input format:\n",
    "      chunks: list of dict with token-level fields\n",
    "      doc_elem_ids: list of element ids (no root)\n",
    "      doc_elem_positions: dict elem_id -> (chunk_idx, token_idx)\n",
    "      parent_map: dict child_elem_id -> parent_elem_id (root=0)\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder: nn.Module, hidden_size: int = 768, root_id: int = 0):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.hidden_size = hidden_size\n",
    "        self.root_id = root_id\n",
    "\n",
    "        self.root_emb = nn.Parameter(torch.zeros(hidden_size))\n",
    "        nn.init.normal_(self.root_emb, mean=0.0, std=0.02)\n",
    "\n",
    "        self.bilinear = nn.Bilinear(hidden_size, hidden_size, 1, bias=False)\n",
    "\n",
    "    def encode_chunks(self, chunks):\n",
    "        # returns list of [num_tokens, hidden]\n",
    "        outs = []\n",
    "        for c in chunks:\n",
    "            input_ids = torch.tensor(c[\"input_ids\"], dtype=torch.long, device=device).unsqueeze(0)\n",
    "            attention_mask = torch.tensor(c[\"attention_mask\"], dtype=torch.long, device=device).unsqueeze(0)\n",
    "            bbox = torch.tensor(c[\"bbox\"], dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "            o = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                bbox=bbox,\n",
    "                return_dict=True\n",
    "            )\n",
    "            outs.append(o.last_hidden_state.squeeze(0))\n",
    "        return outs\n",
    "\n",
    "    def forward(self, chunks, doc_elem_ids, doc_elem_positions, parent_map):\n",
    "        # 1) encoder forward\n",
    "        self.encoder.eval()  # keep eval by default for reproducibility; switch to train() in training loop\n",
    "        with torch.set_grad_enabled(self.training):\n",
    "            enc_outs = self.encode_chunks(chunks)\n",
    "\n",
    "        # 2) element pooling\n",
    "        elem_emb = {}\n",
    "        for eid, (chunk_idx, tok_idx) in doc_elem_positions.items():\n",
    "            elem_emb[eid] = enc_outs[chunk_idx][tok_idx]  # [H]\n",
    "\n",
    "        # 3) decoder input (ROOT + elements)\n",
    "        decoder_inputs = torch.stack(\n",
    "            [self.root_emb.to(device)] + [elem_emb[eid] for eid in doc_elem_ids],\n",
    "            dim=0\n",
    "        )  # [M+1, H]\n",
    "\n",
    "        # 4) build parent targets (indices)\n",
    "        decoder_elem_ids = [self.root_id] + doc_elem_ids\n",
    "        id2idx = {pid: i for i, pid in enumerate(decoder_elem_ids)}\n",
    "        parent_target = torch.tensor(\n",
    "            [id2idx[parent_map[eid]] for eid in doc_elem_ids],\n",
    "            dtype=torch.long,\n",
    "            device=device\n",
    "        )  # [M]\n",
    "\n",
    "        # 5) bilinear parent scoring: logits [M, M+1]\n",
    "        child_emb = decoder_inputs[1:]   # [M, H]\n",
    "        parent_emb = decoder_inputs      # [M+1, H]\n",
    "\n",
    "        logits = []\n",
    "        for i in range(child_emb.size(0)):\n",
    "            c = child_emb[i].unsqueeze(0).repeat(parent_emb.size(0), 1)\n",
    "            score = self.bilinear(c, parent_emb).squeeze(-1)\n",
    "            logits.append(score)\n",
    "        logits = torch.stack(logits, dim=0)\n",
    "\n",
    "        loss = F.cross_entropy(logits, parent_target)\n",
    "        return loss, logits\n",
    "\n",
    "\n",
    "# ====== instantiate encoder (use the same one you tested) ======\n",
    "MODEL_NAME = \"microsoft/layoutlmv3-base\"\n",
    "encoder = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "model = DHFormerMini(encoder=encoder, hidden_size=768, root_id=0).to(device)\n",
    "model.eval()\n",
    "\n",
    "# ====== quick forward test ======\n",
    "with torch.no_grad():\n",
    "    loss, logits = model(chunks, doc_elem_ids, doc_elem_positions, parent_map)\n",
    "\n",
    "print(\"loss:\", float(loss))\n",
    "print(\"logits shape:\", tuple(logits.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d108391-05bb-4615-9091-002f65db9f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id: u2v6z8fqbg\n",
      "num chunks: 13\n",
      "num elements: 105\n"
     ]
    }
   ],
   "source": [
    "# CELL Q: Dataset + DataLoader (batch_size=1)\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# ====== settings ======\n",
    "MODEL_NAME = \"microsoft/layoutlmv3-base\"   # tokenizer 与 encoder 对齐\n",
    "MAX_TOKENS = 512\n",
    "USE_HRES = True\n",
    "# ======================\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "class DocHieNetDocDataset(Dataset):\n",
    "    def __init__(self, index_df, split=\"train\"):\n",
    "        self.df = index_df[index_df[\"split\"] == split].reset_index(drop=True)\n",
    "        self.root_id = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    @staticmethod\n",
    "    def _box_to_1000(box, pw, ph):\n",
    "        x0, y0, x1, y1 = box\n",
    "        return [\n",
    "            int(1000 * x0 / pw),\n",
    "            int(1000 * y0 / ph),\n",
    "            int(1000 * x1 / pw),\n",
    "            int(1000 * y1 / ph),\n",
    "        ]\n",
    "\n",
    "    def _load_elements(self, label_path):\n",
    "        obj = json.loads(Path(label_path).read_text(encoding=\"utf-8\"))\n",
    "        pages_meta = obj[\"pages\"]\n",
    "        contents = sorted(obj[\"contents\"], key=lambda x: x.get(\"order\", 10**9))\n",
    "\n",
    "        elements = []\n",
    "        for c in contents:\n",
    "            p = c[\"page\"]\n",
    "            pw, ph = pages_meta[f\"page{p}\"][\"width\"], pages_meta[f\"page{p}\"][\"height\"]\n",
    "            elements.append({\n",
    "                \"id\": c[\"id\"],\n",
    "                \"page\": p,\n",
    "                \"text\": c.get(\"text\", \"\") or \"\",\n",
    "                \"bbox\": self._box_to_1000(c[\"box\"], pw, ph),\n",
    "                \"linking\": c.get(\"linking\", []),\n",
    "                \"order\": c.get(\"order\", -1),\n",
    "            })\n",
    "        return elements\n",
    "\n",
    "    def _build_parent_map(self, elements):\n",
    "        parent_map = {}\n",
    "        for e in elements:\n",
    "            for pair in e.get(\"linking\", []):\n",
    "                if isinstance(pair, list) and len(pair) == 2:\n",
    "                    p, c = pair\n",
    "                    if c not in parent_map:\n",
    "                        parent_map[c] = p\n",
    "        for e in elements:\n",
    "            cid = e[\"id\"]\n",
    "            if cid not in parent_map:\n",
    "                parent_map[cid] = self.root_id\n",
    "            if parent_map[cid] == -1:\n",
    "                parent_map[cid] = self.root_id\n",
    "        return parent_map\n",
    "\n",
    "    def _tokenize_and_chunk(self, elements):\n",
    "        chunks = []\n",
    "        cur = {\"input_ids\": [], \"attention_mask\": [], \"bbox\": [], \"elem_id\": [], \"page_id\": [], \"inner_pos\": []}\n",
    "        cur_len = 0\n",
    "\n",
    "        def flush():\n",
    "            nonlocal cur, cur_len\n",
    "            if cur_len > 0:\n",
    "                chunks.append(cur)\n",
    "            cur = {\"input_ids\": [], \"attention_mask\": [], \"bbox\": [], \"elem_id\": [], \"page_id\": [], \"inner_pos\": []}\n",
    "            cur_len = 0\n",
    "\n",
    "        for e in elements:\n",
    "            text = (e[\"text\"] or \"\").strip()\n",
    "            if text == \"\":\n",
    "                text = \"[UNK]\"\n",
    "\n",
    "            # LayoutLMv3TokenizerFast expects pretokenized words when text-only is provided\n",
    "            words = text.split()\n",
    "            if len(words) == 0:\n",
    "                words = [\"[UNK]\"]\n",
    "\n",
    "        # We only have element-level bbox, so we replicate it for each word\n",
    "            word_boxes = [e[\"bbox\"]] * len(words)\n",
    "\n",
    "            enc = tokenizer(\n",
    "                words,\n",
    "                boxes=word_boxes,\n",
    "                add_special_tokens=False,\n",
    "                return_attention_mask=True,\n",
    "                truncation=False\n",
    "            )\n",
    "\n",
    "            ids = enc[\"input_ids\"]\n",
    "            am  = enc[\"attention_mask\"]\n",
    "\n",
    "            # inner-layout positions: token position inside this element\n",
    "            inner = list(range(len(ids)))\n",
    "\n",
    "        # token-level metadata replicate\n",
    "            bxs = [e[\"bbox\"]] * len(ids)\n",
    "            eids = [e[\"id\"]]  * len(ids)\n",
    "            pids = [e[\"page\"]] * len(ids)\n",
    "\n",
    "            if cur_len > 0 and cur_len + len(ids) > MAX_TOKENS:\n",
    "                flush()\n",
    "\n",
    "            if len(ids) > MAX_TOKENS:\n",
    "                ids = ids[:MAX_TOKENS]; am = am[:MAX_TOKENS]\n",
    "                inner = inner[:MAX_TOKENS]\n",
    "                bxs = bxs[:MAX_TOKENS]; eids = eids[:MAX_TOKENS]; pids = pids[:MAX_TOKENS]\n",
    "\n",
    "            cur[\"input_ids\"].extend(ids)\n",
    "            cur[\"attention_mask\"].extend(am)\n",
    "            cur[\"bbox\"].extend(bxs)\n",
    "            cur[\"elem_id\"].extend(eids)\n",
    "            cur[\"page_id\"].extend(pids)\n",
    "            cur[\"inner_pos\"].extend(inner)\n",
    "            cur_len += len(ids)\n",
    "\n",
    "        flush()\n",
    "        return chunks\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_pooling(chunks):\n",
    "        from collections import OrderedDict\n",
    "        pooling = []\n",
    "        for c in chunks:\n",
    "            seen = OrderedDict()\n",
    "            for i, eid in enumerate(c[\"elem_id\"]):\n",
    "                if eid not in seen:\n",
    "                    seen[eid] = i\n",
    "            pooling.append({\"elem_ids\": list(seen.keys()), \"first_token_idx\": list(seen.values())})\n",
    "        return pooling\n",
    "\n",
    "    @staticmethod\n",
    "    def _merge_doc_positions(chunks, pooling):\n",
    "        from collections import OrderedDict\n",
    "        pos = OrderedDict()\n",
    "        for chunk_idx, pool in enumerate(pooling):\n",
    "            for eid, tok_idx in zip(pool[\"elem_ids\"], pool[\"first_token_idx\"]):\n",
    "                if eid not in pos:\n",
    "                    pos[eid] = (chunk_idx, tok_idx)\n",
    "        doc_elem_ids = list(pos.keys())\n",
    "        return doc_elem_ids, pos\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        doc_id = row[\"doc_id\"]\n",
    "        label_path = row[\"label_path\"]\n",
    "\n",
    "        elements = self._load_elements(label_path)\n",
    "        parent_map = self._build_parent_map(elements)\n",
    "\n",
    "        chunks = self._tokenize_and_chunk(elements)\n",
    "        pooling = self._build_pooling(chunks)\n",
    "        doc_elem_ids, doc_elem_positions = self._merge_doc_positions(chunks, pooling)\n",
    "\n",
    "        return {\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunks\": chunks,\n",
    "            \"doc_elem_ids\": doc_elem_ids,\n",
    "            \"doc_elem_positions\": doc_elem_positions,\n",
    "            \"parent_map\": parent_map,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch_size=1: 直接返回 dict\n",
    "    return batch[0]\n",
    "\n",
    "train_ds = DocHieNetDocDataset(index_df, split=\"train\")\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# quick iterate one batch\n",
    "b = next(iter(train_loader))\n",
    "print(\"doc_id:\", b[\"doc_id\"])\n",
    "print(\"num chunks:\", len(b[\"chunks\"]))\n",
    "print(\"num elements:\", len(b[\"doc_elem_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd2abae1-2a47-432b-b8a3-dba813011093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_ssa ready on cuda\n"
     ]
    }
   ],
   "source": [
    "# CELL U (REPLACE): SSA decoder with cached block-diag attn mask + model_ssa\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def sinusoidal_position_encoding(pos, dim):\n",
    "    pos = pos.float().unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, dim, 2, device=pos.device).float() * (-math.log(10000.0) / dim))\n",
    "    pe = torch.zeros(pos.size(0), dim, device=pos.device)\n",
    "    pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "    pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "    return pe\n",
    "\n",
    "# ====== KEY SPEED FIX: cache attention masks ======\n",
    "_ATTN_MASK_CACHE = {}\n",
    "\n",
    "def build_block_diag_attn_mask(seq_len: int, window: int, device):\n",
    "    \"\"\"\n",
    "    bool mask [L, L], True means masked (NOT allowed to attend)\n",
    "    cached by (L, window, device_type)\n",
    "    \"\"\"\n",
    "    key = (seq_len, window, device.type)\n",
    "    m = _ATTN_MASK_CACHE.get(key, None)\n",
    "    if m is not None:\n",
    "        return m\n",
    "    win_id = torch.arange(seq_len, device=device) // window\n",
    "    mask = ~(win_id.unsqueeze(0) == win_id.unsqueeze(1))  # True if different window => masked\n",
    "    _ATTN_MASK_CACHE[key] = mask\n",
    "    return mask\n",
    "\n",
    "class SSALayer(nn.Module):\n",
    "    def __init__(self, hidden_size=768, heads=8, ff=2048, dropout=0.1, window=48, shift=0):\n",
    "        super().__init__()\n",
    "        self.window = window\n",
    "        self.shift = shift\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.attn = nn.MultiheadAttention(hidden_size, heads, dropout=dropout, batch_first=True)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff, hidden_size),\n",
    "        )\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, L, H]\n",
    "        B, L, H = x.shape\n",
    "\n",
    "        if self.shift != 0:\n",
    "            x = torch.roll(x, shifts=-self.shift, dims=1)\n",
    "\n",
    "        attn_mask = build_block_diag_attn_mask(L, self.window, x.device)  # cached\n",
    "\n",
    "        q = self.norm1(x)\n",
    "        y, _ = self.attn(q, q, q, attn_mask=attn_mask, need_weights=False)\n",
    "        x = x + self.drop1(y)\n",
    "\n",
    "        if self.shift != 0:\n",
    "            x = torch.roll(x, shifts=self.shift, dims=1)\n",
    "\n",
    "        z = self.norm2(x)\n",
    "        z = self.ffn(z)\n",
    "        x = x + self.drop2(z)\n",
    "        return x\n",
    "\n",
    "class SSADecoder(nn.Module):\n",
    "    def __init__(self, hidden_size=768, heads=8, ff=2048, dropout=0.1, window=48, layers=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(layers):\n",
    "            shift = 0 if (i % 2 == 0) else (window // 2)\n",
    "            self.layers.append(SSALayer(hidden_size, heads, ff, dropout, window, shift))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class DHFormerWithSSA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: nn.Module,\n",
    "        hidden_size: int = 768,\n",
    "        root_id: int = 0,\n",
    "        max_inner_pos: int = 2048,\n",
    "        page_pe_dim: int = 128,\n",
    "        ssa_layers: int = 2,\n",
    "        ssa_heads: int = 8,\n",
    "        ssa_ff: int = 2048,\n",
    "        ssa_dropout: float = 0.1,\n",
    "        window: int = 48,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.hidden_size = hidden_size\n",
    "        self.root_id = root_id\n",
    "\n",
    "        self.root_emb = nn.Parameter(torch.zeros(hidden_size))\n",
    "        nn.init.normal_(self.root_emb, mean=0.0, std=0.02)\n",
    "\n",
    "        self.inner_emb = nn.Embedding(max_inner_pos, hidden_size)\n",
    "        self.page_proj = nn.Linear(page_pe_dim, hidden_size)\n",
    "        self.page_pe_dim = page_pe_dim\n",
    "\n",
    "        self.elem_decoder = SSADecoder(\n",
    "            hidden_size=hidden_size,\n",
    "            heads=ssa_heads,\n",
    "            ff=ssa_ff,\n",
    "            dropout=ssa_dropout,\n",
    "            window=window,\n",
    "            layers=ssa_layers\n",
    "        )\n",
    "\n",
    "        self.bilinear = nn.Bilinear(hidden_size, hidden_size, 1, bias=False)\n",
    "\n",
    "    def _encode_one_chunk(self, c):\n",
    "        input_ids = torch.tensor(c[\"input_ids\"], dtype=torch.long, device=device).unsqueeze(0)\n",
    "        attention_mask = torch.tensor(c[\"attention_mask\"], dtype=torch.long, device=device).unsqueeze(0)\n",
    "        bbox = torch.tensor(c[\"bbox\"], dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        o = self.encoder(input_ids=input_ids, attention_mask=attention_mask, bbox=bbox, return_dict=True)\n",
    "        hidden = o.last_hidden_state.squeeze(0)  # [T,H]\n",
    "\n",
    "        page_ids = torch.tensor(c[\"page_id\"], dtype=torch.long, device=device)\n",
    "        inner_pos = torch.tensor(c[\"inner_pos\"], dtype=torch.long, device=device)\n",
    "\n",
    "        pe = sinusoidal_position_encoding(page_ids, self.page_pe_dim)\n",
    "        page_e = self.page_proj(pe)\n",
    "\n",
    "        inner_pos = torch.clamp(inner_pos, 0, self.inner_emb.num_embeddings - 1)\n",
    "        inner_e = self.inner_emb(inner_pos)\n",
    "\n",
    "        return hidden + page_e + inner_e\n",
    "\n",
    "    def forward(self, chunks, doc_elem_ids, doc_elem_positions, parent_map):\n",
    "        enc_outs = [self._encode_one_chunk(c) for c in chunks]  # list [T,H]\n",
    "\n",
    "        elem_emb = {}\n",
    "        for eid, (chunk_idx, tok_idx) in doc_elem_positions.items():\n",
    "            elem_emb[eid] = enc_outs[chunk_idx][tok_idx]\n",
    "\n",
    "        elem_seq = torch.stack([self.root_emb] + [elem_emb[eid] for eid in doc_elem_ids], dim=0)  # [L,H]\n",
    "        elem_seq = elem_seq.unsqueeze(0)  # [1,L,H]\n",
    "\n",
    "        elem_ctx = self.elem_decoder(elem_seq).squeeze(0)  # [L,H]\n",
    "\n",
    "        decoder_elem_ids = [self.root_id] + doc_elem_ids\n",
    "        id2idx = {pid: i for i, pid in enumerate(decoder_elem_ids)}\n",
    "        parent_target = torch.tensor([id2idx[parent_map[eid]] for eid in doc_elem_ids],\n",
    "                                     dtype=torch.long, device=device)\n",
    "\n",
    "        child_emb = elem_ctx[1:]\n",
    "        parent_emb = elem_ctx\n",
    "\n",
    "        logits = []\n",
    "        for i in range(child_emb.size(0)):\n",
    "            c = child_emb[i].unsqueeze(0).repeat(parent_emb.size(0), 1)\n",
    "            logits.append(self.bilinear(c, parent_emb).squeeze(-1))\n",
    "        logits = torch.stack(logits, dim=0)\n",
    "\n",
    "        loss = F.cross_entropy(logits, parent_target)\n",
    "        return loss, logits\n",
    "\n",
    "# instantiate SSA model\n",
    "MODEL_NAME = \"microsoft/layoutlmv3-base\"\n",
    "encoder = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "model_ssa = DHFormerWithSSA(\n",
    "    encoder=encoder,\n",
    "    hidden_size=768,\n",
    "    root_id=0,\n",
    "    window=48,\n",
    "    ssa_layers=2,\n",
    "    ssa_heads=8,\n",
    "    ssa_ff=2048\n",
    ").to(device)\n",
    "\n",
    "print(\"model_ssa ready on\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a589cef-fbfa-4a9e-9243-64a80854dd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated docs: 161\n",
      "TP=611, Pred=29401, GT=29401\n",
      "Micro Precision=0.0208, Recall=0.0208, F1=0.0208\n"
     ]
    }
   ],
   "source": [
    "# CELL F1: evaluate relation-level Precision/Recall/F1 on test split for model_ssa\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def edges_from_parent_map(doc_elem_ids, parent_map, root_id=0):\n",
    "    # include root edges too (as paper's relation set); if you want \"exclude root edges\", you can filter later\n",
    "    return set((parent_map[eid], eid) for eid in doc_elem_ids)\n",
    "\n",
    "def edges_from_logits(doc_elem_ids, logits, root_id=0):\n",
    "    \"\"\"\n",
    "    logits: [M, M+1] where parents are [ROOT] + doc_elem_ids\n",
    "    returns set of (parent_id, child_id)\n",
    "    \"\"\"\n",
    "    parent_candidates = [root_id] + doc_elem_ids\n",
    "    pred_parent_idx = torch.argmax(logits, dim=1).tolist()  # len M\n",
    "    pred_edges = set()\n",
    "    for child_eid, pidx in zip(doc_elem_ids, pred_parent_idx):\n",
    "        pred_parent_id = parent_candidates[pidx]\n",
    "        pred_edges.add((pred_parent_id, child_eid))\n",
    "    return pred_edges\n",
    "\n",
    "def prf1(gt_edges, pred_edges):\n",
    "    inter = gt_edges & pred_edges\n",
    "    p = len(inter) / len(pred_edges) if pred_edges else 0.0\n",
    "    r = len(inter) / len(gt_edges) if gt_edges else 0.0\n",
    "    f1 = (2*p*r/(p+r)) if (p+r) else 0.0\n",
    "    return p, r, f1, len(inter)\n",
    "\n",
    "# build test loader\n",
    "test_ds = DocHieNetDocDataset(index_df, split=\"test\")\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model_ssa.eval()\n",
    "root_id = 0\n",
    "\n",
    "# accumulators\n",
    "tp_sum = 0\n",
    "pred_sum = 0\n",
    "gt_sum = 0\n",
    "\n",
    "# optional: limit for quick sanity; set to None to run full test\n",
    "LIMIT_DOCS = None  # e.g., 50 for quick run\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader, start=1):\n",
    "        loss, logits = model_ssa(\n",
    "            batch[\"chunks\"],\n",
    "            batch[\"doc_elem_ids\"],\n",
    "            batch[\"doc_elem_positions\"],\n",
    "            batch[\"parent_map\"]\n",
    "        )\n",
    "\n",
    "        gt = edges_from_parent_map(batch[\"doc_elem_ids\"], batch[\"parent_map\"], root_id=root_id)\n",
    "        pred = edges_from_logits(batch[\"doc_elem_ids\"], logits, root_id=root_id)\n",
    "\n",
    "        inter = gt & pred\n",
    "        tp_sum += len(inter)\n",
    "        pred_sum += len(pred)\n",
    "        gt_sum += len(gt)\n",
    "\n",
    "        if LIMIT_DOCS is not None and i >= LIMIT_DOCS:\n",
    "            break\n",
    "\n",
    "# micro P/R/F1\n",
    "precision = tp_sum / pred_sum if pred_sum else 0.0\n",
    "recall = tp_sum / gt_sum if gt_sum else 0.0\n",
    "f1 = (2*precision*recall/(precision+recall)) if (precision+recall) else 0.0\n",
    "\n",
    "print(f\"Evaluated docs: {i if LIMIT_DOCS is None else LIMIT_DOCS}\")\n",
    "print(f\"TP={tp_sum}, Pred={pred_sum}, GT={gt_sum}\")\n",
    "print(f\"Micro Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02509cfa-465c-4cdf-b19a-34c3ce3203fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched model_ssa: bbox will be clamped to [0,1000] before encoder.\n"
     ]
    }
   ],
   "source": [
    "# CELL FIX_BBOX: clamp bbox to [0,1000] before feeding into LayoutLMv3 encoder\n",
    "\n",
    "import types\n",
    "import torch\n",
    "import math\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def sinusoidal_position_encoding(pos, dim):\n",
    "    pos = pos.float().unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, dim, 2, device=pos.device).float() * (-math.log(10000.0) / dim))\n",
    "    pe = torch.zeros(pos.size(0), dim, device=pos.device)\n",
    "    pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "    pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "    return pe\n",
    "\n",
    "def _encode_one_chunk_clamped(self, c):\n",
    "    input_ids = torch.tensor(c[\"input_ids\"], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    attention_mask = torch.tensor(c[\"attention_mask\"], dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    bbox = torch.tensor(c[\"bbox\"], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    bbox = torch.clamp(bbox, 0, 1000)  # <<<关键修复：保证在 [0,1000]\n",
    "\n",
    "    o = self.encoder(input_ids=input_ids, attention_mask=attention_mask, bbox=bbox, return_dict=True)\n",
    "    hidden = o.last_hidden_state.squeeze(0)  # [T,H]\n",
    "\n",
    "    # 下面保持与你原来的 DHFormerWithSSA 一致：page + inner embedding injection\n",
    "    page_ids = torch.tensor(c[\"page_id\"], dtype=torch.long, device=device)\n",
    "    inner_pos = torch.tensor(c[\"inner_pos\"], dtype=torch.long, device=device)\n",
    "\n",
    "    pe = sinusoidal_position_encoding(page_ids, self.page_pe_dim)\n",
    "    page_e = self.page_proj(pe)\n",
    "\n",
    "    inner_pos = torch.clamp(inner_pos, 0, self.inner_emb.num_embeddings - 1)\n",
    "    inner_e = self.inner_emb(inner_pos)\n",
    "\n",
    "    return hidden + page_e + inner_e\n",
    "\n",
    "# 把补丁方法绑定到当前 model_ssa 实例\n",
    "model_ssa._encode_one_chunk = types.MethodType(_encode_one_chunk_clamped, model_ssa)\n",
    "\n",
    "print(\"Patched model_ssa: bbox will be clamped to [0,1000] before encoder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f4e10c8-0889-4872-b398-59734bcc286b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample: tdvwfwxc65 | chunks: 10 | elems: 107\n",
      "test sample:  02nk0izsev | chunks: 34 | elems: 55\n"
     ]
    }
   ],
   "source": [
    "# CELL CHUNKCAP_FIX: fix LayoutLMv3TokenizerFast usage (needs words list + boxes)\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "MODEL_NAME = \"microsoft/layoutlmv3-base\"\n",
    "MAX_TOKENS = 512\n",
    "MAX_CHUNKS_TRAIN = 32\n",
    "MAX_CHUNKS_TEST  = 128\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "class DocHieNetDocDataset(Dataset):\n",
    "    def __init__(self, index_df, split=\"train\"):\n",
    "        self.df = index_df[index_df[\"split\"] == split].reset_index(drop=True)\n",
    "        self.root_id = 0\n",
    "        self.split = split\n",
    "        self.max_chunks = MAX_CHUNKS_TRAIN if split == \"train\" else (MAX_CHUNKS_TEST if split == \"test\" else None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    @staticmethod\n",
    "    def _box_to_1000(box, pw, ph):\n",
    "        x0, y0, x1, y1 = box\n",
    "        return [\n",
    "            int(1000 * x0 / pw),\n",
    "            int(1000 * y0 / ph),\n",
    "            int(1000 * x1 / pw),\n",
    "            int(1000 * y1 / ph),\n",
    "        ]\n",
    "\n",
    "    def _load_elements(self, label_path):\n",
    "        obj = json.loads(Path(label_path).read_text(encoding=\"utf-8\"))\n",
    "        pages_meta = obj[\"pages\"]\n",
    "        contents = sorted(obj[\"contents\"], key=lambda x: x.get(\"order\", 10**9))\n",
    "\n",
    "        elements = []\n",
    "        for c in contents:\n",
    "            p = c[\"page\"]\n",
    "            pw, ph = pages_meta[f\"page{p}\"][\"width\"], pages_meta[f\"page{p}\"][\"height\"]\n",
    "            elements.append({\n",
    "                \"id\": c[\"id\"],\n",
    "                \"page\": p,\n",
    "                \"text\": (c.get(\"text\", \"\") or \"\"),\n",
    "                \"bbox\": self._box_to_1000(c[\"box\"], pw, ph),\n",
    "                \"linking\": c.get(\"linking\", []),\n",
    "                \"order\": c.get(\"order\", -1),\n",
    "            })\n",
    "        return elements\n",
    "\n",
    "    def _build_parent_map(self, elements):\n",
    "        parent_map = {}\n",
    "        for e in elements:\n",
    "            for pair in e.get(\"linking\", []):\n",
    "                if isinstance(pair, list) and len(pair) == 2:\n",
    "                    p, c = pair\n",
    "                    if c not in parent_map:\n",
    "                        parent_map[c] = p\n",
    "        for e in elements:\n",
    "            cid = e[\"id\"]\n",
    "            if cid not in parent_map:\n",
    "                parent_map[cid] = self.root_id\n",
    "            if parent_map[cid] == -1:\n",
    "                parent_map[cid] = self.root_id\n",
    "        return parent_map\n",
    "\n",
    "    def _tokenize_and_chunk(self, elements):\n",
    "        \"\"\"\n",
    "        LayoutLMv3 tokenizer requires:\n",
    "          - words: List[str]\n",
    "          - boxes: List[List[int]] (one box per word)\n",
    "        We'll assign each word the element bbox.\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        cur = {\"input_ids\": [], \"attention_mask\": [], \"bbox\": [], \"elem_id\": [], \"page_id\": [], \"inner_pos\": []}\n",
    "        cur_len = 0\n",
    "\n",
    "        def flush():\n",
    "            nonlocal cur, cur_len\n",
    "            if cur_len > 0:\n",
    "                chunks.append(cur)\n",
    "            cur = {\"input_ids\": [], \"attention_mask\": [], \"bbox\": [], \"elem_id\": [], \"page_id\": [], \"inner_pos\": []}\n",
    "            cur_len = 0\n",
    "\n",
    "        for e in elements:\n",
    "            text = e[\"text\"].strip()\n",
    "            words = text.split() if text else []\n",
    "            if len(words) == 0:\n",
    "                words = [\"[UNK]\"]\n",
    "\n",
    "            boxes = [e[\"bbox\"]] * len(words)\n",
    "\n",
    "            enc = tokenizer(\n",
    "                words,\n",
    "                boxes=boxes,\n",
    "                add_special_tokens=False,\n",
    "                return_attention_mask=True,\n",
    "                truncation=False\n",
    "            )\n",
    "\n",
    "            ids = enc[\"input_ids\"]\n",
    "            am  = enc[\"attention_mask\"]\n",
    "            bxs = enc[\"bbox\"]  # token-level bboxes aligned by tokenizer\n",
    "\n",
    "            # inner position within this element (token-level)\n",
    "            inner = list(range(len(ids)))\n",
    "            eids = [e[\"id\"]] * len(ids)\n",
    "            pids = [e[\"page\"]] * len(ids)\n",
    "\n",
    "            if cur_len > 0 and cur_len + len(ids) > MAX_TOKENS:\n",
    "                flush()\n",
    "\n",
    "            # safeguard: element too long\n",
    "            if len(ids) > MAX_TOKENS:\n",
    "                ids = ids[:MAX_TOKENS]; am = am[:MAX_TOKENS]; bxs = bxs[:MAX_TOKENS]\n",
    "                inner = inner[:MAX_TOKENS]; eids = eids[:MAX_TOKENS]; pids = pids[:MAX_TOKENS]\n",
    "\n",
    "            cur[\"input_ids\"].extend(ids)\n",
    "            cur[\"attention_mask\"].extend(am)\n",
    "            cur[\"bbox\"].extend(bxs)\n",
    "            cur[\"elem_id\"].extend(eids)\n",
    "            cur[\"page_id\"].extend(pids)\n",
    "            cur[\"inner_pos\"].extend(inner)\n",
    "            cur_len += len(ids)\n",
    "\n",
    "        flush()\n",
    "        return chunks\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_pooling(chunks):\n",
    "        pooling = []\n",
    "        for c in chunks:\n",
    "            seen = OrderedDict()\n",
    "            for i, eid in enumerate(c[\"elem_id\"]):\n",
    "                if eid not in seen:\n",
    "                    seen[eid] = i\n",
    "            pooling.append({\"elem_ids\": list(seen.keys()), \"first_token_idx\": list(seen.values())})\n",
    "        return pooling\n",
    "\n",
    "    @staticmethod\n",
    "    def _merge_doc_positions(chunks, pooling):\n",
    "        pos = OrderedDict()\n",
    "        for chunk_idx, pool in enumerate(pooling):\n",
    "            for eid, tok_idx in zip(pool[\"elem_ids\"], pool[\"first_token_idx\"]):\n",
    "                if eid not in pos:\n",
    "                    pos[eid] = (chunk_idx, tok_idx)\n",
    "        doc_elem_ids = list(pos.keys())\n",
    "        return doc_elem_ids, pos\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        doc_id = row[\"doc_id\"]\n",
    "        label_path = row[\"label_path\"]\n",
    "\n",
    "        elements = self._load_elements(label_path)\n",
    "        parent_map_full = self._build_parent_map(elements)\n",
    "\n",
    "        chunks = self._tokenize_and_chunk(elements)\n",
    "\n",
    "        # cap chunks per paper setting\n",
    "        if self.max_chunks is not None and len(chunks) > self.max_chunks:\n",
    "            chunks = chunks[:self.max_chunks]\n",
    "\n",
    "        pooling = self._build_pooling(chunks)\n",
    "        doc_elem_ids, doc_elem_positions = self._merge_doc_positions(chunks, pooling)\n",
    "\n",
    "        parent_map = {eid: parent_map_full[eid] for eid in doc_elem_ids}\n",
    "\n",
    "        return {\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunks\": chunks,\n",
    "            \"doc_elem_ids\": doc_elem_ids,\n",
    "            \"doc_elem_positions\": doc_elem_positions,\n",
    "            \"parent_map\": parent_map,\n",
    "            \"n_chunks\": len(chunks),\n",
    "            \"n_elems\": len(doc_elem_ids),\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return batch[0]\n",
    "\n",
    "train_ds = DocHieNetDocDataset(index_df, split=\"train\")\n",
    "test_ds  = DocHieNetDocDataset(index_df, split=\"test\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "b = next(iter(train_loader))\n",
    "print(\"train sample:\", b[\"doc_id\"], \"| chunks:\", b[\"n_chunks\"], \"| elems:\", b[\"n_elems\"])\n",
    "b2 = next(iter(test_loader))\n",
    "print(\"test sample: \", b2[\"doc_id\"], \"| chunks:\", b2[\"n_chunks\"], \"| elems:\", b2[\"n_elems\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4f21731-b0c5-4bb8-8ff9-4e7754fa7fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleanup done\n"
     ]
    }
   ],
   "source": [
    "# CLEANUP CELL: run once before training to avoid GPU memory fragmentation\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "for name in [\"model\", \"model_pe\", \"model_dec\", \"encoder\", \"model_ssa_old\", \"model_dec_old\"]:\n",
    "    if name in globals():\n",
    "        del globals()[name]\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"cleanup done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5066fc99-ae71-4f16-a7a3-d993078f2942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport time\\nimport math\\nimport random\\nimport numpy as np\\nimport torch\\nimport torch.optim as optim\\n\\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\nassert device == \"cuda\", \"你现在不是 CUDA，先确认 GPU 可用并让 model_ssa 在 cuda 上。\"\\n\\n# reproducibility (optional)\\nSEED = 42\\nrandom.seed(SEED); np.random.seed(SEED)\\ntorch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\\n\\nEPOCHS = 5               # 先用 5 确认速度/流程，没问题再改 100\\nLR_START = 4e-5\\nLR_END = 1e-6\\nWEIGHT_DECAY = 0.01\\nMAX_GRAD_NORM = 1.0\\nPRINT_EVERY_STEP = 20\\nSAVE_PATH = \"best_model_ssa.pt\"\\nbest_f1 = -1.0\\n\\nmodel_ssa.to(device)\\nmodel_ssa.train()\\n\\noptimizer = optim.AdamW(model_ssa.parameters(), lr=LR_START, weight_decay=WEIGHT_DECAY)\\ngamma = (LR_END / LR_START) ** (1.0 / max(EPOCHS - 1, 1))\\nscheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\\n\\nscaler = torch.cuda.amp.GradScaler(enabled=True)\\n\\ndef micro_f1_on_loader(model, loader, root_id=0):\\n    model.eval()\\n    tp_sum = pred_sum = gt_sum = 0\\n    with torch.no_grad():\\n        for batch in loader:\\n            with torch.cuda.amp.autocast(dtype=torch.float16):\\n                loss, logits = model(\\n                    batch[\"chunks\"],\\n                    batch[\"doc_elem_ids\"],\\n                    batch[\"doc_elem_positions\"],\\n                    batch[\"parent_map\"]\\n                )\\n            parent_candidates = [root_id] + batch[\"doc_elem_ids\"]\\n            pred_parent_idx = torch.argmax(logits, dim=1).tolist()\\n            pred_edges = set((parent_candidates[pidx], child_eid)\\n                             for child_eid, pidx in zip(batch[\"doc_elem_ids\"], pred_parent_idx))\\n            gt_edges = set((batch[\"parent_map\"][eid], eid) for eid in batch[\"doc_elem_ids\"])\\n            inter = gt_edges & pred_edges\\n            tp_sum += len(inter); pred_sum += len(pred_edges); gt_sum += len(gt_edges)\\n    p = tp_sum / pred_sum if pred_sum else 0.0\\n    r = tp_sum / gt_sum if gt_sum else 0.0\\n    f1 = (2*p*r/(p+r)) if (p+r) else 0.0\\n    model.train()\\n    return p, r, f1\\n\\nglobal_start = time.time()\\n\\nfor epoch in range(1, EPOCHS + 1):\\n    epoch_start = time.time()\\n    step_times = []\\n\\n    total_loss = 0.0\\n    steps = 0\\n\\n    for step, batch in enumerate(train_loader, start=1):\\n        t0 = time.time()\\n\\n        optimizer.zero_grad(set_to_none=True)\\n\\n        with torch.cuda.amp.autocast(dtype=torch.float16):\\n            loss, _ = model_ssa(\\n                batch[\"chunks\"],\\n                batch[\"doc_elem_ids\"],\\n                batch[\"doc_elem_positions\"],\\n                batch[\"parent_map\"]\\n            )\\n\\n        scaler.scale(loss).backward()\\n        scaler.unscale_(optimizer)\\n        torch.nn.utils.clip_grad_norm_(model_ssa.parameters(), MAX_GRAD_NORM)\\n        scaler.step(optimizer)\\n        scaler.update()\\n\\n        total_loss += float(loss.detach().cpu())\\n        steps += 1\\n\\n        dt = time.time() - t0\\n        step_times.append(dt)\\n\\n        if step % PRINT_EVERY_STEP == 0:\\n            avg_dt = sum(step_times[-PRINT_EVERY_STEP:]) / PRINT_EVERY_STEP\\n            print(f\"[epoch {epoch:03d} | step {step:04d}] loss={loss.item():.4f} | {avg_dt:.2f}s/step\")\\n\\n    avg_train_loss = total_loss / max(steps, 1)\\n\\n    eval_start = time.time()\\n    p, r, f1 = micro_f1_on_loader(model_ssa, test_loader, root_id=0)\\n    eval_time = time.time() - eval_start\\n\\n    # save best\\n    if f1 > best_f1:\\n        best_f1 = f1\\n        torch.save({\"epoch\": epoch, \"model_state_dict\": model_ssa.state_dict(), \"best_f1\": best_f1}, SAVE_PATH)\\n        tag = \" (best)\"\\n    else:\\n        tag = \"\"\\n\\n    epoch_time = time.time() - epoch_start\\n    elapsed = time.time() - global_start\\n    avg_epoch = elapsed / epoch\\n    eta = avg_epoch * (EPOCHS - epoch)\\n    lr = optimizer.param_groups[0][\"lr\"]\\n\\n    print(\\n        f\"\\n[epoch {epoch:03d}/{EPOCHS}] lr={lr:.2e} | train_loss={avg_train_loss:.4f} | test_F1={f1:.4f}{tag}\\n\"\\n        f\"  epoch_time={epoch_time/60:.1f} min | eval_time={eval_time:.1f}s | ETA={eta/60:.1f} min\\n\"\\n    )\\n\\n    scheduler.step()\\n\\nprint(\"done. best_f1=\", best_f1, \"saved:\", SAVE_PATH)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAIN CELL (REPLACE): AMP + step timing + epoch timing + ETA\n",
    "'''\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert device == \"cuda\", \"你现在不是 CUDA，先确认 GPU 可用并让 model_ssa 在 cuda 上。\"\n",
    "\n",
    "# reproducibility (optional)\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "EPOCHS = 5               # 先用 5 确认速度/流程，没问题再改 100\n",
    "LR_START = 4e-5\n",
    "LR_END = 1e-6\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAX_GRAD_NORM = 1.0\n",
    "PRINT_EVERY_STEP = 20\n",
    "SAVE_PATH = \"best_model_ssa.pt\"\n",
    "best_f1 = -1.0\n",
    "\n",
    "model_ssa.to(device)\n",
    "model_ssa.train()\n",
    "\n",
    "optimizer = optim.AdamW(model_ssa.parameters(), lr=LR_START, weight_decay=WEIGHT_DECAY)\n",
    "gamma = (LR_END / LR_START) ** (1.0 / max(EPOCHS - 1, 1))\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "def micro_f1_on_loader(model, loader, root_id=0):\n",
    "    model.eval()\n",
    "    tp_sum = pred_sum = gt_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                loss, logits = model(\n",
    "                    batch[\"chunks\"],\n",
    "                    batch[\"doc_elem_ids\"],\n",
    "                    batch[\"doc_elem_positions\"],\n",
    "                    batch[\"parent_map\"]\n",
    "                )\n",
    "            parent_candidates = [root_id] + batch[\"doc_elem_ids\"]\n",
    "            pred_parent_idx = torch.argmax(logits, dim=1).tolist()\n",
    "            pred_edges = set((parent_candidates[pidx], child_eid)\n",
    "                             for child_eid, pidx in zip(batch[\"doc_elem_ids\"], pred_parent_idx))\n",
    "            gt_edges = set((batch[\"parent_map\"][eid], eid) for eid in batch[\"doc_elem_ids\"])\n",
    "            inter = gt_edges & pred_edges\n",
    "            tp_sum += len(inter); pred_sum += len(pred_edges); gt_sum += len(gt_edges)\n",
    "    p = tp_sum / pred_sum if pred_sum else 0.0\n",
    "    r = tp_sum / gt_sum if gt_sum else 0.0\n",
    "    f1 = (2*p*r/(p+r)) if (p+r) else 0.0\n",
    "    model.train()\n",
    "    return p, r, f1\n",
    "\n",
    "global_start = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start = time.time()\n",
    "    step_times = []\n",
    "\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        t0 = time.time()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            loss, _ = model_ssa(\n",
    "                batch[\"chunks\"],\n",
    "                batch[\"doc_elem_ids\"],\n",
    "                batch[\"doc_elem_positions\"],\n",
    "                batch[\"parent_map\"]\n",
    "            )\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model_ssa.parameters(), MAX_GRAD_NORM)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += float(loss.detach().cpu())\n",
    "        steps += 1\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        step_times.append(dt)\n",
    "\n",
    "        if step % PRINT_EVERY_STEP == 0:\n",
    "            avg_dt = sum(step_times[-PRINT_EVERY_STEP:]) / PRINT_EVERY_STEP\n",
    "            print(f\"[epoch {epoch:03d} | step {step:04d}] loss={loss.item():.4f} | {avg_dt:.2f}s/step\")\n",
    "\n",
    "    avg_train_loss = total_loss / max(steps, 1)\n",
    "\n",
    "    eval_start = time.time()\n",
    "    p, r, f1 = micro_f1_on_loader(model_ssa, test_loader, root_id=0)\n",
    "    eval_time = time.time() - eval_start\n",
    "\n",
    "    # save best\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        torch.save({\"epoch\": epoch, \"model_state_dict\": model_ssa.state_dict(), \"best_f1\": best_f1}, SAVE_PATH)\n",
    "        tag = \" (best)\"\n",
    "    else:\n",
    "        tag = \"\"\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    elapsed = time.time() - global_start\n",
    "    avg_epoch = elapsed / epoch\n",
    "    eta = avg_epoch * (EPOCHS - epoch)\n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    print(\n",
    "        f\"\\n[epoch {epoch:03d}/{EPOCHS}] lr={lr:.2e} | train_loss={avg_train_loss:.4f} | test_F1={f1:.4f}{tag}\\n\"\n",
    "        f\"  epoch_time={epoch_time/60:.1f} min | eval_time={eval_time:.1f}s | ETA={eta/60:.1f} min\\n\"\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"done. best_f1=\", best_f1, \"saved:\", SAVE_PATH)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd0a4752-df27-4940-8db7-4dedd961b63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomra\\AppData\\Local\\Temp\\ipykernel_127652\\3041415095.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(CKPT_PATH, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: best_model_ssa.pt | epoch: 2 | best_f1: 0.18653653826257888\n",
      "[ALL]    TP=5459 Pred=28997 GT=28997 | P=0.1883 R=0.1883 F1=0.1883\n",
      "[NONROOT] TP=767 Pred=16604 GT=23646 | P=0.0462 R=0.0324 F1=0.0381\n"
     ]
    }
   ],
   "source": [
    "# CELL EVALBEST: load best checkpoint and evaluate test micro-F1 (all edges / non-root edges)\n",
    "\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CKPT_PATH = \"best_model_ssa.pt\"\n",
    "ROOT_ID = 0\n",
    "\n",
    "# 1) load checkpoint\n",
    "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "model_ssa.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model_ssa.to(device)\n",
    "model_ssa.eval()\n",
    "\n",
    "print(\"Loaded:\", CKPT_PATH, \"| epoch:\", ckpt.get(\"epoch\"), \"| best_f1:\", ckpt.get(\"best_f1\"))\n",
    "\n",
    "def micro_prf(model, loader, exclude_root=False, root_id=0):\n",
    "    tp = pred_n = gt_n = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            _, logits = model(\n",
    "                batch[\"chunks\"],\n",
    "                batch[\"doc_elem_ids\"],\n",
    "                batch[\"doc_elem_positions\"],\n",
    "                batch[\"parent_map\"]\n",
    "            )\n",
    "\n",
    "            parent_candidates = [root_id] + batch[\"doc_elem_ids\"]\n",
    "            pred_parent_idx = torch.argmax(logits, dim=1).tolist()\n",
    "\n",
    "            pred_edges = set((parent_candidates[pidx], child_eid)\n",
    "                             for child_eid, pidx in zip(batch[\"doc_elem_ids\"], pred_parent_idx))\n",
    "            gt_edges = set((batch[\"parent_map\"][eid], eid) for eid in batch[\"doc_elem_ids\"])\n",
    "\n",
    "            if exclude_root:\n",
    "                pred_edges = set(e for e in pred_edges if e[0] != root_id)\n",
    "                gt_edges = set(e for e in gt_edges if e[0] != root_id)\n",
    "\n",
    "            inter = pred_edges & gt_edges\n",
    "            tp += len(inter)\n",
    "            pred_n += len(pred_edges)\n",
    "            gt_n += len(gt_edges)\n",
    "\n",
    "    p = tp / pred_n if pred_n else 0.0\n",
    "    r = tp / gt_n if gt_n else 0.0\n",
    "    f1 = (2*p*r/(p+r)) if (p+r) else 0.0\n",
    "    return p, r, f1, tp, pred_n, gt_n\n",
    "\n",
    "# 2) evaluate\n",
    "p, r, f1, tp, pred_n, gt_n = micro_prf(model_ssa, test_loader, exclude_root=False, root_id=ROOT_ID)\n",
    "print(f\"[ALL]    TP={tp} Pred={pred_n} GT={gt_n} | P={p:.4f} R={r:.4f} F1={f1:.4f}\")\n",
    "\n",
    "p2, r2, f12, tp2, pred_n2, gt_n2 = micro_prf(model_ssa, test_loader, exclude_root=True, root_id=ROOT_ID)\n",
    "print(f\"[NONROOT] TP={tp2} Pred={pred_n2} GT={gt_n2} | P={p2:.4f} R={r2:.4f} F1={f12:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfbc07bb-99eb-4cac-965f-61ebcf5fdb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test docs: 161\n",
      "Avg TEDS (semantic-label): 0.4589\n"
     ]
    }
   ],
   "source": [
    "# CELL TEDS_LABEL: semantic-label TEDS via zss (closer to paper than id-sensitive)\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import zss\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"zss\"])\n",
    "    import zss\n",
    "\n",
    "ROOT_ID = 0\n",
    "\n",
    "def build_children_ordered(doc_elem_ids, parent_map, root_id=0):\n",
    "    order = {eid: i for i, eid in enumerate([root_id] + doc_elem_ids)}\n",
    "    children = {root_id: []}\n",
    "    for eid in doc_elem_ids:\n",
    "        children.setdefault(eid, [])\n",
    "    for child in doc_elem_ids:\n",
    "        par = parent_map.get(child, root_id)\n",
    "        children.setdefault(par, [])\n",
    "        children[par].append(child)\n",
    "    for k in children:\n",
    "        children[k].sort(key=lambda x: order.get(x, 10**9))\n",
    "    return children\n",
    "\n",
    "def to_zss_tree(children, node_id, label_map):\n",
    "    node = zss.Node(label_map.get(node_id, \"UNK\"))\n",
    "    for ch in children.get(node_id, []):\n",
    "        node.addkid(to_zss_tree(children, ch, label_map))\n",
    "    return node\n",
    "\n",
    "def predict_parent_map_from_logits(doc_elem_ids, logits, root_id=0):\n",
    "    parent_candidates = [root_id] + doc_elem_ids\n",
    "    pred_parent_idx = torch.argmax(logits, dim=1).tolist()\n",
    "    return {child: parent_candidates[pidx] for child, pidx in zip(doc_elem_ids, pred_parent_idx)}\n",
    "\n",
    "def compute_semantic_teds_one(doc_elem_ids, gt_parent_map, pred_parent_map, elem_label_map, root_id=0):\n",
    "    gt_children = build_children_ordered(doc_elem_ids, gt_parent_map, root_id=root_id)\n",
    "    pr_children = build_children_ordered(doc_elem_ids, pred_parent_map, root_id=root_id)\n",
    "\n",
    "    # label map: ROOT + element semantic class\n",
    "    label_map = {root_id: \"ROOT\"}\n",
    "    for eid in doc_elem_ids:\n",
    "        label_map[eid] = elem_label_map.get(eid, \"UNK\")\n",
    "\n",
    "    gt_root = to_zss_tree(gt_children, root_id, label_map)\n",
    "    pr_root = to_zss_tree(pr_children, root_id, label_map)\n",
    "\n",
    "    def insert_cost(n): return 1\n",
    "    def remove_cost(n): return 1\n",
    "    def update_cost(a, b): return 0 if a.label == b.label else 1\n",
    "\n",
    "    ed = zss.distance(\n",
    "        gt_root, pr_root,\n",
    "        get_children=zss.Node.get_children,\n",
    "        insert_cost=insert_cost,\n",
    "        remove_cost=remove_cost,\n",
    "        update_cost=update_cost\n",
    "    )\n",
    "\n",
    "    # size: include root + nodes\n",
    "    denom = 1 + len(doc_elem_ids)\n",
    "    return 1.0 - (ed / denom)\n",
    "\n",
    "# ---- Evaluate on test ----\n",
    "model_ssa.eval()\n",
    "scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # IMPORTANT: we need semantic labels per element id.\n",
    "        # batch currently doesn't include them, so we approximate by re-reading label file via dataset logic:\n",
    "        # simplest: use the fact that your dataset was built from label json; here we re-open it from index_df.\n",
    "        # We'll build elem_label_map by scanning the original label json once per doc.\n",
    "\n",
    "        doc_id = batch[\"doc_id\"]\n",
    "        # locate label path from index_df\n",
    "        row = index_df[index_df[\"doc_id\"] == doc_id].iloc[0]\n",
    "        label_path = row[\"label_path\"]\n",
    "\n",
    "        obj = json.loads(Path(label_path).read_text(encoding=\"utf-8\"))\n",
    "        elem_label_map = {c[\"id\"]: c.get(\"label\", \"UNK\") for c in obj[\"contents\"]}\n",
    "\n",
    "        _, logits = model_ssa(\n",
    "            batch[\"chunks\"],\n",
    "            batch[\"doc_elem_ids\"],\n",
    "            batch[\"doc_elem_positions\"],\n",
    "            batch[\"parent_map\"]\n",
    "        )\n",
    "\n",
    "        pred_parent_map = predict_parent_map_from_logits(batch[\"doc_elem_ids\"], logits, root_id=ROOT_ID)\n",
    "\n",
    "        scores.append(\n",
    "            compute_semantic_teds_one(\n",
    "                doc_elem_ids=batch[\"doc_elem_ids\"],\n",
    "                gt_parent_map=batch[\"parent_map\"],\n",
    "                pred_parent_map=pred_parent_map,\n",
    "                elem_label_map=elem_label_map,\n",
    "                root_id=ROOT_ID\n",
    "            )\n",
    "        )\n",
    "\n",
    "avg_teds = sum(scores) / max(len(scores), 1)\n",
    "print(f\"Test docs: {len(scores)}\")\n",
    "print(f\"Avg TEDS (semantic-label): {avg_teds:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e25bd830-6409-4538-abf4-d221f02b448e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: w6praprwst | chunks: 14 | elems: 75\n",
      "parents missing after fix: 0\n"
     ]
    }
   ],
   "source": [
    "# CELL PARENTMAP_FIX: after chunk truncation, remap missing parents to ROOT to avoid KeyError\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "MODEL_NAME = \"microsoft/layoutlmv3-base\"\n",
    "MAX_TOKENS = 512\n",
    "\n",
    "# you can set these to your current speed config\n",
    "MAX_CHUNKS_TRAIN = 16   # e.g., 16 for RTX 4060 8GB\n",
    "MAX_CHUNKS_TEST  = 64   # e.g., 64 (can raise later)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "class DocHieNetDocDataset(Dataset):\n",
    "    def __init__(self, index_df, split=\"train\"):\n",
    "        self.df = index_df[index_df[\"split\"] == split].reset_index(drop=True)\n",
    "        self.root_id = 0\n",
    "        self.split = split\n",
    "        if split == \"train\":\n",
    "            self.max_chunks = MAX_CHUNKS_TRAIN\n",
    "        elif split == \"test\":\n",
    "            self.max_chunks = MAX_CHUNKS_TEST\n",
    "        else:\n",
    "            self.max_chunks = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    @staticmethod\n",
    "    def _box_to_1000(box, pw, ph):\n",
    "        x0, y0, x1, y1 = box\n",
    "        return [\n",
    "            int(1000 * x0 / pw),\n",
    "            int(1000 * y0 / ph),\n",
    "            int(1000 * x1 / pw),\n",
    "            int(1000 * y1 / ph),\n",
    "        ]\n",
    "\n",
    "    def _load_elements(self, label_path):\n",
    "        obj = json.loads(Path(label_path).read_text(encoding=\"utf-8\"))\n",
    "        pages_meta = obj[\"pages\"]\n",
    "        contents = sorted(obj[\"contents\"], key=lambda x: x.get(\"order\", 10**9))\n",
    "        elements = []\n",
    "        for c in contents:\n",
    "            p = c[\"page\"]\n",
    "            pw, ph = pages_meta[f\"page{p}\"][\"width\"], pages_meta[f\"page{p}\"][\"height\"]\n",
    "            elements.append({\n",
    "                \"id\": c[\"id\"],\n",
    "                \"page\": p,\n",
    "                \"text\": (c.get(\"text\", \"\") or \"\"),\n",
    "                \"bbox\": self._box_to_1000(c[\"box\"], pw, ph),\n",
    "                \"linking\": c.get(\"linking\", []),\n",
    "                \"order\": c.get(\"order\", -1),\n",
    "            })\n",
    "        return elements\n",
    "\n",
    "    def _build_parent_map(self, elements):\n",
    "        parent_map = {}\n",
    "        for e in elements:\n",
    "            for pair in e.get(\"linking\", []):\n",
    "                if isinstance(pair, list) and len(pair) == 2:\n",
    "                    p, c = pair\n",
    "                    if c not in parent_map:\n",
    "                        parent_map[c] = p\n",
    "        for e in elements:\n",
    "            cid = e[\"id\"]\n",
    "            if cid not in parent_map:\n",
    "                parent_map[cid] = self.root_id\n",
    "            if parent_map[cid] == -1:\n",
    "                parent_map[cid] = self.root_id\n",
    "        return parent_map\n",
    "\n",
    "    def _tokenize_and_chunk(self, elements):\n",
    "        chunks = []\n",
    "        cur = {\"input_ids\": [], \"attention_mask\": [], \"bbox\": [], \"elem_id\": [], \"page_id\": [], \"inner_pos\": []}\n",
    "        cur_len = 0\n",
    "\n",
    "        def flush():\n",
    "            nonlocal cur, cur_len\n",
    "            if cur_len > 0:\n",
    "                chunks.append(cur)\n",
    "            cur = {\"input_ids\": [], \"attention_mask\": [], \"bbox\": [], \"elem_id\": [], \"page_id\": [], \"inner_pos\": []}\n",
    "            cur_len = 0\n",
    "\n",
    "        for e in elements:\n",
    "            text = e[\"text\"].strip()\n",
    "            words = text.split() if text else []\n",
    "            if len(words) == 0:\n",
    "                words = [\"[UNK]\"]\n",
    "            boxes = [e[\"bbox\"]] * len(words)\n",
    "\n",
    "            enc = tokenizer(\n",
    "                words,\n",
    "                boxes=boxes,\n",
    "                add_special_tokens=False,\n",
    "                return_attention_mask=True,\n",
    "                truncation=False\n",
    "            )\n",
    "\n",
    "            ids = enc[\"input_ids\"]\n",
    "            am  = enc[\"attention_mask\"]\n",
    "            bxs = enc[\"bbox\"]\n",
    "\n",
    "            inner = list(range(len(ids)))\n",
    "            eids = [e[\"id\"]] * len(ids)\n",
    "            pids = [e[\"page\"]] * len(ids)\n",
    "\n",
    "            if cur_len > 0 and cur_len + len(ids) > MAX_TOKENS:\n",
    "                flush()\n",
    "\n",
    "            if len(ids) > MAX_TOKENS:\n",
    "                ids = ids[:MAX_TOKENS]; am = am[:MAX_TOKENS]; bxs = bxs[:MAX_TOKENS]\n",
    "                inner = inner[:MAX_TOKENS]; eids = eids[:MAX_TOKENS]; pids = pids[:MAX_TOKENS]\n",
    "\n",
    "            cur[\"input_ids\"].extend(ids)\n",
    "            cur[\"attention_mask\"].extend(am)\n",
    "            cur[\"bbox\"].extend(bxs)\n",
    "            cur[\"elem_id\"].extend(eids)\n",
    "            cur[\"page_id\"].extend(pids)\n",
    "            cur[\"inner_pos\"].extend(inner)\n",
    "            cur_len += len(ids)\n",
    "\n",
    "        flush()\n",
    "        return chunks\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_pooling(chunks):\n",
    "        pooling = []\n",
    "        for c in chunks:\n",
    "            seen = OrderedDict()\n",
    "            for i, eid in enumerate(c[\"elem_id\"]):\n",
    "                if eid not in seen:\n",
    "                    seen[eid] = i\n",
    "            pooling.append({\"elem_ids\": list(seen.keys()), \"first_token_idx\": list(seen.values())})\n",
    "        return pooling\n",
    "\n",
    "    @staticmethod\n",
    "    def _merge_doc_positions(chunks, pooling):\n",
    "        pos = OrderedDict()\n",
    "        for chunk_idx, pool in enumerate(pooling):\n",
    "            for eid, tok_idx in zip(pool[\"elem_ids\"], pool[\"first_token_idx\"]):\n",
    "                if eid not in pos:\n",
    "                    pos[eid] = (chunk_idx, tok_idx)\n",
    "        doc_elem_ids = list(pos.keys())\n",
    "        return doc_elem_ids, pos\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        doc_id = row[\"doc_id\"]\n",
    "        label_path = row[\"label_path\"]\n",
    "\n",
    "        elements = self._load_elements(label_path)\n",
    "        parent_map_full = self._build_parent_map(elements)\n",
    "\n",
    "        chunks = self._tokenize_and_chunk(elements)\n",
    "\n",
    "        # cap chunks\n",
    "        if self.max_chunks is not None and len(chunks) > self.max_chunks:\n",
    "            chunks = chunks[:self.max_chunks]\n",
    "\n",
    "        pooling = self._build_pooling(chunks)\n",
    "        doc_elem_ids, doc_elem_positions = self._merge_doc_positions(chunks, pooling)\n",
    "\n",
    "        kept = set(doc_elem_ids)\n",
    "        root = self.root_id\n",
    "\n",
    "        # --- FIX: if parent not in kept, remap to ROOT ---\n",
    "        parent_map = {}\n",
    "        for eid in doc_elem_ids:\n",
    "            p = parent_map_full.get(eid, root)\n",
    "            if p == -1:\n",
    "                p = root\n",
    "            if p != root and p not in kept:\n",
    "                p = root\n",
    "            if p == eid:  # guard self-loop\n",
    "                p = root\n",
    "            parent_map[eid] = p\n",
    "\n",
    "        return {\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunks\": chunks,\n",
    "            \"doc_elem_ids\": doc_elem_ids,\n",
    "            \"doc_elem_positions\": doc_elem_positions,\n",
    "            \"parent_map\": parent_map,\n",
    "            \"n_chunks\": len(chunks),\n",
    "            \"n_elems\": len(doc_elem_ids),\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return batch[0]\n",
    "\n",
    "train_ds = DocHieNetDocDataset(index_df, split=\"train\")\n",
    "test_ds  = DocHieNetDocDataset(index_df, split=\"test\")\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# quick sanity: try one forward to ensure no KeyError\n",
    "b = next(iter(train_loader))\n",
    "print(\"sample:\", b[\"doc_id\"], \"| chunks:\", b[\"n_chunks\"], \"| elems:\", b[\"n_elems\"])\n",
    "print(\"parents missing after fix:\",\n",
    "      sum(1 for eid in b[\"doc_elem_ids\"] if b[\"parent_map\"][eid] not in set([0]+b[\"doc_elem_ids\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4ecfd3-54c2-4a5d-8cf8-1e79ed944569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN CELL (FINAL, REPLACE): AMP + grad accumulation + warmup/decay + step/epoch timing + ETA\n",
    "# + train-step cap per epoch + sparse eval (TEDS less frequent) + optional chunk-cap patch\n",
    "#\n",
    "# Assumes you already have: model_ssa, DocHieNetDocDataset, index_df, collate_fn\n",
    "# This cell rebuilds train_loader/test_loader with faster caps, then trains.\n",
    "'''\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "try:\n",
    "    import zss\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"zss\"])\n",
    "    import zss\n",
    "\n",
    "# ------------------ reproducibility ------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ------------------ speed knobs (RTX 4060 8GB friendly) ------------------\n",
    "EPOCHS = 10                       # 先 20 看趋势；稳定后可加到 30~50\n",
    "MAX_TRAIN_STEPS_PER_EPOCH = 200   # 每个 epoch 只跑这么多 doc（关键加速）\n",
    "PRINT_EVERY_STEP = 20\n",
    "\n",
    "TRAIN_MAX_CHUNKS = 16             # 训练 cap（论文 32；你这张卡建议 16）\n",
    "TEST_MAX_CHUNKS  = 64             # 测试 cap（可先 64；最终可回 128）\n",
    "\n",
    "EVAL_EVERY_EPOCH = 1              # 每个 epoch 都算 F1\n",
    "EVAL_TEDS_EVERY  = 5              # 每 5 个 epoch 才算一次 TEDS（关键加速）\n",
    "\n",
    "# ------------------ optimization ------------------\n",
    "LR_START = 4e-5\n",
    "LR_END = 1e-6\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.05\n",
    "GRAD_ACCUM_STEPS = 2              # 8GB 上建议 2；如果显存更紧就 1\n",
    "MAX_GRAD_NORM = 1.0\n",
    "ROOT_ID = 0\n",
    "\n",
    "SAVE_PATH = \"best_model_ssa_fast.pt\"\n",
    "# choose best by: \"F1_nonroot\" or \"TEDS_sem\"\n",
    "BEST_BY = \"F1_nonroot\"\n",
    "\n",
    "# ------------------ rebuild loaders with new caps ------------------\n",
    "train_ds = DocHieNetDocDataset(index_df, split=\"train\")\n",
    "test_ds  = DocHieNetDocDataset(index_df, split=\"test\")\n",
    "train_ds.max_chunks = TRAIN_MAX_CHUNKS\n",
    "test_ds.max_chunks  = TEST_MAX_CHUNKS\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"Loader caps:\", f\"train_max_chunks={TRAIN_MAX_CHUNKS}\", f\"test_max_chunks={TEST_MAX_CHUNKS}\")\n",
    "print(\"Train steps/epoch cap:\", MAX_TRAIN_STEPS_PER_EPOCH)\n",
    "\n",
    "# ------------------ model/optimizer ------------------\n",
    "model_ssa.to(device)\n",
    "model_ssa.train()\n",
    "\n",
    "optimizer = optim.AdamW(model_ssa.parameters(), lr=LR_START, weight_decay=WEIGHT_DECAY)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "# total updates = epochs * ceil(train_steps/accum)\n",
    "steps_per_epoch = math.ceil(MAX_TRAIN_STEPS_PER_EPOCH / GRAD_ACCUM_STEPS)\n",
    "total_updates = steps_per_epoch * EPOCHS\n",
    "warmup_updates = max(1, int(total_updates * WARMUP_RATIO))\n",
    "\n",
    "def set_lr(lr):\n",
    "    for pg in optimizer.param_groups:\n",
    "        pg[\"lr\"] = lr\n",
    "\n",
    "def lr_schedule(update_idx):\n",
    "    if update_idx <= warmup_updates:\n",
    "        return LR_START * (update_idx / warmup_updates)\n",
    "    t = update_idx - warmup_updates\n",
    "    T = max(1, total_updates - warmup_updates)\n",
    "    gamma = (LR_END / LR_START) ** (1.0 / T)\n",
    "    return LR_START * (gamma ** t)\n",
    "\n",
    "# ------------------ eval helpers ------------------\n",
    "def micro_f1(model, loader, exclude_root=False, root_id=0):\n",
    "    model.eval()\n",
    "    tp = pred_n = gt_n = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                _, logits = model(\n",
    "                    batch[\"chunks\"],\n",
    "                    batch[\"doc_elem_ids\"],\n",
    "                    batch[\"doc_elem_positions\"],\n",
    "                    batch[\"parent_map\"]\n",
    "                )\n",
    "            parent_candidates = [root_id] + batch[\"doc_elem_ids\"]\n",
    "            pred_parent_idx = torch.argmax(logits, dim=1).tolist()\n",
    "\n",
    "            pred_edges = set((parent_candidates[pidx], child_eid)\n",
    "                             for child_eid, pidx in zip(batch[\"doc_elem_ids\"], pred_parent_idx))\n",
    "            gt_edges = set((batch[\"parent_map\"][eid], eid) for eid in batch[\"doc_elem_ids\"])\n",
    "\n",
    "            if exclude_root:\n",
    "                pred_edges = set(e for e in pred_edges if e[0] != root_id)\n",
    "                gt_edges = set(e for e in gt_edges if e[0] != root_id)\n",
    "\n",
    "            inter = pred_edges & gt_edges\n",
    "            tp += len(inter); pred_n += len(pred_edges); gt_n += len(gt_edges)\n",
    "\n",
    "    p = tp / pred_n if pred_n else 0.0\n",
    "    r = tp / gt_n if gt_n else 0.0\n",
    "    f1 = (2*p*r/(p+r)) if (p+r) else 0.0\n",
    "    model.train()\n",
    "    return p, r, f1\n",
    "\n",
    "def build_children_ordered(doc_elem_ids, parent_map, root_id=0):\n",
    "    order = {eid: i for i, eid in enumerate([root_id] + doc_elem_ids)}\n",
    "    children = {root_id: []}\n",
    "    for eid in doc_elem_ids:\n",
    "        children.setdefault(eid, [])\n",
    "    for child in doc_elem_ids:\n",
    "        par = parent_map.get(child, root_id)\n",
    "        children.setdefault(par, [])\n",
    "        children[par].append(child)\n",
    "    for k in children:\n",
    "        children[k].sort(key=lambda x: order.get(x, 10**9))\n",
    "    return children\n",
    "\n",
    "def to_zss_tree(children, node_id, label_map):\n",
    "    node = zss.Node(label_map.get(node_id, \"UNK\"))\n",
    "    for ch in children.get(node_id, []):\n",
    "        node.addkid(to_zss_tree(children, ch, label_map))\n",
    "    return node\n",
    "\n",
    "def semantic_teds_one(doc_id, doc_elem_ids, gt_parent_map, pred_parent_map):\n",
    "    row = index_df[index_df[\"doc_id\"] == doc_id].iloc[0]\n",
    "    label_path = row[\"label_path\"]\n",
    "    obj = json.loads(Path(label_path).read_text(encoding=\"utf-8\"))\n",
    "    elem_label_map = {c[\"id\"]: c.get(\"label\", \"UNK\") for c in obj[\"contents\"]}\n",
    "\n",
    "    gt_children = build_children_ordered(doc_elem_ids, gt_parent_map, root_id=ROOT_ID)\n",
    "    pr_children = build_children_ordered(doc_elem_ids, pred_parent_map, root_id=ROOT_ID)\n",
    "\n",
    "    label_map = {ROOT_ID: \"ROOT\"}\n",
    "    for eid in doc_elem_ids:\n",
    "        label_map[eid] = elem_label_map.get(eid, \"UNK\")\n",
    "\n",
    "    gt_root = to_zss_tree(gt_children, ROOT_ID, label_map)\n",
    "    pr_root = to_zss_tree(pr_children, ROOT_ID, label_map)\n",
    "\n",
    "    def insert_cost(n): return 1\n",
    "    def remove_cost(n): return 1\n",
    "    def update_cost(a, b): return 0 if a.label == b.label else 1\n",
    "\n",
    "    ed = zss.distance(\n",
    "        gt_root, pr_root,\n",
    "        get_children=zss.Node.get_children,\n",
    "        insert_cost=insert_cost,\n",
    "        remove_cost=remove_cost,\n",
    "        update_cost=update_cost\n",
    "    )\n",
    "    denom = 1 + len(doc_elem_ids)\n",
    "    return 1.0 - (ed / denom)\n",
    "\n",
    "def eval_semantic_teds(model, loader):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                _, logits = model(\n",
    "                    batch[\"chunks\"],\n",
    "                    batch[\"doc_elem_ids\"],\n",
    "                    batch[\"doc_elem_positions\"],\n",
    "                    batch[\"parent_map\"]\n",
    "                )\n",
    "            parent_candidates = [ROOT_ID] + batch[\"doc_elem_ids\"]\n",
    "            pred_parent_idx = torch.argmax(logits, dim=1).tolist()\n",
    "            pred_parent_map = {child: parent_candidates[pidx]\n",
    "                               for child, pidx in zip(batch[\"doc_elem_ids\"], pred_parent_idx)}\n",
    "            scores.append(semantic_teds_one(batch[\"doc_id\"], batch[\"doc_elem_ids\"], batch[\"parent_map\"], pred_parent_map))\n",
    "    model.train()\n",
    "    return sum(scores) / max(len(scores), 1)\n",
    "\n",
    "# ------------------ train loop ------------------\n",
    "global_start = time.time()\n",
    "global_update = 0\n",
    "\n",
    "best_key = -1.0\n",
    "best_snapshot = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start = time.time()\n",
    "    step_times = []\n",
    "\n",
    "    model_ssa.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    updates = 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        t0 = time.time()\n",
    "\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            loss, _ = model_ssa(\n",
    "                batch[\"chunks\"],\n",
    "                batch[\"doc_elem_ids\"],\n",
    "                batch[\"doc_elem_positions\"],\n",
    "                batch[\"parent_map\"]\n",
    "            )\n",
    "\n",
    "        scaler.scale(loss / GRAD_ACCUM_STEPS).backward()\n",
    "\n",
    "        total_loss += float(loss.detach().cpu())\n",
    "        steps += 1\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        step_times.append(dt)\n",
    "\n",
    "        if step % PRINT_EVERY_STEP == 0:\n",
    "            avg_dt = sum(step_times[-PRINT_EVERY_STEP:]) / PRINT_EVERY_STEP\n",
    "            print(f\"[epoch {epoch:03d} | step {step:04d}] loss={loss.item():.4f} | {avg_dt:.2f}s/step\")\n",
    "\n",
    "        # optimizer update\n",
    "        if step % GRAD_ACCUM_STEPS == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model_ssa.parameters(), MAX_GRAD_NORM)\n",
    "\n",
    "            global_update += 1\n",
    "            lr = lr_schedule(global_update)\n",
    "            set_lr(lr)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            updates += 1\n",
    "\n",
    "        if step >= MAX_TRAIN_STEPS_PER_EPOCH:\n",
    "            break\n",
    "\n",
    "    avg_train_loss = total_loss / max(steps, 1)\n",
    "\n",
    "    # ------------------ eval ------------------\n",
    "    eval_start = time.time()\n",
    "\n",
    "    if (epoch % EVAL_EVERY_EPOCH) == 0:\n",
    "        _, _, f1_all = micro_f1(model_ssa, test_loader, exclude_root=False, root_id=ROOT_ID)\n",
    "        _, _, f1_nr  = micro_f1(model_ssa, test_loader, exclude_root=True,  root_id=ROOT_ID)\n",
    "    else:\n",
    "        f1_all, f1_nr = float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    teds_sem = float(\"nan\")\n",
    "    if (epoch % EVAL_TEDS_EVERY) == 0:\n",
    "        teds_sem = eval_semantic_teds(model_ssa, test_loader)\n",
    "\n",
    "    eval_time = time.time() - eval_start\n",
    "\n",
    "    # ------------------ choose best & save ------------------\n",
    "    if BEST_BY == \"F1_nonroot\":\n",
    "        key = f1_nr\n",
    "    else:\n",
    "        key = teds_sem\n",
    "\n",
    "    tag = \"\"\n",
    "    if not (isinstance(key, float) and (math.isnan(key))):\n",
    "        if key > best_key:\n",
    "            best_key = key\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model_ssa.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"best_key\": best_key,\n",
    "                    \"best_by\": BEST_BY,\n",
    "                    \"f1_all\": f1_all,\n",
    "                    \"f1_nonroot\": f1_nr,\n",
    "                    \"teds_sem\": teds_sem,\n",
    "                    \"seed\": SEED,\n",
    "                    \"global_update\": global_update,\n",
    "                    \"caps\": {\"train_max_chunks\": TRAIN_MAX_CHUNKS, \"test_max_chunks\": TEST_MAX_CHUNKS},\n",
    "                    \"train_steps_per_epoch_cap\": MAX_TRAIN_STEPS_PER_EPOCH,\n",
    "                    \"grad_accum_steps\": GRAD_ACCUM_STEPS,\n",
    "                    \"note\": \"fast timed train for RTX 4060 8GB\"\n",
    "                },\n",
    "                SAVE_PATH\n",
    "            )\n",
    "            tag = \" (best)\"\n",
    "\n",
    "    # ------------------ timing / ETA ------------------\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    elapsed = time.time() - global_start\n",
    "    avg_epoch = elapsed / epoch\n",
    "    eta = avg_epoch * (EPOCHS - epoch)\n",
    "    lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    # pretty print\n",
    "    f1_all_str  = f\"{f1_all:.4f}\" if not (isinstance(f1_all, float) and math.isnan(f1_all)) else \"skip\"\n",
    "    f1_nr_str   = f\"{f1_nr:.4f}\"  if not (isinstance(f1_nr, float) and math.isnan(f1_nr))  else \"skip\"\n",
    "    teds_str    = f\"{teds_sem:.4f}\" if not (isinstance(teds_sem, float) and math.isnan(teds_sem)) else \"skip\"\n",
    "\n",
    "    print(\n",
    "        f\"\\n[epoch {epoch:03d}/{EPOCHS}] lr={lr_now:.2e} | updates={updates} | train_loss={avg_train_loss:.4f}\\n\"\n",
    "        f\"  test_F1_all={f1_all_str} | test_F1_nonroot={f1_nr_str} | test_TEDS_sem={teds_str}{tag}\\n\"\n",
    "        f\"  epoch_time={epoch_time/60:.1f} min | eval_time={eval_time:.1f}s | ETA={eta/60:.1f} min\\n\"\n",
    "    )\n",
    "\n",
    "print(\"done. best_key=\", best_key, \"best_by=\", BEST_BY, \"saved:\", SAVE_PATH)\n",
    "\n",
    "\n",
    "try:\n",
    "    import zss\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"zss\"])\n",
    "    import zss\n",
    "\n",
    "CKPT_PATH = \"best_model_ssa_fast.pt\"\n",
    "ROOT_ID = 0\n",
    "\n",
    "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "model_ssa.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model_ssa.to(device)\n",
    "model_ssa.eval()\n",
    "\n",
    "print(\"Loaded:\", CKPT_PATH, \"| epoch:\", ckpt.get(\"epoch\"), \"| best_key:\", ckpt.get(\"best_key\"))\n",
    "\n",
    "def micro_prf(model, loader, exclude_root=False, root_id=0):\n",
    "    model.eval()\n",
    "    tp = pred_n = gt_n = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            _, logits = model(\n",
    "                batch[\"chunks\"],\n",
    "                batch[\"doc_elem_ids\"],\n",
    "                batch[\"doc_elem_positions\"],\n",
    "                batch[\"parent_map\"]\n",
    "            )\n",
    "            parent_candidates = [root_id] + batch[\"doc_elem_ids\"]\n",
    "            pred_parent_idx = torch.argmax(logits, dim=1).tolist()\n",
    "\n",
    "            pred_edges = set((parent_candidates[pidx], child_eid)\n",
    "                             for child_eid, pidx in zip(batch[\"doc_elem_ids\"], pred_parent_idx))\n",
    "            gt_edges = set((batch[\"parent_map\"][eid], eid) for eid in batch[\"doc_elem_ids\"])\n",
    "\n",
    "            if exclude_root:\n",
    "                pred_edges = set(e for e in pred_edges if e[0] != root_id)\n",
    "                gt_edges = set(e for e in gt_edges if e[0] != root_id)\n",
    "\n",
    "            inter = pred_edges & gt_edges\n",
    "            tp += len(inter)\n",
    "            pred_n += len(pred_edges)\n",
    "            gt_n += len(gt_edges)\n",
    "\n",
    "    p = tp / pred_n if pred_n else 0.0\n",
    "    r = tp / gt_n if gt_n else 0.0\n",
    "    f1 = (2*p*r/(p+r)) if (p+r) else 0.0\n",
    "    return p, r, f1\n",
    "\n",
    "def build_children_ordered(doc_elem_ids, parent_map, root_id=0):\n",
    "    order = {eid: i for i, eid in enumerate([root_id] + doc_elem_ids)}\n",
    "    children = {root_id: []}\n",
    "    for eid in doc_elem_ids:\n",
    "        children.setdefault(eid, [])\n",
    "    for child in doc_elem_ids:\n",
    "        par = parent_map.get(child, root_id)\n",
    "        children.setdefault(par, [])\n",
    "        children[par].append(child)\n",
    "    for k in children:\n",
    "        children[k].sort(key=lambda x: order.get(x, 10**9))\n",
    "    return children\n",
    "\n",
    "def to_zss_tree(children, node_id, label_map):\n",
    "    node = zss.Node(label_map.get(node_id, \"UNK\"))\n",
    "    for ch in children.get(node_id, []):\n",
    "        node.addkid(to_zss_tree(children, ch, label_map))\n",
    "    return node\n",
    "\n",
    "def semantic_teds_one(doc_id, doc_elem_ids, gt_parent_map, pred_parent_map):\n",
    "    row = index_df[index_df[\"doc_id\"] == doc_id].iloc[0]\n",
    "    label_path = row[\"label_path\"]\n",
    "    obj = json.loads(Path(label_path).read_text(encoding=\"utf-8\"))\n",
    "    elem_label_map = {c[\"id\"]: c.get(\"label\", \"UNK\") for c in obj[\"contents\"]}\n",
    "\n",
    "    gt_children = build_children_ordered(doc_elem_ids, gt_parent_map, root_id=ROOT_ID)\n",
    "    pr_children = build_children_ordered(doc_elem_ids, pred_parent_map, root_id=ROOT_ID)\n",
    "\n",
    "    label_map = {ROOT_ID: \"ROOT\"}\n",
    "    for eid in doc_elem_ids:\n",
    "        label_map[eid] = elem_label_map.get(eid, \"UNK\")\n",
    "\n",
    "    gt_root = to_zss_tree(gt_children, ROOT_ID, label_map)\n",
    "    pr_root = to_zss_tree(pr_children, ROOT_ID, label_map)\n",
    "\n",
    "    def insert_cost(n): return 1\n",
    "    def remove_cost(n): return 1\n",
    "    def update_cost(a, b): return 0 if a.label == b.label else 1\n",
    "\n",
    "    ed = zss.distance(\n",
    "        gt_root, pr_root,\n",
    "        get_children=zss.Node.get_children,\n",
    "        insert_cost=insert_cost,\n",
    "        remove_cost=remove_cost,\n",
    "        update_cost=update_cost\n",
    "    )\n",
    "\n",
    "    denom = 1 + len(doc_elem_ids)\n",
    "    return 1.0 - (ed / denom)\n",
    "\n",
    "def eval_semantic_teds(model, loader):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            doc_id = batch[\"doc_id\"]\n",
    "            _, logits = model(\n",
    "                batch[\"chunks\"],\n",
    "                batch[\"doc_elem_ids\"],\n",
    "                batch[\"doc_elem_positions\"],\n",
    "                batch[\"parent_map\"]\n",
    "            )\n",
    "            parent_candidates = [ROOT_ID] + batch[\"doc_elem_ids\"]\n",
    "            pred_parent_idx = torch.argmax(logits, dim=1).tolist()\n",
    "            pred_parent_map = {child: parent_candidates[pidx]\n",
    "                               for child, pidx in zip(batch[\"doc_elem_ids\"], pred_parent_idx)}\n",
    "\n",
    "            scores.append(\n",
    "                semantic_teds_one(doc_id, batch[\"doc_elem_ids\"], batch[\"parent_map\"], pred_parent_map)\n",
    "            )\n",
    "    return sum(scores)/max(len(scores), 1)\n",
    "\n",
    "# Final metrics on test_loader\n",
    "p_all, r_all, f1_all = micro_prf(model_ssa, test_loader, exclude_root=False, root_id=ROOT_ID)\n",
    "p_nr,  r_nr,  f1_nr  = micro_prf(model_ssa, test_loader, exclude_root=True,  root_id=ROOT_ID)\n",
    "teds_sem = eval_semantic_teds(model_ssa, test_loader)\n",
    "\n",
    "print(\"\\n===== FINAL TEST REPORT =====\")\n",
    "print(f\"F1_all     : {f1_all:.4f} (P={p_all:.4f}, R={r_all:.4f})\")\n",
    "print(f\"F1_nonroot : {f1_nr:.4f} (P={p_nr:.4f}, R={r_nr:.4f})\")\n",
    "print(f\"TEDS_sem   : {teds_sem:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c5304b9-ec1e-4d70-8c53-84e57983bfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomra\\AppData\\Local\\Temp\\ipykernel_127652\\2639875039.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(CKPT_PATH, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: best_model_ssa_fast.pt | epoch: 1 | best_key: 0.04392875901255789\n",
      "\n",
      "===== FINAL TEST REPORT =====\n",
      "F1_all     : 0.1810 (P=0.1810, R=0.1810)\n",
      "F1_nonroot : 0.0369 (P=0.0404, R=0.0339)\n",
      "TEDS_sem   : 0.3002\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import zss\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"zss\"])\n",
    "    import zss\n",
    "\n",
    "CKPT_PATH = \"best_model_ssa_fast.pt\"\n",
    "ROOT_ID = 0\n",
    "\n",
    "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "model_ssa.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model_ssa.to(device)\n",
    "model_ssa.eval()\n",
    "\n",
    "print(\"Loaded:\", CKPT_PATH, \"| epoch:\", ckpt.get(\"epoch\"), \"| best_key:\", ckpt.get(\"best_key\"))\n",
    "\n",
    "def micro_prf(model, loader, exclude_root=False, root_id=0):\n",
    "    model.eval()\n",
    "    tp = pred_n = gt_n = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            _, logits = model(\n",
    "                batch[\"chunks\"],\n",
    "                batch[\"doc_elem_ids\"],\n",
    "                batch[\"doc_elem_positions\"],\n",
    "                batch[\"parent_map\"]\n",
    "            )\n",
    "            parent_candidates = [root_id] + batch[\"doc_elem_ids\"]\n",
    "            pred_parent_idx = torch.argmax(logits, dim=1).tolist()\n",
    "\n",
    "            pred_edges = set((parent_candidates[pidx], child_eid)\n",
    "                             for child_eid, pidx in zip(batch[\"doc_elem_ids\"], pred_parent_idx))\n",
    "            gt_edges = set((batch[\"parent_map\"][eid], eid) for eid in batch[\"doc_elem_ids\"])\n",
    "\n",
    "            if exclude_root:\n",
    "                pred_edges = set(e for e in pred_edges if e[0] != root_id)\n",
    "                gt_edges = set(e for e in gt_edges if e[0] != root_id)\n",
    "\n",
    "            inter = pred_edges & gt_edges\n",
    "            tp += len(inter)\n",
    "            pred_n += len(pred_edges)\n",
    "            gt_n += len(gt_edges)\n",
    "\n",
    "    p = tp / pred_n if pred_n else 0.0\n",
    "    r = tp / gt_n if gt_n else 0.0\n",
    "    f1 = (2*p*r/(p+r)) if (p+r) else 0.0\n",
    "    return p, r, f1\n",
    "\n",
    "def build_children_ordered(doc_elem_ids, parent_map, root_id=0):\n",
    "    order = {eid: i for i, eid in enumerate([root_id] + doc_elem_ids)}\n",
    "    children = {root_id: []}\n",
    "    for eid in doc_elem_ids:\n",
    "        children.setdefault(eid, [])\n",
    "    for child in doc_elem_ids:\n",
    "        par = parent_map.get(child, root_id)\n",
    "        children.setdefault(par, [])\n",
    "        children[par].append(child)\n",
    "    for k in children:\n",
    "        children[k].sort(key=lambda x: order.get(x, 10**9))\n",
    "    return children\n",
    "\n",
    "def to_zss_tree(children, node_id, label_map):\n",
    "    node = zss.Node(label_map.get(node_id, \"UNK\"))\n",
    "    for ch in children.get(node_id, []):\n",
    "        node.addkid(to_zss_tree(children, ch, label_map))\n",
    "    return node\n",
    "\n",
    "def semantic_teds_one(doc_id, doc_elem_ids, gt_parent_map, pred_parent_map):\n",
    "    row = index_df[index_df[\"doc_id\"] == doc_id].iloc[0]\n",
    "    label_path = row[\"label_path\"]\n",
    "    obj = json.loads(Path(label_path).read_text(encoding=\"utf-8\"))\n",
    "    elem_label_map = {c[\"id\"]: c.get(\"label\", \"UNK\") for c in obj[\"contents\"]}\n",
    "\n",
    "    gt_children = build_children_ordered(doc_elem_ids, gt_parent_map, root_id=ROOT_ID)\n",
    "    pr_children = build_children_ordered(doc_elem_ids, pred_parent_map, root_id=ROOT_ID)\n",
    "\n",
    "    label_map = {ROOT_ID: \"ROOT\"}\n",
    "    for eid in doc_elem_ids:\n",
    "        label_map[eid] = elem_label_map.get(eid, \"UNK\")\n",
    "\n",
    "    gt_root = to_zss_tree(gt_children, ROOT_ID, label_map)\n",
    "    pr_root = to_zss_tree(pr_children, ROOT_ID, label_map)\n",
    "\n",
    "    def insert_cost(n): return 1\n",
    "    def remove_cost(n): return 1\n",
    "    def update_cost(a, b): return 0 if a.label == b.label else 1\n",
    "\n",
    "    ed = zss.distance(\n",
    "        gt_root, pr_root,\n",
    "        get_children=zss.Node.get_children,\n",
    "        insert_cost=insert_cost,\n",
    "        remove_cost=remove_cost,\n",
    "        update_cost=update_cost\n",
    "    )\n",
    "\n",
    "    denom = 1 + len(doc_elem_ids)\n",
    "    return 1.0 - (ed / denom)\n",
    "\n",
    "def eval_semantic_teds(model, loader):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            doc_id = batch[\"doc_id\"]\n",
    "            _, logits = model(\n",
    "                batch[\"chunks\"],\n",
    "                batch[\"doc_elem_ids\"],\n",
    "                batch[\"doc_elem_positions\"],\n",
    "                batch[\"parent_map\"]\n",
    "            )\n",
    "            parent_candidates = [ROOT_ID] + batch[\"doc_elem_ids\"]\n",
    "            pred_parent_idx = torch.argmax(logits, dim=1).tolist()\n",
    "            pred_parent_map = {child: parent_candidates[pidx]\n",
    "                               for child, pidx in zip(batch[\"doc_elem_ids\"], pred_parent_idx)}\n",
    "\n",
    "            scores.append(\n",
    "                semantic_teds_one(doc_id, batch[\"doc_elem_ids\"], batch[\"parent_map\"], pred_parent_map)\n",
    "            )\n",
    "    return sum(scores)/max(len(scores), 1)\n",
    "\n",
    "# Final metrics on test_loader\n",
    "p_all, r_all, f1_all = micro_prf(model_ssa, test_loader, exclude_root=False, root_id=ROOT_ID)\n",
    "p_nr,  r_nr,  f1_nr  = micro_prf(model_ssa, test_loader, exclude_root=True,  root_id=ROOT_ID)\n",
    "teds_sem = eval_semantic_teds(model_ssa, test_loader)\n",
    "\n",
    "print(\"\\n===== FINAL TEST REPORT =====\")\n",
    "print(f\"F1_all     : {f1_all:.4f} (P={p_all:.4f}, R={r_all:.4f})\")\n",
    "print(f\"F1_nonroot : {f1_nr:.4f} (P={p_nr:.4f}, R={r_nr:.4f})\")\n",
    "print(f\"TEDS_sem   : {teds_sem:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33a54325-4ebd-4028-aab0-2142691a91f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== baseline (PageE+InnerE) ===\n",
      "use_page_emb=True | use_inner_emb=True\n",
      "F1_all     : 0.1810 (P=0.1810, R=0.1810)\n",
      "F1_nonroot : 0.0369 (P=0.0404, R=0.0339)\n",
      "TEDS_sem   : 0.3002\n",
      "\n",
      "=== w/o PageE ===\n",
      "use_page_emb=False | use_inner_emb=True\n",
      "F1_all     : 0.1896 (P=0.1896, R=0.1896)\n",
      "F1_nonroot : 0.0000 (P=0.0000, R=0.0000)\n",
      "TEDS_sem   : 0.6376\n",
      "\n",
      "=== w/o InnerE ===\n",
      "use_page_emb=True | use_inner_emb=False\n",
      "F1_all     : 0.1260 (P=0.1260, R=0.1260)\n",
      "F1_nonroot : 0.0198 (P=0.0228, R=0.0175)\n",
      "TEDS_sem   : 0.4121\n",
      "\n",
      "===== SUMMARY (copy to report) =====\n",
      "baseline (PageE+InnerE): F1_all=0.1810 | F1_nonroot=0.0369 | TEDS_sem=0.3002\n",
      "w/o PageE: F1_all=0.1896 | F1_nonroot=0.0000 | TEDS_sem=0.6376\n",
      "w/o InnerE: F1_all=0.1260 | F1_nonroot=0.0198 | TEDS_sem=0.4121\n"
     ]
    }
   ],
   "source": [
    "# CELL ABLATION: w/o Page Embedding / w/o Inner-layout Embedding on model_ssa, report F1_all/F1_nonroot/TEDS_sem\n",
    "\n",
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import types\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ROOT_ID = 0\n",
    "\n",
    "# ensure zss\n",
    "try:\n",
    "    import zss\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"zss\"])\n",
    "    import zss\n",
    "\n",
    "# --- (1) patch model_ssa to support toggles ---\n",
    "# We override _encode_one_chunk to conditionally add page/inner embeddings.\n",
    "# This assumes your model_ssa has: encoder, inner_emb, page_proj, page_pe_dim.\n",
    "def sinusoidal_position_encoding(pos, dim):\n",
    "    pos = pos.float().unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, dim, 2, device=pos.device).float() * (-math.log(10000.0) / dim))\n",
    "    pe = torch.zeros(pos.size(0), dim, device=pos.device)\n",
    "    pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "    pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "    return pe\n",
    "\n",
    "# save original method once (if not saved yet)\n",
    "if not hasattr(model_ssa, \"_encode_one_chunk_orig\"):\n",
    "    model_ssa._encode_one_chunk_orig = model_ssa._encode_one_chunk\n",
    "\n",
    "def _encode_one_chunk_toggled(self, c):\n",
    "    # base encoder output\n",
    "    input_ids = torch.tensor(c[\"input_ids\"], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    attention_mask = torch.tensor(c[\"attention_mask\"], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    bbox = torch.tensor(c[\"bbox\"], dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    o = self.encoder(input_ids=input_ids, attention_mask=attention_mask, bbox=bbox, return_dict=True)\n",
    "    hidden = o.last_hidden_state.squeeze(0)  # [T,H]\n",
    "\n",
    "    # toggles\n",
    "    use_page = getattr(self, \"use_page_emb\", True)\n",
    "    use_inner = getattr(self, \"use_inner_emb\", True)\n",
    "\n",
    "    if use_page:\n",
    "        page_ids = torch.tensor(c[\"page_id\"], dtype=torch.long, device=device)\n",
    "        pe = sinusoidal_position_encoding(page_ids, self.page_pe_dim)  # [T, page_pe_dim]\n",
    "        page_e = self.page_proj(pe)                                    # [T, H]\n",
    "        hidden = hidden + page_e\n",
    "\n",
    "    if use_inner:\n",
    "        inner_pos = torch.tensor(c[\"inner_pos\"], dtype=torch.long, device=device)\n",
    "        inner_pos = torch.clamp(inner_pos, 0, self.inner_emb.num_embeddings - 1)\n",
    "        inner_e = self.inner_emb(inner_pos)                             # [T, H]\n",
    "        hidden = hidden + inner_e\n",
    "\n",
    "    return hidden\n",
    "\n",
    "# bind patched method\n",
    "model_ssa._encode_one_chunk = types.MethodType(_encode_one_chunk_toggled, model_ssa)\n",
    "\n",
    "# --- (2) metrics: F1_all / F1_nonroot / TEDS_semantic ---\n",
    "def micro_f1(model, loader, exclude_root=False, root_id=0):\n",
    "    model.eval()\n",
    "    tp = pred_n = gt_n = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            _, logits = model(\n",
    "                batch[\"chunks\"],\n",
    "                batch[\"doc_elem_ids\"],\n",
    "                batch[\"doc_elem_positions\"],\n",
    "                batch[\"parent_map\"]\n",
    "            )\n",
    "            parent_candidates = [root_id] + batch[\"doc_elem_ids\"]\n",
    "            pred_parent_idx = torch.argmax(logits, dim=1).tolist()\n",
    "\n",
    "            pred_edges = set((parent_candidates[pidx], child_eid)\n",
    "                             for child_eid, pidx in zip(batch[\"doc_elem_ids\"], pred_parent_idx))\n",
    "            gt_edges = set((batch[\"parent_map\"][eid], eid) for eid in batch[\"doc_elem_ids\"])\n",
    "\n",
    "            if exclude_root:\n",
    "                pred_edges = set(e for e in pred_edges if e[0] != root_id)\n",
    "                gt_edges = set(e for e in gt_edges if e[0] != root_id)\n",
    "\n",
    "            inter = pred_edges & gt_edges\n",
    "            tp += len(inter)\n",
    "            pred_n += len(pred_edges)\n",
    "            gt_n += len(gt_edges)\n",
    "\n",
    "    p = tp / pred_n if pred_n else 0.0\n",
    "    r = tp / gt_n if gt_n else 0.0\n",
    "    f1 = (2*p*r/(p+r)) if (p+r) else 0.0\n",
    "    model.train()\n",
    "    return p, r, f1\n",
    "\n",
    "def build_children_ordered(doc_elem_ids, parent_map, root_id=0):\n",
    "    order = {eid: i for i, eid in enumerate([root_id] + doc_elem_ids)}\n",
    "    children = {root_id: []}\n",
    "    for eid in doc_elem_ids:\n",
    "        children.setdefault(eid, [])\n",
    "    for child in doc_elem_ids:\n",
    "        par = parent_map.get(child, root_id)\n",
    "        children.setdefault(par, [])\n",
    "        children[par].append(child)\n",
    "    for k in children:\n",
    "        children[k].sort(key=lambda x: order.get(x, 10**9))\n",
    "    return children\n",
    "\n",
    "def to_zss_tree(children, node_id, label_map):\n",
    "    node = zss.Node(label_map.get(node_id, \"UNK\"))\n",
    "    for ch in children.get(node_id, []):\n",
    "        node.addkid(to_zss_tree(children, ch, label_map))\n",
    "    return node\n",
    "\n",
    "def semantic_teds_one(doc_id, doc_elem_ids, gt_parent_map, pred_parent_map):\n",
    "    row = index_df[index_df[\"doc_id\"] == doc_id].iloc[0]\n",
    "    label_path = row[\"label_path\"]\n",
    "    obj = json.loads(Path(label_path).read_text(encoding=\"utf-8\"))\n",
    "    elem_label_map = {c[\"id\"]: c.get(\"label\", \"UNK\") for c in obj[\"contents\"]}\n",
    "\n",
    "    gt_children = build_children_ordered(doc_elem_ids, gt_parent_map, root_id=ROOT_ID)\n",
    "    pr_children = build_children_ordered(doc_elem_ids, pred_parent_map, root_id=ROOT_ID)\n",
    "\n",
    "    label_map = {ROOT_ID: \"ROOT\"}\n",
    "    for eid in doc_elem_ids:\n",
    "        label_map[eid] = elem_label_map.get(eid, \"UNK\")\n",
    "\n",
    "    gt_root = to_zss_tree(gt_children, ROOT_ID, label_map)\n",
    "    pr_root = to_zss_tree(pr_children, ROOT_ID, label_map)\n",
    "\n",
    "    def insert_cost(n): return 1\n",
    "    def remove_cost(n): return 1\n",
    "    def update_cost(a, b): return 0 if a.label == b.label else 1\n",
    "\n",
    "    ed = zss.distance(\n",
    "        gt_root, pr_root,\n",
    "        get_children=zss.Node.get_children,\n",
    "        insert_cost=insert_cost,\n",
    "        remove_cost=remove_cost,\n",
    "        update_cost=update_cost\n",
    "    )\n",
    "\n",
    "    denom = 1 + len(doc_elem_ids)\n",
    "    return 1.0 - (ed / denom)\n",
    "\n",
    "def eval_semantic_teds(model, loader):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            doc_id = batch[\"doc_id\"]\n",
    "            _, logits = model(\n",
    "                batch[\"chunks\"],\n",
    "                batch[\"doc_elem_ids\"],\n",
    "                batch[\"doc_elem_positions\"],\n",
    "                batch[\"parent_map\"]\n",
    "            )\n",
    "            parent_candidates = [ROOT_ID] + batch[\"doc_elem_ids\"]\n",
    "            pred_parent_idx = torch.argmax(logits, dim=1).tolist()\n",
    "            pred_parent_map = {child: parent_candidates[pidx]\n",
    "                               for child, pidx in zip(batch[\"doc_elem_ids\"], pred_parent_idx)}\n",
    "            scores.append(semantic_teds_one(doc_id, batch[\"doc_elem_ids\"], batch[\"parent_map\"], pred_parent_map))\n",
    "    model.train()\n",
    "    return sum(scores) / max(len(scores), 1)\n",
    "\n",
    "def run_one(name, use_page, use_inner):\n",
    "    model_ssa.use_page_emb = use_page\n",
    "    model_ssa.use_inner_emb = use_inner\n",
    "\n",
    "    p_all, r_all, f1_all = micro_f1(model_ssa, test_loader, exclude_root=False, root_id=ROOT_ID)\n",
    "    p_nr,  r_nr,  f1_nr  = micro_f1(model_ssa, test_loader, exclude_root=True,  root_id=ROOT_ID)\n",
    "    teds = eval_semantic_teds(model_ssa, test_loader)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"use_page_emb={use_page} | use_inner_emb={use_inner}\")\n",
    "    print(f\"F1_all     : {f1_all:.4f} (P={p_all:.4f}, R={r_all:.4f})\")\n",
    "    print(f\"F1_nonroot : {f1_nr:.4f} (P={p_nr:.4f}, R={r_nr:.4f})\")\n",
    "    print(f\"TEDS_sem   : {teds:.4f}\")\n",
    "    return {\"name\": name, \"F1_all\": f1_all, \"F1_nonroot\": f1_nr, \"TEDS_sem\": teds}\n",
    "\n",
    "# --- (3) run ablations ---\n",
    "model_ssa.to(device)\n",
    "\n",
    "res = []\n",
    "res.append(run_one(\"baseline (PageE+InnerE)\", True, True))\n",
    "res.append(run_one(\"w/o PageE\", False, True))\n",
    "res.append(run_one(\"w/o InnerE\", True, False))\n",
    "\n",
    "print(\"\\n===== SUMMARY (copy to report) =====\")\n",
    "for r in res:\n",
    "    print(f'{r[\"name\"]}: F1_all={r[\"F1_all\"]:.4f} | F1_nonroot={r[\"F1_nonroot\"]:.4f} | TEDS_sem={r[\"TEDS_sem\"]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121eee1e-f36e-4922-91d0-a267bea0d672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hrdoc)",
   "language": "python",
   "name": "hrdoc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
